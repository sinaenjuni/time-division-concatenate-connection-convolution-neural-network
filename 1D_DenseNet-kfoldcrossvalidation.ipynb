{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "pf32MK1QQp6b",
    "outputId": "78d0297e-b627-4a59-e46e-69633420c5b5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/googledrive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "colab_type": "code",
    "id": "wXz2orRxQsB6",
    "outputId": "1a2c1451-1f71-4354-834c-79e381a01f4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip install livelossplot \n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "\n",
    "from keras import backend\n",
    "\n",
    "from keras.layers import AveragePooling1D, Input, GlobalMaxPooling1D\n",
    "from keras import layers, models\n",
    "from keras import backend\n",
    "\n",
    "        \n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AVpGoW9HF3C5"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ByRdciSCZ55"
   },
   "source": [
    "## Load Data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "EwSMiMi2QyBr",
    "outputId": "b272f4f7-f070-4727-978a-dc0addd1a768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240, 100) (3240,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_set_path = './data_set/my_data/class_9'\n",
    "data_set_path = './data_set/svm/'\n",
    "\n",
    "save_path = './save_file/data_3_100_my_svm'\n",
    "\n",
    "x_data = np.load(data_set_path+\"/x_data_3_100_my_svm.npy\")\n",
    "y_data = np.load(data_set_path+\"/y_data_3_100_my_svm.npy\")\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "data_classes = 3\n",
    "\n",
    "# # Normalization\n",
    "# for idx in range(len(x_data)):\n",
    "#     min = np.min(x_data[idx])\n",
    "#     max = np.max(x_data[idx])\n",
    "#     x_data[idx] = (x_data[idx] - min) / (max - min)\n",
    "\n",
    "# fig = plt.figure(figsize=(25,10))\n",
    "\n",
    "# for id in range(4):\n",
    "#     ax = fig.add_subplot(2,4,id+1)\n",
    "#     ax.plot(x_data[id])\n",
    "#     ax.grid(True)\n",
    "#     ax.set_title(f\"x_train{id}\")\n",
    "\n",
    "# # Standardize\n",
    "# for idx in range(len(x_data)):\n",
    "#     standard_deviation = np.std(x_data[idx])\n",
    "#     mean = np.mean(x_data[idx])\n",
    "#     x_data[idx] = (x_data[idx] - mean) / standard_deviation\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(25,10))\n",
    "\n",
    "# for id in range(4):\n",
    "#     ax = fig.add_subplot(2,4,id+1)\n",
    "#     ax.plot(x_data[id])\n",
    "#     ax.grid(True)\n",
    "#     ax.set_title(f\"x_train{id}\")\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "\n",
    "for id in range(4):\n",
    "    ax = fig.add_subplot(2,4,id+1)\n",
    "    ax.plot(x_data[id])\n",
    "    ax.grid(True)\n",
    "    ax.set_title(f\"x_train{id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjCs0sUyFeeV"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2MvkkwRBMkc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesDataAugmentation:\n",
    "\n",
    "    # def __init__(self):\n",
    "        # self.sigma = 0.05\n",
    "\n",
    "    def DA_Jitter(self, X, sigma=0.05):\n",
    "        myNoise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
    "        return X+myNoise\n",
    "\n",
    "    def DA_Scaling(self, X, sigma=0.1):\n",
    "        scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1)) # shape=(1,3)\n",
    "        myNoise = np.matmul(np.ones((X.shape[0],1)), scalingFactor)\n",
    "        return X*myNoise\n",
    "\n",
    "\n",
    "    def GenerateRandomCurves(self, X, sigma=0.2, knot=4):\n",
    "        \n",
    "        xx = np.arange(0, X.shape[0], (X.shape[0]-1) / (knot + 1)) #(3,1) * (1, 6) = (3,6) // (6,3)\n",
    "\n",
    "        yy = np.random.normal(loc = 1.0, scale = sigma, size = (knot + 2)) # (6, 3)\n",
    "        x_range = np.arange(X.shape[0])\n",
    "\n",
    "        cs_x = CubicSpline(xx, yy)\n",
    "\n",
    "        return np.array(cs_x(x_range))\n",
    "\n",
    "\n",
    "    def DA_MagWarp(self, X, sigma=0.2):\n",
    "        return X * self.GenerateRandomCurves(X, sigma)\n",
    "\n",
    "\n",
    "\n",
    "    def DistortTimesteps(self, X, sigma=0.2, knot=4):\n",
    "        tt = self.GenerateRandomCurves(X, sigma, knot) # Regard these samples aroun 1 as time intervals\n",
    "        tt_cum = np.cumsum(tt)        # Add intervals to make a cumulative graph\n",
    "        t_scale = (X.shape[0]-1) / tt_cum[-1] \n",
    "\n",
    "        tt_cum = tt_cum * t_scale\n",
    "\n",
    "        return tt_cum\n",
    "\n",
    "\n",
    "\n",
    "    def DA_TimeWarp(self, X, sigma=0.2):\n",
    "        tt_new = self.DistortTimesteps(X, sigma)\n",
    "\n",
    "        X_new = np.zeros(X.shape) #(3600, 3)      0\n",
    "\n",
    "        x_range = np.arange(X.shape[0]) #(3600,)  0~3599\n",
    "\n",
    "        X_new = np.interp(x_range, tt_new, X)\n",
    "\n",
    "        return X_new\n",
    "\n",
    "\n",
    "\n",
    "    def DA_Permutation(self, X, nPerm=4, minSegLength=10):\n",
    "        data = X\n",
    "        X_new = np.zeros(data.shape)     #(100,)\n",
    "        idx = np.random.permutation(nPerm) \n",
    "\n",
    "        bWhile = True\n",
    "        while bWhile == True:\n",
    "            segs = np.zeros(nPerm+1, dtype=int)\n",
    "            \n",
    "            segs[1:-1] = np.sort(np.random.randint(minSegLength, data.shape[0]-minSegLength, nPerm-1))\n",
    "            segs[-1] = data.shape[0]\n",
    "\n",
    "            if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
    "                bWhile = False\n",
    "\n",
    "        pp = 0\n",
    "        for ii in range(nPerm):\n",
    "            x_temp = data[segs[idx[ii]]:segs[idx[ii]+1]]\n",
    "            X_new[pp:pp+len(x_temp)] = x_temp\n",
    "            pp += len(x_temp)\n",
    "\n",
    "        return X_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def DA_Permutation(self, X, nPerm=4, minSegLength=10):\n",
    "        data = X\n",
    "        X_new = np.zeros(data.shape)     #(100,)\n",
    "        idx = np.random.permutation(nPerm) \n",
    "\n",
    "        bWhile = True\n",
    "        while bWhile == True:\n",
    "            segs = np.zeros(nPerm+1, dtype=int)\n",
    "            \n",
    "            segs[1:-1] = np.sort(np.random.randint(minSegLength, data.shape[0]-minSegLength, nPerm-1))\n",
    "            segs[-1] = data.shape[0]\n",
    "\n",
    "            if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
    "                bWhile = False\n",
    "\n",
    "        pp = 0\n",
    "        for ii in range(nPerm):\n",
    "            x_temp = data[segs[idx[ii]]:segs[idx[ii]+1]]\n",
    "            X_new[pp:pp+len(x_temp)] = x_temp\n",
    "            pp += len(x_temp)\n",
    "\n",
    "        return X_new\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aH3x6zo2FiyT"
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "MNbBVlyURAd8",
    "outputId": "ce63d075-9322-4f3b-8a24-2eeebc8e7be1"
   },
   "outputs": [],
   "source": [
    "\n",
    "#data_set_구분\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=1, shuffle=False)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "#y_data onehot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, data_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, data_classes)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OAzDrwbFqnn"
   },
   "source": [
    "## Data packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZYWOEh9sR1ss",
    "outputId": "12f68d36-1b16-4a83-f70d-fa64f469218b"
   },
   "outputs": [],
   "source": [
    "isPerdata = True\n",
    "# isPerdata = False\n",
    "\n",
    "if not isPerdata:\n",
    "    x_train = x_train.reshape(-1, 100)\n",
    "    x_test  = x_test.reshape(-1, 100)\n",
    "    y_train = y_train.reshape(-1, 9)\n",
    "    y_test = y_test.reshape(-1, 9)\n",
    "else:\n",
    "    # x_train = x_train.reshape(-1, 1, 100)\n",
    "    # x_test  = x_test.reshape(-1, 1, 100)\n",
    "#     y_train = y_train.reshape(-1, 1, 9)\n",
    "#     y_test = y_test.reshape(-1, 1, 9)\n",
    "    x_train = x_train.reshape(-1, 100,1)\n",
    "    x_test  = x_test.reshape(-1, 100,1)\n",
    "    # y_train = y_train.reshape(-1, 9)\n",
    "    # y_test = y_test.reshape(-1, 9)\n",
    "\n",
    "\n",
    "# x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "# x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240, 100, 1) (3240, 3)\n"
     ]
    }
   ],
   "source": [
    "#y_data onehot encoding\n",
    "y = keras.utils.to_categorical(y_data, data_classes)\n",
    "x = x_data.reshape(-1, 100,1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YilogMXq358K"
   },
   "outputs": [],
   "source": [
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# 특정 클래스에 대한 정밀도\n",
    "def single_class_precision(interesting_class_id):\n",
    "    def prec(y_true, y_pred):\n",
    "        class_id_true = K.argmax(y_true, axis=-1)\n",
    "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
    "        precision_mask = K.cast(K.equal(class_id_pred, interesting_class_id), 'int32')\n",
    "        class_prec_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * precision_mask\n",
    "        class_prec = K.cast(K.sum(class_prec_tensor), 'float32') / K.cast(K.maximum(K.sum(precision_mask), 1), 'float32')\n",
    "        return class_prec\n",
    "    return prec\n",
    "\n",
    "\n",
    "# 특정 클래스에 대한 재현율\n",
    "def single_class_recall(interesting_class_id):\n",
    "    def recall(y_true, y_pred):\n",
    "        class_id_true = K.argmax(y_true, axis=-1)\n",
    "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
    "        recall_mask = K.cast(K.equal(class_id_true, interesting_class_id), 'int32')\n",
    "        class_recall_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * recall_mask\n",
    "        class_recall = K.cast(K.sum(class_recall_tensor), 'float32') / K.cast(K.maximum(K.sum(recall_mask), 1), 'float32')\n",
    "        return class_recall\n",
    "    return recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iMdPXqMF-xu"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, blocks, growth_rate, name):\n",
    "    for i in range(blocks):\n",
    "        x = conv_block(x, growth_rate, name=name + '_block' + str(i + 1))\n",
    "    return x\n",
    "\n",
    "def conv_block(x, growth_rate, name):\n",
    "    bn_axis = 2\n",
    "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_0_bn')(x)\n",
    "\n",
    "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
    "\n",
    "    x1 = layers.Conv1D(4 * growth_rate, 1, use_bias=True, kernel_initializer='he_normal', name=name + '_1_conv')(x1)\n",
    "\n",
    "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x1)\n",
    "\n",
    "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
    "\n",
    "    x1 = layers.Conv1D(growth_rate, 3, padding='same', use_bias=True, kernel_initializer='he_normal', name=name + '_2_conv')(x1)\n",
    "\n",
    "    x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "\n",
    "    # x = layers.Conv1D(growth_rate, 1, padding='same', use_bias=False, name=name + '_3_conv')(x)\n",
    "    # x = layers.Add(name=name + '_concat')([x, x1])\n",
    "\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def transition_block(x, reduction, name):\n",
    "    bn_axis = 2\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_bn')(x)\n",
    "\n",
    "    x = layers.Activation('relu', name=name + '_relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(int(backend.int_shape(x)[bn_axis] * reduction), 1, use_bias=False, kernel_initializer='he_normal', name=name + '_conv')(x)\n",
    "                      \n",
    "#     x = layers.AveragePooling1D(2, strides=2, name=name + '_pool')(x)\n",
    "    x = layers.MaxPooling1D(2, strides=2, name=name + '_pool')(x)\n",
    "\n",
    "    # x = spatial_pyramid_pool(x, 3, bn_axis, name)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "class SpatialPyramidPooling(Layer):\n",
    "    \"\"\"Spatial pyramid pooling layer for 2D inputs.\n",
    "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
    "    K. He, X. Zhang, S. Ren, J. Sun\n",
    "    # Arguments\n",
    "        pool_list: list of int\n",
    "            List of pooling regions to use. The length of the list is the number of pooling regions,\n",
    "            each int in the list is the number of regions in that pool. For example [1,2,4] would be 3\n",
    "            regions with 1, 2x2 and 4x4 max pools, so 21 outputs per feature map\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        2D tensor with shape:\n",
    "        `(samples, channels * sum([i * i for i in pool_list])`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_list, **kwargs):\n",
    "\n",
    "        self.pool_list = pool_list\n",
    "\n",
    "        self.num_outputs_per_channel = sum([i for i in pool_list])\n",
    "\n",
    "        super(SpatialPyramidPooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[2]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.nb_channels * self.num_outputs_per_channel)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list}\n",
    "        base_config = super(SpatialPyramidPooling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        input_shape = K.shape(x)\n",
    "\n",
    "        num_rows = input_shape[1]\n",
    "        # num_cols = input_shape[2]\n",
    "\n",
    "        row_length = [K.cast(num_rows, 'float32') / i for i in self.pool_list]\n",
    "        # col_length = [K.cast(num_cols, 'float32') / i for i in self.pool_list]\n",
    "\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "\n",
    "        for pool_num, num_pool_regions in enumerate(self.pool_list):\n",
    "            for jy in range(num_pool_regions):\n",
    "                # for ix in range(num_pool_regions):\n",
    "                #     x1 = ix * col_length[pool_num]\n",
    "                #     x2 = ix * col_length[pool_num] + col_length[pool_num]\n",
    "                    y1 = jy * row_length[pool_num]\n",
    "                    y2 = jy * row_length[pool_num] + row_length[pool_num]\n",
    "\n",
    "                    # x1 = K.cast(K.round(x1), 'int32')\n",
    "                    # x2 = K.cast(K.round(x2), 'int32')\n",
    "                    y1 = K.cast(K.round(y1), 'int32')\n",
    "                    y2 = K.cast(K.round(y2), 'int32')\n",
    "\n",
    "                    # new_shape = [input_shape[0], y2 - y1,\n",
    "                    #                 x2 - x1, input_shape[3]]\n",
    "\n",
    "                    new_shape = [input_shape[0], y2 - y1, input_shape[2]]\n",
    "\n",
    "\n",
    "                    x_crop = x[:, y1:y2, :]\n",
    "                    xm = K.reshape(x_crop, new_shape)\n",
    "#                     pooled_val = K.max(xm, axis=1)\n",
    "                    pooled_val = K.mean(xm, axis=1)\n",
    "                    outputs.append(pooled_val)\n",
    "\n",
    "        #outputs = K.concatenate(outputs,axis = 1)\n",
    "        outputs = K.concatenate(outputs)\n",
    "        #outputs = K.reshape(outputs,(len(self.pool_list),self.num_outputs_per_channel,input_shape[0],input_shape[1]))\n",
    "        #outputs = K.permute_dimensions(outputs,(3,1,0,2))\n",
    "        #outputs = K.reshape(outputs,(input_shape[0], self.num_outputs_per_channel * self.nb_channels))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DcnnNetv3(depth=None,\n",
    "            growthRate = None,\n",
    "            include_top = None,\n",
    "            input_tensor = None,\n",
    "            input_shape = None,            \n",
    "            pooling = None,\n",
    "            classes = None,\n",
    "            **kwargs):\n",
    "    \n",
    "\n",
    "\n",
    "    if type(depth) is list:\n",
    "        if len(depth) == 4:\n",
    "            blocks = depth\n",
    "            depth = sum(blocks) * 2 + 5\n",
    "        else:\n",
    "            assert True, \"custom depth length is 4\"\n",
    "    else:\n",
    "        if depth == 121:\n",
    "            blocks = [6, 12, 24, 16]\n",
    "        elif depth == 169:\n",
    "            blocks = [6, 12, 32, 32]\n",
    "        elif depth == 201:\n",
    "            blocks = [6, 12, 48, 32]\n",
    "        elif depth == 161:\n",
    "            blocks = [6, 12, 36, 24]\n",
    "        else:\n",
    "            print(f'{depth} is not allowed depth, you will use custom_blocks parameter')\n",
    "\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "\n",
    "    nChannels = 2 * growthRate\n",
    "    bn_axis = 2\n",
    "\n",
    "\n",
    "    x = layers.ZeroPadding1D(padding=3)(img_input)\n",
    "    x = layers.Conv1D(nChannels, 7, strides=1, use_bias=True, name='conv1/conv')(x)\n",
    "\n",
    "#     x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "#     x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "\n",
    "#     x = layers.ZeroPadding1D(padding=1)(x)\n",
    "#     x = layers.MaxPooling1D(3, strides=2, name='pool1')(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    # [6, 12, 48, 32]\n",
    "    x = dense_block(x, blocks[0], growthRate, name='conv2')\n",
    "    x = transition_block(x, 0.5, name='pool2')\n",
    "\n",
    "    x = dense_block(x, blocks[1], growthRate, name='conv3')\n",
    "    x = transition_block(x, 0.5, name='pool3')\n",
    "\n",
    "    x = dense_block(x, blocks[2], growthRate, name='conv4')\n",
    "    x = transition_block(x, 0.5, name='pool4')\n",
    "\n",
    "    x = dense_block(x, blocks[3], growthRate, name='conv5')\n",
    "\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='bn')(x)\n",
    "    x = layers.Activation('relu', name='relu')(x)\n",
    "\n",
    "    \n",
    "##################################################\n",
    "\n",
    "\n",
    "# FCN Classification block\n",
    "#     x = layers.MaxPooling1D(2, strides=2, name='max_pool')(x)\n",
    "#     x = layers.Conv1D(4096,6, strides=1, activation='relu',name='fc1',padding='valid')(x)\n",
    "#     x = layers.Conv1D(1440,12, strides=1, activation='relu',name='fc2',padding='valid')(x)\n",
    "#     x = layers.Conv1D(9, 1, strides=1,activation='softmax',name='predictions',padding='valid')(x)\n",
    "\n",
    "################################################\n",
    "    \n",
    "#     x = SpatialPyramidPooling([1,2,4], name='spp')(x)\n",
    "\n",
    "    if include_top:\n",
    "#         # !!!!!\n",
    "        x = layers.GlobalAveragePooling1D(name='avg_pool')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='fc')(x)\n",
    "#     else:\n",
    "#         if pooling == 'avg':\n",
    "#             x = layers.GlobalAveragePooling1D(name='avg_pool')(x)\n",
    "#         elif pooling == 'max':\n",
    "#             x = layers.GlobalMaxPooling1D(name='max_pool')(x)\n",
    "\n",
    "\n",
    "    model = models.Model(img_input, x, name=f'densenet{depth}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createModel():\n",
    "    inputs = layers.Input(shape=(100, 1))\n",
    "\n",
    "    modelv3 = DcnnNetv3(depth=201, growthRate=24, include_top=True, \n",
    "                        input_tensor=inputs, pooling='max', classes=data_classes)\n",
    "    modelv3.summary()\n",
    "    modelv3.name\n",
    "    return modelv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Learning rate:  0.01\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_1 (ZeroPadding1D (None, 106, 1)       0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv1D)             (None, 100, 48)      384         zero_padding1d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 100, 48)      192         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 100, 48)      0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv1D)    (None, 100, 96)      4704        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 100, 96)      384         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 100, 96)      0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv1D)    (None, 100, 24)      6936        conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 100, 72)      0           conv1/conv[0][0]                 \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 100, 72)      288         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 100, 72)      0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv1D)    (None, 100, 96)      7008        conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 100, 96)      384         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 100, 96)      0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv1D)    (None, 100, 24)      6936        conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 100, 96)      0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 100, 96)      384         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 100, 96)      0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv1D)    (None, 100, 96)      9312        conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 100, 96)      384         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 100, 96)      0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv1D)    (None, 100, 24)      6936        conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 100, 120)     0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 100, 120)     480         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 100, 120)     0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv1D)    (None, 100, 96)      11616       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 100, 96)      384         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 100, 96)      0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv1D)    (None, 100, 24)      6936        conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 100, 144)     0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 100, 144)     576         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 100, 144)     0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv1D)    (None, 100, 96)      13920       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 100, 96)      384         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 100, 96)      0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv1D)    (None, 100, 24)      6936        conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 100, 168)     0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 100, 168)     672         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 100, 168)     0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv1D)    (None, 100, 96)      16224       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 100, 96)      384         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 100, 96)      0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv1D)    (None, 100, 24)      6936        conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 100, 192)     0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 100, 192)     768         conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 100, 192)     0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv1D)             (None, 100, 96)      18432       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (MaxPooling1D)       (None, 50, 96)       0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 50, 96)       384         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 50, 96)       0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv1D)    (None, 50, 96)       9312        conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 50, 96)       384         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 50, 96)       0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 50, 120)      0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 50, 120)      480         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 50, 120)      0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv1D)    (None, 50, 96)       11616       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 50, 96)       384         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 50, 96)       0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 50, 144)      0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 50, 144)      576         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 50, 144)      0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv1D)    (None, 50, 96)       13920       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 50, 96)       384         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 50, 96)       0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 50, 168)      0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 50, 168)      672         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 50, 168)      0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv1D)    (None, 50, 96)       16224       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 50, 96)       384         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 50, 96)       0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 50, 192)      0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 50, 192)      768         conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 50, 192)      0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv1D)    (None, 50, 96)       18528       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 50, 96)       384         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 50, 96)       0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 50, 216)      0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 50, 216)      864         conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 50, 216)      0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv1D)    (None, 50, 96)       20832       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 50, 96)       384         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 50, 96)       0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 50, 240)      0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 50, 240)      960         conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 50, 240)      0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv1D)    (None, 50, 96)       23136       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 50, 96)       384         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 50, 96)       0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 50, 264)      0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 50, 264)      1056        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 50, 264)      0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv1D)    (None, 50, 96)       25440       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 50, 96)       384         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 50, 96)       0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 50, 288)      0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 50, 288)      1152        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 50, 288)      0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv1D)    (None, 50, 96)       27744       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 50, 96)       384         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 50, 96)       0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv1D)    (None, 50, 24)       6936        conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 50, 312)      0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 50, 312)      1248        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 50, 312)      0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv1D)   (None, 50, 96)       30048       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 50, 96)       384         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 50, 96)       0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv1D)   (None, 50, 24)       6936        conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 50, 336)      0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 50, 336)      1344        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 50, 336)      0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv1D)   (None, 50, 96)       32352       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 50, 96)       384         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 50, 96)       0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv1D)   (None, 50, 24)       6936        conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 50, 360)      0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 50, 360)      1440        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 50, 360)      0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv1D)   (None, 50, 96)       34656       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 50, 96)       384         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 50, 96)       0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv1D)   (None, 50, 24)       6936        conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 50, 384)      0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 50, 384)      1536        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 50, 384)      0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv1D)             (None, 50, 192)      73728       pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (MaxPooling1D)       (None, 25, 192)      0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 25, 192)      768         pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 25, 192)      0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv1D)    (None, 25, 96)       18528       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 25, 96)       384         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 25, 96)       0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 25, 216)      0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 25, 216)      864         conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 25, 216)      0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv1D)    (None, 25, 96)       20832       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 25, 96)       384         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 25, 96)       0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 25, 240)      0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 25, 240)      960         conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 25, 240)      0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv1D)    (None, 25, 96)       23136       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 25, 96)       384         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 25, 96)       0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 25, 264)      0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 25, 264)      1056        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 25, 264)      0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv1D)    (None, 25, 96)       25440       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 25, 96)       384         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 25, 96)       0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 25, 288)      0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 25, 288)      1152        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 25, 288)      0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv1D)    (None, 25, 96)       27744       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 25, 96)       384         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 25, 96)       0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 25, 312)      0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 25, 312)      1248        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 25, 312)      0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv1D)    (None, 25, 96)       30048       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 25, 96)       384         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 25, 96)       0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 25, 336)      0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 25, 336)      1344        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 25, 336)      0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv1D)    (None, 25, 96)       32352       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 25, 96)       384         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 25, 96)       0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 25, 360)      0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 25, 360)      1440        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 25, 360)      0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv1D)    (None, 25, 96)       34656       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 25, 96)       384         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 25, 96)       0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 25, 384)      0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 25, 384)      1536        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 25, 384)      0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv1D)    (None, 25, 96)       36960       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 25, 96)       384         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 25, 96)       0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv1D)    (None, 25, 24)       6936        conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 25, 408)      0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 25, 408)      1632        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 25, 408)      0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv1D)   (None, 25, 96)       39264       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 25, 96)       384         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 25, 96)       0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 25, 432)      0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 25, 432)      1728        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 25, 432)      0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv1D)   (None, 25, 96)       41568       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 25, 96)       384         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 25, 96)       0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 25, 456)      0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 25, 456)      1824        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 25, 456)      0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv1D)   (None, 25, 96)       43872       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 25, 96)       384         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 25, 96)       0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 25, 480)      0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 25, 480)      1920        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 25, 480)      0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv1D)   (None, 25, 96)       46176       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 25, 96)       384         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 25, 96)       0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 25, 504)      0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 25, 504)      2016        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 25, 504)      0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv1D)   (None, 25, 96)       48480       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 25, 96)       384         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 25, 96)       0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 25, 528)      0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 25, 528)      2112        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 25, 528)      0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv1D)   (None, 25, 96)       50784       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 25, 96)       384         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 25, 96)       0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 25, 552)      0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 25, 552)      2208        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 25, 552)      0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv1D)   (None, 25, 96)       53088       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 25, 96)       384         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 25, 96)       0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 25, 576)      0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 25, 576)      2304        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 25, 576)      0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv1D)   (None, 25, 96)       55392       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 25, 96)       384         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 25, 96)       0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 25, 600)      0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 25, 600)      2400        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 25, 600)      0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv1D)   (None, 25, 96)       57696       conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 25, 96)       384         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 25, 96)       0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 25, 624)      0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 25, 624)      2496        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 25, 624)      0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv1D)   (None, 25, 96)       60000       conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 25, 96)       384         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 25, 96)       0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 25, 648)      0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 25, 648)      2592        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 25, 648)      0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv1D)   (None, 25, 96)       62304       conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 25, 96)       384         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 25, 96)       0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 25, 672)      0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 25, 672)      2688        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 25, 672)      0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv1D)   (None, 25, 96)       64608       conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 25, 96)       384         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 25, 96)       0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 25, 696)      0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 25, 696)      2784        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 25, 696)      0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv1D)   (None, 25, 96)       66912       conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 25, 96)       384         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 25, 96)       0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 25, 720)      0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 25, 720)      2880        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 25, 720)      0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv1D)   (None, 25, 96)       69216       conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 25, 96)       384         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 25, 96)       0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 25, 744)      0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 25, 744)      2976        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 25, 744)      0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv1D)   (None, 25, 96)       71520       conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 25, 96)       384         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 25, 96)       0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 25, 768)      0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_0_bn (BatchNormal (None, 25, 768)      3072        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_0_relu (Activatio (None, 25, 768)      0           conv4_block25_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_conv (Conv1D)   (None, 25, 96)       73824       conv4_block25_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_bn (BatchNormal (None, 25, 96)       384         conv4_block25_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_relu (Activatio (None, 25, 96)       0           conv4_block25_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block25_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_concat (Concatena (None, 25, 792)      0           conv4_block24_concat[0][0]       \n",
      "                                                                 conv4_block25_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_0_bn (BatchNormal (None, 25, 792)      3168        conv4_block25_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_0_relu (Activatio (None, 25, 792)      0           conv4_block26_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_conv (Conv1D)   (None, 25, 96)       76128       conv4_block26_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_bn (BatchNormal (None, 25, 96)       384         conv4_block26_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_relu (Activatio (None, 25, 96)       0           conv4_block26_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block26_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_concat (Concatena (None, 25, 816)      0           conv4_block25_concat[0][0]       \n",
      "                                                                 conv4_block26_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_0_bn (BatchNormal (None, 25, 816)      3264        conv4_block26_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_0_relu (Activatio (None, 25, 816)      0           conv4_block27_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_conv (Conv1D)   (None, 25, 96)       78432       conv4_block27_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_bn (BatchNormal (None, 25, 96)       384         conv4_block27_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_relu (Activatio (None, 25, 96)       0           conv4_block27_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block27_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_concat (Concatena (None, 25, 840)      0           conv4_block26_concat[0][0]       \n",
      "                                                                 conv4_block27_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_0_bn (BatchNormal (None, 25, 840)      3360        conv4_block27_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_0_relu (Activatio (None, 25, 840)      0           conv4_block28_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_conv (Conv1D)   (None, 25, 96)       80736       conv4_block28_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_bn (BatchNormal (None, 25, 96)       384         conv4_block28_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_relu (Activatio (None, 25, 96)       0           conv4_block28_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block28_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_concat (Concatena (None, 25, 864)      0           conv4_block27_concat[0][0]       \n",
      "                                                                 conv4_block28_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_0_bn (BatchNormal (None, 25, 864)      3456        conv4_block28_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_0_relu (Activatio (None, 25, 864)      0           conv4_block29_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_conv (Conv1D)   (None, 25, 96)       83040       conv4_block29_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_bn (BatchNormal (None, 25, 96)       384         conv4_block29_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_relu (Activatio (None, 25, 96)       0           conv4_block29_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block29_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_concat (Concatena (None, 25, 888)      0           conv4_block28_concat[0][0]       \n",
      "                                                                 conv4_block29_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_0_bn (BatchNormal (None, 25, 888)      3552        conv4_block29_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_0_relu (Activatio (None, 25, 888)      0           conv4_block30_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_conv (Conv1D)   (None, 25, 96)       85344       conv4_block30_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_bn (BatchNormal (None, 25, 96)       384         conv4_block30_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_relu (Activatio (None, 25, 96)       0           conv4_block30_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block30_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_concat (Concatena (None, 25, 912)      0           conv4_block29_concat[0][0]       \n",
      "                                                                 conv4_block30_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_0_bn (BatchNormal (None, 25, 912)      3648        conv4_block30_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_0_relu (Activatio (None, 25, 912)      0           conv4_block31_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_conv (Conv1D)   (None, 25, 96)       87648       conv4_block31_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_bn (BatchNormal (None, 25, 96)       384         conv4_block31_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_relu (Activatio (None, 25, 96)       0           conv4_block31_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block31_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_concat (Concatena (None, 25, 936)      0           conv4_block30_concat[0][0]       \n",
      "                                                                 conv4_block31_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_0_bn (BatchNormal (None, 25, 936)      3744        conv4_block31_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_0_relu (Activatio (None, 25, 936)      0           conv4_block32_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_conv (Conv1D)   (None, 25, 96)       89952       conv4_block32_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_bn (BatchNormal (None, 25, 96)       384         conv4_block32_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_relu (Activatio (None, 25, 96)       0           conv4_block32_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block32_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_concat (Concatena (None, 25, 960)      0           conv4_block31_concat[0][0]       \n",
      "                                                                 conv4_block32_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_0_bn (BatchNormal (None, 25, 960)      3840        conv4_block32_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_0_relu (Activatio (None, 25, 960)      0           conv4_block33_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_conv (Conv1D)   (None, 25, 96)       92256       conv4_block33_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_bn (BatchNormal (None, 25, 96)       384         conv4_block33_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_relu (Activatio (None, 25, 96)       0           conv4_block33_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block33_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_concat (Concatena (None, 25, 984)      0           conv4_block32_concat[0][0]       \n",
      "                                                                 conv4_block33_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_0_bn (BatchNormal (None, 25, 984)      3936        conv4_block33_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_0_relu (Activatio (None, 25, 984)      0           conv4_block34_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_conv (Conv1D)   (None, 25, 96)       94560       conv4_block34_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_bn (BatchNormal (None, 25, 96)       384         conv4_block34_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_relu (Activatio (None, 25, 96)       0           conv4_block34_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block34_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_concat (Concatena (None, 25, 1008)     0           conv4_block33_concat[0][0]       \n",
      "                                                                 conv4_block34_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_0_bn (BatchNormal (None, 25, 1008)     4032        conv4_block34_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_0_relu (Activatio (None, 25, 1008)     0           conv4_block35_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_conv (Conv1D)   (None, 25, 96)       96864       conv4_block35_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_bn (BatchNormal (None, 25, 96)       384         conv4_block35_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_relu (Activatio (None, 25, 96)       0           conv4_block35_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block35_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_concat (Concatena (None, 25, 1032)     0           conv4_block34_concat[0][0]       \n",
      "                                                                 conv4_block35_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_0_bn (BatchNormal (None, 25, 1032)     4128        conv4_block35_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_0_relu (Activatio (None, 25, 1032)     0           conv4_block36_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_conv (Conv1D)   (None, 25, 96)       99168       conv4_block36_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_bn (BatchNormal (None, 25, 96)       384         conv4_block36_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_relu (Activatio (None, 25, 96)       0           conv4_block36_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block36_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_concat (Concatena (None, 25, 1056)     0           conv4_block35_concat[0][0]       \n",
      "                                                                 conv4_block36_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block37_0_bn (BatchNormal (None, 25, 1056)     4224        conv4_block36_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block37_0_relu (Activatio (None, 25, 1056)     0           conv4_block37_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block37_1_conv (Conv1D)   (None, 25, 96)       101472      conv4_block37_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block37_1_bn (BatchNormal (None, 25, 96)       384         conv4_block37_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block37_1_relu (Activatio (None, 25, 96)       0           conv4_block37_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block37_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block37_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block37_concat (Concatena (None, 25, 1080)     0           conv4_block36_concat[0][0]       \n",
      "                                                                 conv4_block37_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block38_0_bn (BatchNormal (None, 25, 1080)     4320        conv4_block37_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block38_0_relu (Activatio (None, 25, 1080)     0           conv4_block38_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block38_1_conv (Conv1D)   (None, 25, 96)       103776      conv4_block38_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block38_1_bn (BatchNormal (None, 25, 96)       384         conv4_block38_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block38_1_relu (Activatio (None, 25, 96)       0           conv4_block38_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block38_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block38_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block38_concat (Concatena (None, 25, 1104)     0           conv4_block37_concat[0][0]       \n",
      "                                                                 conv4_block38_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block39_0_bn (BatchNormal (None, 25, 1104)     4416        conv4_block38_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block39_0_relu (Activatio (None, 25, 1104)     0           conv4_block39_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block39_1_conv (Conv1D)   (None, 25, 96)       106080      conv4_block39_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block39_1_bn (BatchNormal (None, 25, 96)       384         conv4_block39_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block39_1_relu (Activatio (None, 25, 96)       0           conv4_block39_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block39_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block39_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block39_concat (Concatena (None, 25, 1128)     0           conv4_block38_concat[0][0]       \n",
      "                                                                 conv4_block39_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block40_0_bn (BatchNormal (None, 25, 1128)     4512        conv4_block39_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block40_0_relu (Activatio (None, 25, 1128)     0           conv4_block40_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block40_1_conv (Conv1D)   (None, 25, 96)       108384      conv4_block40_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block40_1_bn (BatchNormal (None, 25, 96)       384         conv4_block40_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block40_1_relu (Activatio (None, 25, 96)       0           conv4_block40_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block40_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block40_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block40_concat (Concatena (None, 25, 1152)     0           conv4_block39_concat[0][0]       \n",
      "                                                                 conv4_block40_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block41_0_bn (BatchNormal (None, 25, 1152)     4608        conv4_block40_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block41_0_relu (Activatio (None, 25, 1152)     0           conv4_block41_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block41_1_conv (Conv1D)   (None, 25, 96)       110688      conv4_block41_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block41_1_bn (BatchNormal (None, 25, 96)       384         conv4_block41_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block41_1_relu (Activatio (None, 25, 96)       0           conv4_block41_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block41_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block41_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block41_concat (Concatena (None, 25, 1176)     0           conv4_block40_concat[0][0]       \n",
      "                                                                 conv4_block41_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block42_0_bn (BatchNormal (None, 25, 1176)     4704        conv4_block41_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block42_0_relu (Activatio (None, 25, 1176)     0           conv4_block42_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block42_1_conv (Conv1D)   (None, 25, 96)       112992      conv4_block42_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block42_1_bn (BatchNormal (None, 25, 96)       384         conv4_block42_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block42_1_relu (Activatio (None, 25, 96)       0           conv4_block42_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block42_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block42_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block42_concat (Concatena (None, 25, 1200)     0           conv4_block41_concat[0][0]       \n",
      "                                                                 conv4_block42_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block43_0_bn (BatchNormal (None, 25, 1200)     4800        conv4_block42_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block43_0_relu (Activatio (None, 25, 1200)     0           conv4_block43_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block43_1_conv (Conv1D)   (None, 25, 96)       115296      conv4_block43_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block43_1_bn (BatchNormal (None, 25, 96)       384         conv4_block43_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block43_1_relu (Activatio (None, 25, 96)       0           conv4_block43_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block43_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block43_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block43_concat (Concatena (None, 25, 1224)     0           conv4_block42_concat[0][0]       \n",
      "                                                                 conv4_block43_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block44_0_bn (BatchNormal (None, 25, 1224)     4896        conv4_block43_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block44_0_relu (Activatio (None, 25, 1224)     0           conv4_block44_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block44_1_conv (Conv1D)   (None, 25, 96)       117600      conv4_block44_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block44_1_bn (BatchNormal (None, 25, 96)       384         conv4_block44_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block44_1_relu (Activatio (None, 25, 96)       0           conv4_block44_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block44_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block44_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block44_concat (Concatena (None, 25, 1248)     0           conv4_block43_concat[0][0]       \n",
      "                                                                 conv4_block44_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block45_0_bn (BatchNormal (None, 25, 1248)     4992        conv4_block44_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block45_0_relu (Activatio (None, 25, 1248)     0           conv4_block45_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block45_1_conv (Conv1D)   (None, 25, 96)       119904      conv4_block45_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block45_1_bn (BatchNormal (None, 25, 96)       384         conv4_block45_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block45_1_relu (Activatio (None, 25, 96)       0           conv4_block45_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block45_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block45_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block45_concat (Concatena (None, 25, 1272)     0           conv4_block44_concat[0][0]       \n",
      "                                                                 conv4_block45_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block46_0_bn (BatchNormal (None, 25, 1272)     5088        conv4_block45_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block46_0_relu (Activatio (None, 25, 1272)     0           conv4_block46_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block46_1_conv (Conv1D)   (None, 25, 96)       122208      conv4_block46_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block46_1_bn (BatchNormal (None, 25, 96)       384         conv4_block46_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block46_1_relu (Activatio (None, 25, 96)       0           conv4_block46_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block46_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block46_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block46_concat (Concatena (None, 25, 1296)     0           conv4_block45_concat[0][0]       \n",
      "                                                                 conv4_block46_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block47_0_bn (BatchNormal (None, 25, 1296)     5184        conv4_block46_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block47_0_relu (Activatio (None, 25, 1296)     0           conv4_block47_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block47_1_conv (Conv1D)   (None, 25, 96)       124512      conv4_block47_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block47_1_bn (BatchNormal (None, 25, 96)       384         conv4_block47_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block47_1_relu (Activatio (None, 25, 96)       0           conv4_block47_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block47_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block47_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block47_concat (Concatena (None, 25, 1320)     0           conv4_block46_concat[0][0]       \n",
      "                                                                 conv4_block47_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block48_0_bn (BatchNormal (None, 25, 1320)     5280        conv4_block47_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block48_0_relu (Activatio (None, 25, 1320)     0           conv4_block48_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block48_1_conv (Conv1D)   (None, 25, 96)       126816      conv4_block48_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block48_1_bn (BatchNormal (None, 25, 96)       384         conv4_block48_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block48_1_relu (Activatio (None, 25, 96)       0           conv4_block48_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block48_2_conv (Conv1D)   (None, 25, 24)       6936        conv4_block48_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block48_concat (Concatena (None, 25, 1344)     0           conv4_block47_concat[0][0]       \n",
      "                                                                 conv4_block48_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 25, 1344)     5376        conv4_block48_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 25, 1344)     0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv1D)             (None, 25, 672)      903168      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (MaxPooling1D)       (None, 12, 672)      0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 12, 672)      2688        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 12, 672)      0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv1D)    (None, 12, 96)       64608       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 12, 96)       384         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 12, 96)       0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 12, 696)      0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 12, 696)      2784        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 12, 696)      0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv1D)    (None, 12, 96)       66912       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 12, 96)       384         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 12, 96)       0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 12, 720)      0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 12, 720)      2880        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 12, 720)      0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv1D)    (None, 12, 96)       69216       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 12, 96)       384         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 12, 96)       0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 12, 744)      0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 12, 744)      2976        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 12, 744)      0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv1D)    (None, 12, 96)       71520       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 12, 96)       384         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 12, 96)       0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 12, 768)      0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 12, 768)      3072        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 12, 768)      0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv1D)    (None, 12, 96)       73824       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 12, 96)       384         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 12, 96)       0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 12, 792)      0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 12, 792)      3168        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 12, 792)      0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv1D)    (None, 12, 96)       76128       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 12, 96)       384         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 12, 96)       0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 12, 816)      0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 12, 816)      3264        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 12, 816)      0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv1D)    (None, 12, 96)       78432       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 12, 96)       384         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 12, 96)       0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 12, 840)      0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 12, 840)      3360        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 12, 840)      0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv1D)    (None, 12, 96)       80736       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 12, 96)       384         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 12, 96)       0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 12, 864)      0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 12, 864)      3456        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 12, 864)      0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv1D)    (None, 12, 96)       83040       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 12, 96)       384         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 12, 96)       0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv1D)    (None, 12, 24)       6936        conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 12, 888)      0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 12, 888)      3552        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 12, 888)      0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv1D)   (None, 12, 96)       85344       conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 12, 96)       384         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 12, 96)       0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 12, 912)      0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 12, 912)      3648        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 12, 912)      0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv1D)   (None, 12, 96)       87648       conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 12, 96)       384         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 12, 96)       0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 12, 936)      0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 12, 936)      3744        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 12, 936)      0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv1D)   (None, 12, 96)       89952       conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 12, 96)       384         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 12, 96)       0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 12, 960)      0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 12, 960)      3840        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 12, 960)      0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv1D)   (None, 12, 96)       92256       conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 12, 96)       384         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 12, 96)       0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 12, 984)      0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 12, 984)      3936        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 12, 984)      0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv1D)   (None, 12, 96)       94560       conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 12, 96)       384         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 12, 96)       0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 12, 1008)     0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 12, 1008)     4032        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 12, 1008)     0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv1D)   (None, 12, 96)       96864       conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 12, 96)       384         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 12, 96)       0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 12, 1032)     0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 12, 1032)     4128        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 12, 1032)     0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv1D)   (None, 12, 96)       99168       conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 12, 96)       384         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 12, 96)       0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 12, 1056)     0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block17_0_bn (BatchNormal (None, 12, 1056)     4224        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block17_0_relu (Activatio (None, 12, 1056)     0           conv5_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block17_1_conv (Conv1D)   (None, 12, 96)       101472      conv5_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block17_1_bn (BatchNormal (None, 12, 96)       384         conv5_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block17_1_relu (Activatio (None, 12, 96)       0           conv5_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block17_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block17_concat (Concatena (None, 12, 1080)     0           conv5_block16_concat[0][0]       \n",
      "                                                                 conv5_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block18_0_bn (BatchNormal (None, 12, 1080)     4320        conv5_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block18_0_relu (Activatio (None, 12, 1080)     0           conv5_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block18_1_conv (Conv1D)   (None, 12, 96)       103776      conv5_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block18_1_bn (BatchNormal (None, 12, 96)       384         conv5_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block18_1_relu (Activatio (None, 12, 96)       0           conv5_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block18_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block18_concat (Concatena (None, 12, 1104)     0           conv5_block17_concat[0][0]       \n",
      "                                                                 conv5_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block19_0_bn (BatchNormal (None, 12, 1104)     4416        conv5_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block19_0_relu (Activatio (None, 12, 1104)     0           conv5_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block19_1_conv (Conv1D)   (None, 12, 96)       106080      conv5_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block19_1_bn (BatchNormal (None, 12, 96)       384         conv5_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block19_1_relu (Activatio (None, 12, 96)       0           conv5_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block19_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block19_concat (Concatena (None, 12, 1128)     0           conv5_block18_concat[0][0]       \n",
      "                                                                 conv5_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block20_0_bn (BatchNormal (None, 12, 1128)     4512        conv5_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block20_0_relu (Activatio (None, 12, 1128)     0           conv5_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block20_1_conv (Conv1D)   (None, 12, 96)       108384      conv5_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block20_1_bn (BatchNormal (None, 12, 96)       384         conv5_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block20_1_relu (Activatio (None, 12, 96)       0           conv5_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block20_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block20_concat (Concatena (None, 12, 1152)     0           conv5_block19_concat[0][0]       \n",
      "                                                                 conv5_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block21_0_bn (BatchNormal (None, 12, 1152)     4608        conv5_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block21_0_relu (Activatio (None, 12, 1152)     0           conv5_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block21_1_conv (Conv1D)   (None, 12, 96)       110688      conv5_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block21_1_bn (BatchNormal (None, 12, 96)       384         conv5_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block21_1_relu (Activatio (None, 12, 96)       0           conv5_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block21_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block21_concat (Concatena (None, 12, 1176)     0           conv5_block20_concat[0][0]       \n",
      "                                                                 conv5_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block22_0_bn (BatchNormal (None, 12, 1176)     4704        conv5_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block22_0_relu (Activatio (None, 12, 1176)     0           conv5_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block22_1_conv (Conv1D)   (None, 12, 96)       112992      conv5_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block22_1_bn (BatchNormal (None, 12, 96)       384         conv5_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block22_1_relu (Activatio (None, 12, 96)       0           conv5_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block22_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block22_concat (Concatena (None, 12, 1200)     0           conv5_block21_concat[0][0]       \n",
      "                                                                 conv5_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block23_0_bn (BatchNormal (None, 12, 1200)     4800        conv5_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block23_0_relu (Activatio (None, 12, 1200)     0           conv5_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block23_1_conv (Conv1D)   (None, 12, 96)       115296      conv5_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block23_1_bn (BatchNormal (None, 12, 96)       384         conv5_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block23_1_relu (Activatio (None, 12, 96)       0           conv5_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block23_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block23_concat (Concatena (None, 12, 1224)     0           conv5_block22_concat[0][0]       \n",
      "                                                                 conv5_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block24_0_bn (BatchNormal (None, 12, 1224)     4896        conv5_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block24_0_relu (Activatio (None, 12, 1224)     0           conv5_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block24_1_conv (Conv1D)   (None, 12, 96)       117600      conv5_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block24_1_bn (BatchNormal (None, 12, 96)       384         conv5_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block24_1_relu (Activatio (None, 12, 96)       0           conv5_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block24_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block24_concat (Concatena (None, 12, 1248)     0           conv5_block23_concat[0][0]       \n",
      "                                                                 conv5_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block25_0_bn (BatchNormal (None, 12, 1248)     4992        conv5_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block25_0_relu (Activatio (None, 12, 1248)     0           conv5_block25_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block25_1_conv (Conv1D)   (None, 12, 96)       119904      conv5_block25_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block25_1_bn (BatchNormal (None, 12, 96)       384         conv5_block25_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block25_1_relu (Activatio (None, 12, 96)       0           conv5_block25_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block25_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block25_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block25_concat (Concatena (None, 12, 1272)     0           conv5_block24_concat[0][0]       \n",
      "                                                                 conv5_block25_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block26_0_bn (BatchNormal (None, 12, 1272)     5088        conv5_block25_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block26_0_relu (Activatio (None, 12, 1272)     0           conv5_block26_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block26_1_conv (Conv1D)   (None, 12, 96)       122208      conv5_block26_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block26_1_bn (BatchNormal (None, 12, 96)       384         conv5_block26_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block26_1_relu (Activatio (None, 12, 96)       0           conv5_block26_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block26_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block26_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block26_concat (Concatena (None, 12, 1296)     0           conv5_block25_concat[0][0]       \n",
      "                                                                 conv5_block26_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block27_0_bn (BatchNormal (None, 12, 1296)     5184        conv5_block26_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block27_0_relu (Activatio (None, 12, 1296)     0           conv5_block27_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block27_1_conv (Conv1D)   (None, 12, 96)       124512      conv5_block27_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block27_1_bn (BatchNormal (None, 12, 96)       384         conv5_block27_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block27_1_relu (Activatio (None, 12, 96)       0           conv5_block27_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block27_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block27_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block27_concat (Concatena (None, 12, 1320)     0           conv5_block26_concat[0][0]       \n",
      "                                                                 conv5_block27_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block28_0_bn (BatchNormal (None, 12, 1320)     5280        conv5_block27_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block28_0_relu (Activatio (None, 12, 1320)     0           conv5_block28_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block28_1_conv (Conv1D)   (None, 12, 96)       126816      conv5_block28_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block28_1_bn (BatchNormal (None, 12, 96)       384         conv5_block28_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block28_1_relu (Activatio (None, 12, 96)       0           conv5_block28_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block28_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block28_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block28_concat (Concatena (None, 12, 1344)     0           conv5_block27_concat[0][0]       \n",
      "                                                                 conv5_block28_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block29_0_bn (BatchNormal (None, 12, 1344)     5376        conv5_block28_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block29_0_relu (Activatio (None, 12, 1344)     0           conv5_block29_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block29_1_conv (Conv1D)   (None, 12, 96)       129120      conv5_block29_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block29_1_bn (BatchNormal (None, 12, 96)       384         conv5_block29_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block29_1_relu (Activatio (None, 12, 96)       0           conv5_block29_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block29_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block29_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block29_concat (Concatena (None, 12, 1368)     0           conv5_block28_concat[0][0]       \n",
      "                                                                 conv5_block29_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block30_0_bn (BatchNormal (None, 12, 1368)     5472        conv5_block29_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block30_0_relu (Activatio (None, 12, 1368)     0           conv5_block30_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block30_1_conv (Conv1D)   (None, 12, 96)       131424      conv5_block30_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block30_1_bn (BatchNormal (None, 12, 96)       384         conv5_block30_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block30_1_relu (Activatio (None, 12, 96)       0           conv5_block30_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block30_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block30_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block30_concat (Concatena (None, 12, 1392)     0           conv5_block29_concat[0][0]       \n",
      "                                                                 conv5_block30_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block31_0_bn (BatchNormal (None, 12, 1392)     5568        conv5_block30_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block31_0_relu (Activatio (None, 12, 1392)     0           conv5_block31_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block31_1_conv (Conv1D)   (None, 12, 96)       133728      conv5_block31_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block31_1_bn (BatchNormal (None, 12, 96)       384         conv5_block31_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block31_1_relu (Activatio (None, 12, 96)       0           conv5_block31_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block31_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block31_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block31_concat (Concatena (None, 12, 1416)     0           conv5_block30_concat[0][0]       \n",
      "                                                                 conv5_block31_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block32_0_bn (BatchNormal (None, 12, 1416)     5664        conv5_block31_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block32_0_relu (Activatio (None, 12, 1416)     0           conv5_block32_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block32_1_conv (Conv1D)   (None, 12, 96)       136032      conv5_block32_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block32_1_bn (BatchNormal (None, 12, 96)       384         conv5_block32_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block32_1_relu (Activatio (None, 12, 96)       0           conv5_block32_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block32_2_conv (Conv1D)   (None, 12, 24)       6936        conv5_block32_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block32_concat (Concatena (None, 12, 1440)     0           conv5_block31_concat[0][0]       \n",
      "                                                                 conv5_block32_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 12, 1440)     5760        conv5_block32_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 12, 1440)     0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling1 (None, 1440)         0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc (Dense)                      (None, 3)            4323        avg_pool[0][0]                   \n",
      "==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 9,048,243\n",
      "Trainable params: 8,876,547\n",
      "Non-trainable params: 171,696\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2160 samples, validate on 1080 samples\n",
      "Epoch 1/300\n",
      "epoch:  0\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 96s 45ms/step - loss: 0.2224 - acc: 0.9032 - prec: 0.9792 - recall: 0.9668 - prec_1: 0.8560 - recall_1: 0.9030 - prec_2: 0.9196 - recall_2: 0.8541 - f1_m: 0.8891 - recall_m: 0.8778 - precision_m: 0.9172 - val_loss: 0.6329 - val_acc: 0.8833 - val_prec: 0.3556 - val_recall: 0.3500 - val_prec_1: 0.4148 - val_recall_1: 0.2948 - val_prec_2: 0.3426 - val_recall_2: 0.3370 - val_f1_m: 0.8626 - val_recall_m: 0.8417 - val_precision_m: 0.8876\n",
      "Epoch 2/300\n",
      "epoch:  1\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.1419 - acc: 0.9500 - prec: 0.9791 - recall: 0.9827 - prec_1: 0.9201 - recall_1: 0.9305 - prec_2: 0.9566 - recall_2: 0.9362 - f1_m: 0.9490 - recall_m: 0.9481 - precision_m: 0.9498 - val_loss: 0.8850 - val_acc: 0.7278 - val_prec: 0.3556 - val_recall: 0.1065 - val_prec_1: 0.3806 - val_recall_1: 0.3664 - val_prec_2: 0.3447 - val_recall_2: 0.3454 - val_f1_m: 0.7280 - val_recall_m: 0.7269 - val_precision_m: 0.7292\n",
      "Epoch 3/300\n",
      "epoch:  2\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0657 - acc: 0.9787 - prec: 0.9967 - recall: 1.0000 - prec_1: 0.9662 - recall_1: 0.9744 - prec_2: 0.9729 - recall_2: 0.9681 - f1_m: 0.9785 - recall_m: 0.9782 - precision_m: 0.9787 - val_loss: 1.2849 - val_acc: 0.7444 - val_prec: 0.3556 - val_recall: 0.1065 - val_prec_1: 0.3813 - val_recall_1: 0.3948 - val_prec_2: 0.3481 - val_recall_2: 0.3435 - val_f1_m: 0.7444 - val_recall_m: 0.7444 - val_precision_m: 0.7444\n",
      "Epoch 4/300\n",
      "epoch:  3\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0531 - acc: 0.9833 - prec: 0.9938 - recall: 0.9951 - prec_1: 0.9699 - recall_1: 0.9809 - prec_2: 0.9841 - recall_2: 0.9772 - f1_m: 0.9836 - recall_m: 0.9833 - precision_m: 0.9838 - val_loss: 0.3863 - val_acc: 0.7259 - val_prec: 0.3556 - val_recall: 0.1320 - val_prec_1: 0.3596 - val_recall_1: 0.4148 - val_prec_2: 0.3481 - val_recall_2: 0.2784 - val_f1_m: 0.7488 - val_recall_m: 0.7009 - val_precision_m: 0.8658\n",
      "Epoch 5/300\n",
      "epoch:  4\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 0.0299 - acc: 0.9907 - prec: 0.9909 - recall: 1.0000 - prec_1: 0.9859 - recall_1: 0.9883 - prec_2: 0.9931 - recall_2: 0.9857 - f1_m: 0.9907 - recall_m: 0.9907 - precision_m: 0.9907 - val_loss: 0.0443 - val_acc: 0.9917 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3417 - val_f1_m: 0.9912 - val_recall_m: 0.9907 - val_precision_m: 0.9917\n",
      "Epoch 6/300\n",
      "epoch:  5\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0440 - acc: 0.9847 - prec: 0.9919 - recall: 0.9927 - prec_1: 0.9734 - recall_1: 0.9847 - prec_2: 0.9863 - recall_2: 0.9841 - f1_m: 0.9847 - recall_m: 0.9838 - precision_m: 0.9856 - val_loss: 0.1198 - val_acc: 0.9880 - val_prec: 0.3556 - val_recall: 0.3537 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3398 - val_f1_m: 0.9889 - val_recall_m: 0.9880 - val_precision_m: 0.9898\n",
      "Epoch 7/300\n",
      "epoch:  6\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0208 - acc: 0.9949 - prec: 0.9984 - recall: 1.0000 - prec_1: 0.9906 - recall_1: 0.9942 - prec_2: 0.9937 - recall_2: 0.9898 - f1_m: 0.9947 - recall_m: 0.9944 - precision_m: 0.9949 - val_loss: 0.1769 - val_acc: 0.9296 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.3586 - val_prec_2: 0.3436 - val_recall_2: 0.3213 - val_f1_m: 0.9296 - val_recall_m: 0.9296 - val_precision_m: 0.9296\n",
      "Epoch 8/300\n",
      "epoch:  7\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0186 - acc: 0.9944 - prec: 0.9951 - recall: 1.0000 - prec_1: 0.9885 - recall_1: 0.9953 - prec_2: 0.9988 - recall_2: 0.9891 - f1_m: 0.9947 - recall_m: 0.9944 - precision_m: 0.9949 - val_loss: 0.9553 - val_acc: 0.8074 - val_prec: 0.3556 - val_recall: 0.2856 - val_prec_1: 0.4148 - val_recall_1: 0.2806 - val_prec_2: 0.3426 - val_recall_2: 0.3287 - val_f1_m: 0.7966 - val_recall_m: 0.7843 - val_precision_m: 0.8112\n",
      "Epoch 9/300\n",
      "epoch:  8\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0307 - acc: 0.9898 - prec: 0.9986 - recall: 1.0000 - prec_1: 0.9844 - recall_1: 0.9883 - prec_2: 0.9837 - recall_2: 0.9834 - f1_m: 0.9898 - recall_m: 0.9898 - precision_m: 0.9898 - val_loss: 0.0890 - val_acc: 0.9713 - val_prec: 0.3556 - val_recall: 0.3546 - val_prec_1: 0.4148 - val_recall_1: 0.3775 - val_prec_2: 0.3447 - val_recall_2: 0.3463 - val_f1_m: 0.9708 - val_recall_m: 0.9704 - val_precision_m: 0.9713\n",
      "Epoch 10/300\n",
      "epoch:  9\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0273 - acc: 0.9926 - prec: 0.9953 - recall: 1.0000 - prec_1: 0.9875 - recall_1: 0.9937 - prec_2: 0.9918 - recall_2: 0.9857 - f1_m: 0.9928 - recall_m: 0.9926 - precision_m: 0.9931 - val_loss: 0.0277 - val_acc: 0.9935 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4086 - val_prec_2: 0.3481 - val_recall_2: 0.3463 - val_f1_m: 0.9930 - val_recall_m: 0.9926 - val_precision_m: 0.9935\n",
      "Epoch 11/300\n",
      "epoch:  10\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 0.0216 - acc: 0.9940 - prec: 0.9967 - recall: 1.0000 - prec_1: 0.9894 - recall_1: 0.9940 - prec_2: 0.9940 - recall_2: 0.9900 - f1_m: 0.9937 - recall_m: 0.9935 - precision_m: 0.9940 - val_loss: 0.0232 - val_acc: 0.9917 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4077 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9925 - val_recall_m: 0.9917 - val_precision_m: 0.9934\n",
      "Epoch 12/300\n",
      "epoch:  11\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0177 - acc: 0.9935 - prec: 0.9973 - recall: 0.9990 - prec_1: 0.9880 - recall_1: 0.9949 - prec_2: 0.9945 - recall_2: 0.9892 - f1_m: 0.9937 - recall_m: 0.9935 - precision_m: 0.9940 - val_loss: 0.0215 - val_acc: 0.9926 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3435 - val_f1_m: 0.9926 - val_recall_m: 0.9926 - val_precision_m: 0.9926\n",
      "Epoch 13/300\n",
      "epoch:  12\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0172 - acc: 0.9944 - prec: 0.9986 - recall: 1.0000 - prec_1: 0.9907 - recall_1: 0.9910 - prec_2: 0.9916 - recall_2: 0.9929 - f1_m: 0.9944 - recall_m: 0.9944 - precision_m: 0.9944 - val_loss: 0.1190 - val_acc: 0.9537 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4082 - val_recall_1: 0.4148 - val_prec_2: 0.3481 - val_recall_2: 0.3012 - val_f1_m: 0.9537 - val_recall_m: 0.9537 - val_precision_m: 0.9537\n",
      "Epoch 14/300\n",
      "epoch:  13\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0167 - acc: 0.9935 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9917 - recall_1: 0.9901 - prec_2: 0.9874 - recall_2: 0.9922 - f1_m: 0.9937 - recall_m: 0.9935 - precision_m: 0.9940 - val_loss: 0.0202 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9940 - val_recall_m: 0.9935 - val_precision_m: 0.9944\n",
      "Epoch 15/300\n",
      "epoch:  14\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0151 - acc: 0.9968 - prec: 0.9970 - recall: 1.0000 - prec_1: 0.9961 - recall_1: 0.9964 - prec_2: 0.9965 - recall_2: 0.9942 - f1_m: 0.9968 - recall_m: 0.9968 - precision_m: 0.9968 - val_loss: 0.0973 - val_acc: 0.9713 - val_prec: 0.3556 - val_recall: 0.3546 - val_prec_1: 0.4148 - val_recall_1: 0.3910 - val_prec_2: 0.3481 - val_recall_2: 0.3426 - val_f1_m: 0.9713 - val_recall_m: 0.9713 - val_precision_m: 0.9713\n",
      "Epoch 16/300\n",
      "epoch:  15\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0101 - acc: 0.9968 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9921 - recall_1: 0.9973 - prec_2: 0.9972 - recall_2: 0.9934 - f1_m: 0.9968 - recall_m: 0.9968 - precision_m: 0.9968 - val_loss: 0.0982 - val_acc: 0.9880 - val_prec: 0.3556 - val_recall: 0.3481 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9883 - val_recall_m: 0.9870 - val_precision_m: 0.9897\n",
      "Epoch 17/300\n",
      "epoch:  16\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0082 - acc: 0.9972 - prec: 0.9984 - recall: 1.0000 - prec_1: 0.9952 - recall_1: 0.9961 - prec_2: 0.9965 - recall_2: 0.9964 - f1_m: 0.9972 - recall_m: 0.9972 - precision_m: 0.9972 - val_loss: 0.0386 - val_acc: 0.9917 - val_prec: 0.3556 - val_recall: 0.3528 - val_prec_1: 0.4148 - val_recall_1: 0.4099 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9917 - val_recall_m: 0.9917 - val_precision_m: 0.9917\n",
      "Epoch 18/300\n",
      "epoch:  17\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0093 - acc: 0.9972 - prec: 0.9980 - recall: 1.0000 - prec_1: 0.9954 - recall_1: 0.9988 - prec_2: 0.9986 - recall_2: 0.9935 - f1_m: 0.9972 - recall_m: 0.9972 - precision_m: 0.9972 - val_loss: 0.0248 - val_acc: 0.9935 - val_prec: 0.3556 - val_recall: 0.3546 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9940 - val_recall_m: 0.9935 - val_precision_m: 0.9944\n",
      "Epoch 19/300\n",
      "epoch:  18\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0164 - acc: 0.9944 - prec: 0.9988 - recall: 1.0000 - prec_1: 0.9928 - recall_1: 0.9948 - prec_2: 0.9933 - recall_2: 0.9922 - f1_m: 0.9942 - recall_m: 0.9940 - precision_m: 0.9944 - val_loss: 0.0261 - val_acc: 0.9926 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4080 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9926 - val_recall_m: 0.9917 - val_precision_m: 0.9935\n",
      "Epoch 20/300\n",
      "epoch:  19\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0107 - acc: 0.9958 - prec: 0.9955 - recall: 0.9991 - prec_1: 0.9907 - recall_1: 0.9975 - prec_2: 0.9983 - recall_2: 0.9923 - f1_m: 0.9963 - recall_m: 0.9958 - precision_m: 0.9968 - val_loss: 0.0193 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 21/300\n",
      "epoch:  20\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0031 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9977 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9988 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0194 - val_acc: 0.9935 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9935 - val_recall_m: 0.9935 - val_precision_m: 0.9935\n",
      "Epoch 22/300\n",
      "epoch:  21\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0039 - acc: 0.9986 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9956 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9960 - f1_m: 0.9988 - recall_m: 0.9986 - precision_m: 0.9991 - val_loss: 0.0181 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 23/300\n",
      "epoch:  22\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0044 - acc: 0.9991 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9979 - recall_1: 0.9989 - prec_2: 0.9984 - recall_2: 0.9985 - f1_m: 0.9991 - recall_m: 0.9991 - precision_m: 0.9991 - val_loss: 0.0165 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 24/300\n",
      "epoch:  23\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0041 - acc: 0.9991 - prec: 0.9981 - recall: 0.9988 - prec_1: 0.9986 - recall_1: 0.9990 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 0.9991 - recall_m: 0.9991 - precision_m: 0.9991 - val_loss: 0.0132 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 25/300\n",
      "epoch:  24\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0035 - acc: 0.9986 - prec: 0.9987 - recall: 1.0000 - prec_1: 0.9986 - recall_1: 0.9976 - prec_2: 0.9981 - recall_2: 0.9986 - f1_m: 0.9986 - recall_m: 0.9986 - precision_m: 0.9986 - val_loss: 0.0111 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9958 - val_recall_m: 0.9954 - val_precision_m: 0.9963\n",
      "Epoch 26/300\n",
      "epoch:  25\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0028 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 0.9990 - prec_2: 0.9985 - recall_2: 1.0000 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0134 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 27/300\n",
      "epoch:  26\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0028 - acc: 0.9991 - prec: 0.9984 - recall: 1.0000 - prec_1: 0.9984 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9976 - f1_m: 0.9991 - recall_m: 0.9991 - precision_m: 0.9991 - val_loss: 0.0158 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 28/300\n",
      "epoch:  27\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.0029 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9979 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9987 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0135 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 29/300\n",
      "epoch:  28\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0016 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0131 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 30/300\n",
      "epoch:  29\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.0032 - acc: 0.9986 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9986 - recall_1: 0.9976 - prec_2: 0.9966 - recall_2: 0.9990 - f1_m: 0.9986 - recall_m: 0.9986 - precision_m: 0.9986 - val_loss: 0.0185 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 31/300\n",
      "epoch:  30\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.0083 - acc: 0.9963 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9941 - recall_1: 0.9948 - prec_2: 0.9927 - recall_2: 0.9944 - f1_m: 0.9965 - recall_m: 0.9963 - precision_m: 0.9968 - val_loss: 0.0200 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300\n",
      "epoch:  31\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0027 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9984 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9985 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0178 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3463 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 33/300\n",
      "epoch:  32\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 0.0051 - acc: 0.9986 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9983 - recall_1: 0.9978 - prec_2: 0.9968 - recall_2: 0.9988 - f1_m: 0.9986 - recall_m: 0.9986 - precision_m: 0.9986 - val_loss: 0.0244 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 34/300\n",
      "epoch:  33\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.0035 - acc: 0.9986 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9968 - recall_1: 0.9989 - prec_2: 0.9983 - recall_2: 0.9976 - f1_m: 0.9986 - recall_m: 0.9986 - precision_m: 0.9986 - val_loss: 0.0175 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 35/300\n",
      "epoch:  34\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 0.0016 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 0.9990 - prec_2: 0.9986 - recall_2: 1.0000 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0207 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 36/300\n",
      "epoch:  35\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 0.0024 - acc: 0.9991 - prec: 0.9977 - recall: 1.0000 - prec_1: 0.9984 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9976 - f1_m: 0.9993 - recall_m: 0.9991 - precision_m: 0.9995 - val_loss: 0.0154 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4148 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 37/300\n",
      "epoch:  36\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 0.0022 - acc: 0.9991 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9979 - recall_1: 0.9989 - prec_2: 0.9981 - recall_2: 0.9990 - f1_m: 0.9991 - recall_m: 0.9991 - precision_m: 0.9991 - val_loss: 0.0139 - val_acc: 0.9935 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9935 - val_recall_m: 0.9935 - val_precision_m: 0.9935\n",
      "Epoch 38/300\n",
      "epoch:  37\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 0.0027 - acc: 0.9991 - prec: 0.9973 - recall: 1.0000 - prec_1: 0.9979 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9972 - f1_m: 0.9991 - recall_m: 0.9991 - precision_m: 0.9991 - val_loss: 0.0411 - val_acc: 0.9898 - val_prec: 0.3556 - val_recall: 0.3528 - val_prec_1: 0.4148 - val_recall_1: 0.4071 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9898 - val_recall_m: 0.9889 - val_precision_m: 0.9907\n",
      "Epoch 39/300\n",
      "epoch:  38\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.0026 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 0.9990 - prec_2: 0.9980 - recall_2: 1.0000 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0670 - val_acc: 0.9917 - val_prec: 0.3556 - val_recall: 0.3519 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9912 - val_recall_m: 0.9907 - val_precision_m: 0.9917\n",
      "Epoch 40/300\n",
      "epoch:  39\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 0.0036 - acc: 0.9981 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9955 - recall_1: 0.9988 - prec_2: 0.9983 - recall_2: 0.9959 - f1_m: 0.9981 - recall_m: 0.9981 - precision_m: 0.9981 - val_loss: 0.0178 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4148 - val_prec_2: 0.3481 - val_recall_2: 0.3435 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 41/300\n",
      "epoch:  40\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.0015 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0179 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 42/300\n",
      "epoch:  41\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 0.0014 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 0.9998 - recall_m: 0.9995 - precision_m: 1.0000 - val_loss: 0.0188 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 43/300\n",
      "epoch:  42\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 5.4967e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0178 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 44/300\n",
      "epoch:  43\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 4.0792e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0165 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 45/300\n",
      "epoch:  44\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 8.7539e-04 - acc: 0.9995 - prec: 0.9981 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9987 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0163 - val_acc: 0.9935 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9935 - val_recall_m: 0.9935 - val_precision_m: 0.9935\n",
      "Epoch 46/300\n",
      "epoch:  45\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.0020 - acc: 0.9991 - prec: 1.0000 - recall: 0.9986 - prec_1: 0.9983 - recall_1: 1.0000 - prec_2: 0.9990 - recall_2: 0.9989 - f1_m: 0.9991 - recall_m: 0.9991 - precision_m: 0.9991 - val_loss: 0.0159 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 47/300\n",
      "epoch:  46\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 6.9938e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0155 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300\n",
      "epoch:  47\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 8.0339e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0143 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 49/300\n",
      "epoch:  48\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0010 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0147 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 50/300\n",
      "epoch:  49\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 3.7166e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0147 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 51/300\n",
      "epoch:  50\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 3.8962e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0146 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 52/300\n",
      "epoch:  51\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 3.4151e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0149 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 53/300\n",
      "epoch:  52\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 6.2572e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0139 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 54/300\n",
      "epoch:  53\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 2.7459e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0160 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 55/300\n",
      "epoch:  54\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 3.0968e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0157 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 56/300\n",
      "epoch:  55\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 3.1023e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0153 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 57/300\n",
      "epoch:  56\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 2.7746e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0161 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 58/300\n",
      "epoch:  57\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 2.7946e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0173 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 59/300\n",
      "epoch:  58\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 3.2954e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0164 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 60/300\n",
      "epoch:  59\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 3.7174e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0164 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 61/300\n",
      "epoch:  60\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.4960e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0164 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 62/300\n",
      "epoch:  61\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 3.7303e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0171 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 63/300\n",
      "epoch:  62\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 14s 6ms/step - loss: 0.0015 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9977 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9987 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0136 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4130 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 64/300\n",
      "epoch:  63\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.8499e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0132 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 65/300\n",
      "epoch:  64\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0013 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 0.9989 - prec_2: 0.9977 - recall_2: 1.0000 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0144 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 66/300\n",
      "epoch:  65\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 0.0023 - acc: 0.9991 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 0.9981 - prec_2: 0.9934 - recall_2: 1.0000 - f1_m: 0.9991 - recall_m: 0.9991 - precision_m: 0.9991 - val_loss: 0.0243 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 67/300\n",
      "epoch:  66\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0013 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9977 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9988 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0232 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 68/300\n",
      "epoch:  67\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0020 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9988 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9990 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0153 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 69/300\n",
      "epoch:  68\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 2.5529e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0150 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 70/300\n",
      "epoch:  69\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 2.4918e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0158 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 71/300\n",
      "epoch:  70\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 4.3734e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0174 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 72/300\n",
      "epoch:  71\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.9668e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0163 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 73/300\n",
      "epoch:  72\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 2.0372e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0157 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 74/300\n",
      "epoch:  73\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 2.1449e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0152 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 75/300\n",
      "epoch:  74\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 7ms/step - loss: 1.8872e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0151 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 76/300\n",
      "epoch:  75\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 1.7740e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0158 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 77/300\n",
      "epoch:  76\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 1.9999e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0166 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 78/300\n",
      "epoch:  77\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.0013 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9983 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9989 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0161 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300\n",
      "epoch:  78\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 1.9014e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0164 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 80/300\n",
      "epoch:  79\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 1.5324e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0164 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 81/300\n",
      "epoch:  80\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 0.0015 - acc: 0.9995 - prec: 0.9980 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9989 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0159 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 82/300\n",
      "epoch:  81\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 5.2802e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0165 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 83/300\n",
      "epoch:  82\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 4.5197e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0161 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 84/300\n",
      "epoch:  83\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 1.9706e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0169 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 85/300\n",
      "epoch:  84\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 0.0025 - acc: 0.9991 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9965 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9984 - f1_m: 0.9991 - recall_m: 0.9991 - precision_m: 0.9991 - val_loss: 0.0122 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 86/300\n",
      "epoch:  85\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 7.6304e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0152 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 87/300\n",
      "epoch:  86\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 2.3432e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0154 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 88/300\n",
      "epoch:  87\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 3.6192e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0169 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 89/300\n",
      "epoch:  88\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 2.4548e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0187 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 90/300\n",
      "epoch:  89\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 1.4109e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0188 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 91/300\n",
      "epoch:  90\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 7.9810e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0234 - val_acc: 0.9935 - val_prec: 0.3556 - val_recall: 0.3546 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9935 - val_recall_m: 0.9935 - val_precision_m: 0.9935\n",
      "Epoch 92/300\n",
      "epoch:  91\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 2.2738e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0206 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 93/300\n",
      "epoch:  92\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.7687e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0191 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 94/300\n",
      "epoch:  93\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 13s 6ms/step - loss: 5.4489e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0216 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 95/300\n",
      "epoch:  94\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 1.9913e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0207 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 96/300\n",
      "epoch:  95\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 3.1899e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0210 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 97/300\n",
      "epoch:  96\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.5233e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0213 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 98/300\n",
      "epoch:  97\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 6.6699e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0195 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 99/300\n",
      "epoch:  98\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 1.7757e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0198 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 100/300\n",
      "epoch:  99\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0020 - acc: 0.9986 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9964 - recall_1: 0.9990 - prec_2: 0.9988 - recall_2: 0.9967 - f1_m: 0.9986 - recall_m: 0.9986 - precision_m: 0.9986 - val_loss: 0.0193 - val_acc: 0.9935 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4105 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9935 - val_recall_m: 0.9935 - val_precision_m: 0.9935\n",
      "Epoch 101/300\n",
      "epoch:  100\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0022 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9984 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9987 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0141 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 102/300\n",
      "epoch:  101\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 2.1289e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0137 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 103/300\n",
      "epoch:  102\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 1.7230e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0142 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 104/300\n",
      "epoch:  103\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 2.1239e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0147 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 105/300\n",
      "epoch:  104\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 2.9766e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0151 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 106/300\n",
      "epoch:  105\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.7515e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0155 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 107/300\n",
      "epoch:  106\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 2.8395e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0188 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 108/300\n",
      "epoch:  107\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.0033 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 0.9987 - prec_2: 0.9984 - recall_2: 1.0000 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0202 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4099 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 109/300\n",
      "epoch:  108\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 2.3987e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0218 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4123 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/300\n",
      "epoch:  109\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 3.8591e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0181 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 111/300\n",
      "epoch:  110\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 4.2294e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0136 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 112/300\n",
      "epoch:  111\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 2.1563e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0133 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 113/300\n",
      "epoch:  112\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 7.1326e-05 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0132 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 114/300\n",
      "epoch:  113\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 1.4458e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0130 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 115/300\n",
      "epoch:  114\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 1.0709e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0133 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 116/300\n",
      "epoch:  115\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 3.1018e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0131 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 117/300\n",
      "epoch:  116\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.2634e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0131 - val_acc: 0.9963 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9963 - val_recall_m: 0.9963 - val_precision_m: 0.9963\n",
      "Epoch 118/300\n",
      "epoch:  117\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.8196e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0138 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 119/300\n",
      "epoch:  118\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.4566e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0137 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 120/300\n",
      "epoch:  119\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.7291e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0137 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 121/300\n",
      "epoch:  120\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 1.0957e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0138 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 122/300\n",
      "epoch:  121\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 8.5692e-05 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0140 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 123/300\n",
      "epoch:  122\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 9.1251e-05 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0146 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 124/300\n",
      "epoch:  123\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 6.6462e-05 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0145 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 125/300\n",
      "epoch:  124\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 13s 6ms/step - loss: 6.8554e-05 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0145 - val_acc: 0.9944 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9944 - val_recall_m: 0.9944 - val_precision_m: 0.9944\n",
      "Epoch 126/300\n",
      "epoch:  125\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 6.2768e-05 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0145 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 127/300\n",
      "epoch:  126\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 5.5169e-05 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0144 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 128/300\n",
      "epoch:  127\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 6.8432e-05 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000 - val_loss: 0.0147 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4139 - val_prec_2: 0.3481 - val_recall_2: 0.3444 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 129/300\n",
      "epoch:  128\n",
      "Learning rate:  0.01\n",
      "2160/2160 [==============================] - 12s 6ms/step - loss: 5.8727e-04 - acc: 0.9995 - prec: 1.0000 - recall: 1.0000 - prec_1: 0.9986 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 0.9989 - f1_m: 0.9995 - recall_m: 0.9995 - precision_m: 0.9995 - val_loss: 0.0190 - val_acc: 0.9954 - val_prec: 0.3556 - val_recall: 0.3556 - val_prec_1: 0.4148 - val_recall_1: 0.4114 - val_prec_2: 0.3481 - val_recall_2: 0.3454 - val_f1_m: 0.9954 - val_recall_m: 0.9954 - val_precision_m: 0.9954\n",
      "Epoch 130/300\n",
      "epoch:  129\n",
      "Learning rate:  0.01\n",
      " 512/2160 [======>.......................] - ETA: 8s - loss: 1.2297e-04 - acc: 1.0000 - prec: 1.0000 - recall: 1.0000 - prec_1: 1.0000 - recall_1: 1.0000 - prec_2: 1.0000 - recall_2: 1.0000 - f1_m: 1.0000 - recall_m: 1.0000 - precision_m: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0285c0e68e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#                         workers=4,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                         callbacks=callbacks))\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mmy_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyqt/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/pyqt/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyqt/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyqt/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyqt/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "batch_size = 64\n",
    "split_val = 0.3\n",
    "fit_shuffle = True\n",
    "optimizer = 'sgd'\n",
    "start_lr = 1e-2\n",
    "\n",
    "split_shuffle = False\n",
    "history = []\n",
    "my_models = []\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "        lr = start_lr\n",
    "        if epoch > 225:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 150:\n",
    "            lr *= 1e-1\n",
    "        print('epoch: ', epoch)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "sgd = SGD(lr=lr_schedule(0), decay=1e-4, momentum=0.9, nesterov=True)\n",
    "# adam = Adam(lr=lr_schedule(0))\n",
    "# nAdam = Nadam(lr=lr_schedule(0))\n",
    "\n",
    "\n",
    "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# modelv3.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "#                                                                          f1_m, recall_m, precision_m])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# Instantiate the cross validator\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "# Loop through the indices the split() method returns\n",
    "\n",
    "for index, (train_indices, val_indices) in enumerate(skf.split(x_data, y_data)):\n",
    "\n",
    "#     print(x[train_indices], x[val_indices])\n",
    "#     print(y[train_indices], y[val_indices])\n",
    "    xtrain, xval = x[train_indices], x[val_indices]\n",
    "    ytrain, yval = y[train_indices], y[val_indices]\n",
    "    \n",
    "    model = None\n",
    "    model = createModel()\n",
    "    \n",
    "    csv_logger_path = save_path+f'/{model.name}(batch_size={batch_size}, split_val={split_val}, fit_shuffle={fit_shuffle}, split_shuffle={split_shuffle}, optimizer={optimizer}, lr=s:{start_lr}, per:1e-1, e:(150, 225))_log.csv'\n",
    "    if not os.path.isdir(os.path.dirname(csv_logger_path)):\n",
    "        os.makedirs(os.path.dirname(csv_logger_path))\n",
    "    csv_logger = CSVLogger(csv_logger_path, append=True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "    callbacks = [lr_scheduler, csv_logger]\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "                                                              \n",
    "                                                                single_class_precision(0), single_class_recall(0),\n",
    "                                                                single_class_precision(1), single_class_recall(1),\n",
    "                                                                single_class_precision(2), single_class_recall(2),\n",
    "#                                                                 single_class_precision(3), single_class_recall(3),\n",
    "#                                                                 single_class_precision(4), single_class_recall(4),\n",
    "#                                                                 single_class_precision(5), single_class_recall(5),\n",
    "#                                                                 single_class_precision(6), single_class_recall(6),\n",
    "#                                                                 single_class_precision(7), single_class_recall(7),\n",
    "#                                                                 single_class_precision(8), single_class_recall(8)\n",
    "                                                                            f1_m, recall_m, precision_m,\n",
    "                                                                          ]) \n",
    "    \n",
    "    \n",
    "    history.append(model.fit(xtrain, ytrain, \n",
    "                        epochs=300, batch_size=batch_size, \n",
    "                        shuffle=fit_shuffle,\n",
    "#                         workers=4, \n",
    "                        validation_data=(xval, yval),\n",
    "                        callbacks=callbacks))\n",
    "\n",
    "    my_models.append(model)\n",
    "\n",
    "# # 4. 모델 학습시키기\n",
    "# history = modelv3.fit(x_train, y_train, \n",
    "#                     epochs=300, batch_size=batch_size, \n",
    "#                     shuffle=fit_shuffle,\n",
    "# #                     workers=1, \n",
    "#                       validation_data=(x_test, y_test),\n",
    "#                     callbacks=callbacks)\n",
    "\n",
    "# histories[model] = models[model].fit(x_train, y_train, epochs=300, batch_size=32, workers=4, validation_data=(x_train, y_train))\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "# evaluate = modelv3.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate = modelv3.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(\"loss     acc      f1      recall      precision\")\n",
    "print(evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hist in history:\n",
    "    for key, value in hist.history.items():\n",
    "        if key == \"val_acc\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_f1_m\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_recall_m\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_precision_m\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_prec\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_recall\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_prec_1\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_recall_1\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_prec_2\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_recall_2\":\n",
    "            value = max(value)\n",
    "            print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.35555555555555557 + 0.4148148148148148 + "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ps8QIuN_mWhs"
   },
   "source": [
    "# version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiBxlLW7H-WZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# inputs = Input(shape=(100, 1))\n",
    "\n",
    "# modelv1 = DcnnNetv1(depth=100, growthRate=32, include_top=True, input_tensor=inputs, pooling='avg', classes=9)\n",
    "# modelv1.summary()\n",
    "# modelv1.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfsjvrWu9jjn"
   },
   "source": [
    "# v1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "9AK0uGhL9nwy",
    "outputId": "1914d5b7-aae1-499f-e744-f790287e3573"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "batch_size = 64\n",
    "split_val = 0.3\n",
    "fit_shuffle = True\n",
    "optimizer = 'sgd'\n",
    "start_lr = 1e-2\n",
    "split_shuffle = False\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "        lr = start_lr\n",
    "        if epoch > 225:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 150:\n",
    "            lr *= 1e-1\n",
    "        print('epoch: ', epoch)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "sgd = SGD(lr=lr_schedule(0), decay=1e-4, momentum=0.9, nesterov=True)\n",
    "# adam = Adam(lr=lr_schedule(0))\n",
    "# nAdam = Nadam(lr=lr_schedule(0))\n",
    "\n",
    "\n",
    "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# models[model].compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "#                                                                                     single_class_precision(0), single_class_recall(0),\n",
    "#                                                                                     single_class_precision(1), single_class_recall(1),\n",
    "#                                                                                     single_class_precision(2), single_class_recall(2),\n",
    "#                                                                                     single_class_precision(3), single_class_recall(3),\n",
    "#                                                                                     single_class_precision(4), single_class_recall(4),\n",
    "#                                                                                     single_class_precision(5), single_class_recall(5),\n",
    "#                                                                                     single_class_precision(3), single_class_recall(6),\n",
    "#                                                                                     single_class_precision(4), single_class_recall(7),\n",
    "#                                                                                     single_class_precision(5), single_class_recall(8)])\n",
    "\n",
    "modelv1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "csv_logger_path = save_path+f'/{modelv1.name}(batch_size={batch_size}, split_val={split_val}, fit_shuffle={fit_shuffle}, split_shuffle={split_shuffle}, optimizer={optimizer}, lr=s:{start_lr}, per:1e-1, e:(150, 225))_log.csv'\n",
    "if not os.path.isdir(os.path.dirname(csv_logger_path)):\n",
    "    os.makedirs(os.path.dirname(csv_logger_path))\n",
    "csv_logger = CSVLogger(csv_logger_path, append=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "callbacks = [lr_scheduler, csv_logger]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "history = modelv1.fit(x_train, y_train, \n",
    "                                        epochs=300, batch_size=batch_size, \n",
    "                                        shuffle=fit_shuffle,\n",
    "                                        workers=4, validation_data=(x_test, y_test),\n",
    "                                        callbacks=callbacks)\n",
    "\n",
    "# histories[model] = models[model].fit(x_train, y_train, epochs=300, batch_size=32, workers=4, validation_data=(x_train, y_train))\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "evaluate = modelv1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "FquYdJ59SDb_",
    "outputId": "29ae5bd6-557a-4a85-a8db-67e0e53b0582"
   },
   "outputs": [],
   "source": [
    "# 6. 모델 평가하기\n",
    "evaluate = modelv1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jyXxNNjSSnb"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtIw2AGf9fu6"
   },
   "source": [
    "# version2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Y_wU9z4T69A5",
    "outputId": "6ee3d579-6f69-49dc-f9d0-6b939517a8ee"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(100, 1))\n",
    "\n",
    "modelv2 = DcnnNetv2(depth=201, growthRate=32, include_top=True, input_tensor=inputs, pooling='avg', classes=9)\n",
    "modelv2.summary()\n",
    "modelv2.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awvyPBrqLXzj"
   },
   "source": [
    "# version2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-jkUc-9jLTlF",
    "outputId": "d334086f-1d42-4722-9bdf-fb8489ea970e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "batch_size = 64\n",
    "split_val = 0.3\n",
    "fit_shuffle = True\n",
    "optimizer = 'sgd'\n",
    "start_lr = 1e-2\n",
    "\n",
    "split_shuffle = False\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "        lr = start_lr\n",
    "        if epoch > 225:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 150:\n",
    "            lr *= 1e-1\n",
    "        print('epoch: ', epoch)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "sgd = SGD(lr=lr_schedule(0), decay=1e-4, momentum=0.9, nesterov=True)\n",
    "# adam = Adam(lr=lr_schedule(0))\n",
    "# nAdam = Nadam(lr=lr_schedule(0))\n",
    "\n",
    "\n",
    "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# models[model].compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "#                                                                                     single_class_precision(0), single_class_recall(0),\n",
    "#                                                                                     single_class_precision(1), single_class_recall(1),\n",
    "#                                                                                     single_class_precision(2), single_class_recall(2),\n",
    "#                                                                                     single_class_precision(3), single_class_recall(3),\n",
    "#                                                                                     single_class_precision(4), single_class_recall(4),\n",
    "#                                                                                     single_class_precision(5), single_class_recall(5),\n",
    "#                                                                                     single_class_precision(3), single_class_recall(6),\n",
    "#                                                                                     single_class_precision(4), single_class_recall(7),\n",
    "#                                                                                     single_class_precision(5), single_class_recall(8)])\n",
    "\n",
    "modelv2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "                                                                         f1_m, recall_m, precision_m])\n",
    "\n",
    "csv_logger_path = save_path+f'/{modelv2.name}(batch_size={batch_size}, split_val={split_val}, fit_shuffle={fit_shuffle}, split_shuffle={split_shuffle}, optimizer={optimizer}, lr=s:{start_lr}, per:1e-1, e:(150, 225))_log.csv'\n",
    "if not os.path.isdir(os.path.dirname(csv_logger_path)):\n",
    "    os.makedirs(os.path.dirname(csv_logger_path))\n",
    "csv_logger = CSVLogger(csv_logger_path, append=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "callbacks = [lr_scheduler, csv_logger]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "history = modelv2.fit(x_train, y_train, \n",
    "                    epochs=300, batch_size=batch_size, \n",
    "                    shuffle=fit_shuffle,\n",
    "#                     workers=1, \n",
    "                      validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# histories[model] = models[model].fit(x_train, y_train, epochs=300, batch_size=32, workers=4, validation_data=(x_train, y_train))\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "evaluate = modelv2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "dz5gIfVHbSJW",
    "outputId": "c2a98073-fc17-4f62-fe73-3af81086b854"
   },
   "outputs": [],
   "source": [
    "evaluate = modelv2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(\"loss     acc      f1      recall      precision\")\n",
    "print(evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "qn-aGNVuO35L",
    "outputId": "2a0d756d-5860-4e55-d839-b9ca09ca341d"
   },
   "outputs": [],
   "source": [
    "epoch:  0\n",
    "Learning rate:  0.01\n",
    "Train on 2268 samples, validate on 972 samples\n",
    "Epoch 1/300\n",
    "epoch:  0\n",
    "Learning rate:  0.01\n",
    "2268/2268 [==============================] - 110s 48ms/step - loss: 0.5774 - acc: 0.7743 - f1_m: 0.7240 - recall_m: 0.6958 - precision_m: 0.7823 - val_loss: 8.3245 - val_acc: 0.3313 - val_f1_m: 0.3313 - val_recall_m: 0.3313 - val_precision_m: 0.3313\n",
    "Epoch 2/300\n",
    "epoch:  1\n",
    "Learning rate:  0.01\n",
    "2268/2268 [==============================] - 14s 6ms/step - loss: 0.1662 - acc: 0.9374 - f1_m: 0.9380 - recall_m: 0.9361 - precision_m: 0.9399 - val_loss: 0.8003 - val_acc: 0.7840 - val_f1_m: 0.7828 - val_recall_m: 0.7809 - val_precision_m: 0.7848\n",
    "Epoch 3/300\n",
    "epoch:  2\n",
    "Learning rate:  0.01\n",
    "2268/2268 [==============================] - 15s 7ms/step - loss: 0.1239 - acc: 0.9612 - f1_m: 0.9616 - recall_m: 0.9603 - precision_m: 0.9628 - val_loss: 5.2734 - val_acc: 0.3539 - val_f1_m: 0.3538 - val_recall_m: 0.3313 - val_precision_m: 0.3803\n",
    "Epoch 4/300\n",
    "epoch:  3\n",
    "Learning rate:  0.01\n",
    " 832/2268 [==========>...................] - ETA: 8s - loss: 0.1580 - acc: 0.9363 - f1_m: 0.9380 - recall_m: 0.9303 - precision_m: 0.9462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JtOOvKnfqt-"
   },
   "source": [
    "# version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlMAN-jlfphr"
   },
   "outputs": [],
   "source": [
    "img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "nChannels = 2 * growthRate\n",
    "bn_axis = 2\n",
    "\n",
    "x = layers.ZeroPadding1D(padding=1)(img_input)\n",
    "x = layers.Conv1D(nChannels, 3, strides=1, use_bias=False, name='conv1/conv')(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bg0XYdPOSDA4"
   },
   "source": [
    "# 실험 결과\n",
    "\n",
    "## version1\n",
    "\n",
    "depth = 100\n",
    "\n",
    "### concatenate\n",
    "```\n",
    "start lr = 0.01\n",
    "    972/972 [==============================] - 1s 975us/step\n",
    "    [0.26581049285380653, 0.9259259259259259]\n",
    "\n",
    "start lr = 0.1\n",
    "    972/972 [==============================] - 1s 976us/step\n",
    "    [0.43105335702636727, 0.9176954732510288]\n",
    "```\n",
    "\n",
    "\n",
    "### add\n",
    "\n",
    "```\n",
    "start lr = 0.01\n",
    "    972/972 [==============================] - 1s 520us/step\n",
    "    [0.3993826474181909, 0.8713991769547325]\n",
    "\n",
    "start lr = 0.1\n",
    "\n",
    "학습되지 않음\n",
    "```\n",
    "\n",
    "## version 2\n",
    "\n",
    "201 layer\n",
    "\n",
    "### concatenate\n",
    "\n",
    "```\n",
    "start lr = 0.1\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    [0.5396262257064811, 0.8446502060066035]\n",
    "Data normalization\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [2.230289366755466, 0.6306584359687052, 0.6331204848034392, 0.6255144030468944, 0.6410546889030394]\n",
    "min-max normalization\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [2.013901627603382, 0.6707818932494019, 0.6677523871017582, 0.6615226339901426, 0.674254747329916]\n",
    "\n",
    "start lr = 0.01\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [0.5466709941502952, 0.8631687245251219, 0.8630309512095197, 0.8621399179407598, 0.8639362478943028]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python \n",
    "start lr = 0.01\n",
    "x = layers.ZeroPadding1D(padding=3)(img_input)\n",
    "x = layers.Conv1D(nChannels, 7, strides=1, use_bias=False, name='conv1/conv')(x)\n",
    "\n",
    "# x = layers.BatchNormalization(\n",
    "#     axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "# x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "\n",
    "# x = layers.ZeroPadding1D(padding=1)(x)\n",
    "# x = layers.MaxPooling1D(3, strides=2, name='pool1')(x)\n",
    "\n",
    "    972/972 [==============================] - 2s 2ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [0.22257122489547493, 0.9454732510288066, 0.9459430514049137, 0.9454732510288066, 0.9464204066084245]\n",
    "\n",
    "x = layers.ZeroPadding1D(padding=3)(img_input)\n",
    "x = layers.Conv1D(nChannels, 7, strides=2, use_bias=False, name='conv1/conv')(x)\n",
    "\n",
    "# x = layers.BatchNormalization(\n",
    "#     axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "# x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "\n",
    "# x = layers.ZeroPadding1D(padding=1)(x)\n",
    "# x = layers.MaxPooling1D(3, strides=2, name='pool1')(x)\n",
    "\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [0.5289181569809623, 0.8837448559670782, 0.8851272049264162, 0.8816872427983539, 0.8886497324876824]\n",
    "\n",
    "```\n",
    "\n",
    "### add\n",
    "\n",
    "```\n",
    "start lr = 0.01\n",
    "    972/972 [==============================] - 1s 923us/step\n",
    "    [0.8464200724790125, 0.7129629632082496]\n",
    "\n",
    "\n",
    "start lr = 0.1\n",
    "    972/972 [==============================] - 1s 915us/step\n",
    "    [1.0798768680772663, 0.49074074074074076]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLgdTKS9XtSV"
   },
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "params = {\n",
    "    'batch_size' : [2, 4, 8, 16, 32, 64, 128, 256],\n",
    "    'fit_shuffle' : [False, True],\n",
    "    'split_shuffle' : [False, True],\n",
    "    'start_lr' : [1e-1, 1e-2, 1e-3],\n",
    "    'optimizer' : ['sgd','adam','nAdam'],\n",
    "    'split_val' : [0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "batch_size = params['batch_size'][0]\n",
    "fit_shuffle = params['fit_shuffle'][0]\n",
    "split_shuffle = params['split_shuffle'][0]\n",
    "start_lr = params['start_lr'][0]\n",
    "optimizer = params['optimizer'][0]\n",
    "split_val = params['split_val'][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_s-3f2pReqM"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential #스택형 모델\n",
    "from keras.models import Model #함수형 모델\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Gw7r_DpX-rs"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# 특정 클래스에 대한 정밀도\n",
    "def single_class_precision(interesting_class_id):\n",
    "    def prec(y_true, y_pred):\n",
    "        class_id_true = K.argmax(y_true, axis=-1)\n",
    "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
    "        precision_mask = K.cast(K.equal(class_id_pred, interesting_class_id), 'int32')\n",
    "        class_prec_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * precision_mask\n",
    "        class_prec = K.cast(K.sum(class_prec_tensor), 'float32') / K.cast(K.maximum(K.sum(precision_mask), 1), 'float32')\n",
    "        return class_prec\n",
    "    return prec\n",
    "\n",
    "\n",
    "# 특정 클래스에 대한 재현율\n",
    "def single_class_recall(interesting_class_id):\n",
    "    def recall(y_true, y_pred):\n",
    "        class_id_true = K.argmax(y_true, axis=-1)\n",
    "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
    "        recall_mask = K.cast(K.equal(class_id_true, interesting_class_id), 'int32')\n",
    "        class_recall_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * recall_mask\n",
    "        class_recall = K.cast(K.sum(class_recall_tensor), 'float32') / K.cast(K.maximum(K.sum(recall_mask), 1), 'float32')\n",
    "        return class_recall\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZa_m3V5_a-Z"
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "model_type = {\n",
    "    \"MLP\" : {},\n",
    "    \"LSTM\" : {},\n",
    "    \"CONV1D\" : {},\n",
    "    \"CONV1D+LSTM\" : {},\n",
    "    \"LSTM+CONV1D\" : {}\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "evaluates = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsbzcSNPubub"
   },
   "source": [
    "#MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyC9i6cgFK1-"
   },
   "outputs": [],
   "source": [
    "\n",
    "for ii in range(6):\n",
    "    iii = 0\n",
    "    inputs = Input(shape=(1, 100))\n",
    "    x = Dense(10, activation='relu')(inputs)\n",
    "    for iii in range(ii):\n",
    "        x = Dense(10, activation='relu')(x)\n",
    "    y = Dense(9, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    models[f\"[MLP]{ii+1}_layer\"] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5k2Zv6BeuwUe"
   },
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZ0AkepIuzmR"
   },
   "outputs": [],
   "source": [
    "\n",
    "for ii in range(6):\n",
    "    iii = 0\n",
    "    inputs = Input(shape=(1,100))\n",
    "    x = LSTM(10, return_sequences=True, activation=\"relu\")(inputs)\n",
    "    for iii in range(ii):\n",
    "        if iii != ii - 1:\n",
    "            x = LSTM(10, return_sequences=True, activation=\"relu\")(x)\n",
    "        else:\n",
    "            x = LSTM(10, return_sequences=True, activation=\"relu\")(x)\n",
    "    y = Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "    models[f\"[LSTM]{ii+1}_layer\"] = Model(inputs=inputs, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pt0wBtJ0uf9D"
   },
   "source": [
    "#Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQ11U9_cufdc"
   },
   "outputs": [],
   "source": [
    "\n",
    "for ii in range(6):\n",
    "    inputs = Input(shape=(1,100))\n",
    "    x = Conv1D(10, 1, activation=\"relu\", padding=\"same\", strides=1)(inputs)\n",
    "    for iii in range(ii):\n",
    "        x = Conv1D(100, 3, activation=\"relu\", padding=\"same\", strides=1)(x)\n",
    "    y = Dense(9, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    models[f\"[Conv1D]{ii+1}_layer\"] = model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "datVAgzhul62"
   },
   "source": [
    "#Conv1D + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-7dCgy-ulXn"
   },
   "outputs": [],
   "source": [
    "for ii in range(6):\n",
    "    inputs = Input(shape=(1,100))\n",
    "    x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(inputs)\n",
    "    x = LSTM(10, activation=\"relu\", return_sequences=True)(x)\n",
    "    for iii in range(ii):\n",
    "        x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "        x = LSTM(10, activation=\"relu\", return_sequences=True)(x)  \n",
    "    y = Dense(9, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    models[f\"[Conv1D + LSTM]{ii+1}_layer\"] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b86CS4nLulQ3"
   },
   "source": [
    "#LSTM + Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "TP2dAIciuuBq",
    "outputId": "384fde54-57ab-460c-9912-684576ade9e4"
   },
   "outputs": [],
   "source": [
    " for ii in range(6):\n",
    "    inputs = Input(shape=(1,100))\n",
    "    x = LSTM(10, activation=\"relu\", return_sequences=True)(inputs)\n",
    "    x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "    for iii in range(ii):\n",
    "        x = LSTM(10, activation=\"relu\", return_sequences=True)(x)  \n",
    "        x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "    y = Dense(9, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    models[f\"[LSTM + Conv1D]{ii+1}_layer\"] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VWYJUHQw88a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cze9MLtlw845"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "8iTEQH-kJmKa",
    "outputId": "3ea2f8b9-68d2-4766-b340-48de2186cca6"
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WPzvqKLCWCb"
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    models[model].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lkBi8hG_Renb",
    "outputId": "ae34035d-db04-43a8-cfc7-ba99da73be7f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    def lr_schedule(epoch):\n",
    "        lr = start_lr\n",
    "        if epoch > 225:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 150:\n",
    "            lr *= 1e-1\n",
    "        print('epoch: ', epoch)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "    # 3. 모델 학습과정 설정하기\n",
    "    if optimizer == 'sgd':\n",
    "        sgd = SGD(lr=lr_schedule(0), decay=1e-4, momentum=0.9, nesterov=True)\n",
    "    elif optimizer == 'adam':\n",
    "        adam = Adam(lr=lr_schedule(0))\n",
    "    elif optimizer == 'nAdam':\n",
    "        nAdam = Nadam(lr=lr_schedule(0))\n",
    "\n",
    "\n",
    "    # model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # models[model].compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "    #                                                                                     single_class_precision(0), single_class_recall(0),\n",
    "    #                                                                                     single_class_precision(1), single_class_recall(1),\n",
    "    #                                                                                     single_class_precision(2), single_class_recall(2),\n",
    "    #                                                                                     single_class_precision(3), single_class_recall(3),\n",
    "    #                                                                                     single_class_precision(4), single_class_recall(4),\n",
    "    #                                                                                     single_class_precision(5), single_class_recall(5),\n",
    "    #                                                                                     single_class_precision(3), single_class_recall(6),\n",
    "    #                                                                                     single_class_precision(4), single_class_recall(7),\n",
    "    #                                                                                     single_class_precision(5), single_class_recall(8)])\n",
    "\n",
    "    models[model].compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    csv_logger_path = save_path+f'/{model}(batch_size={batch_size}, split_val={split_val}, fit_shuffle={fit_shuffle}, split_shuffle={split_shuffle}, optimizer={optimizer}, lr=s:{start_lr}, per:1e-1, e:(150, 225))_log.csv'\n",
    "    if not os.path.isdir(os.path.dirname(csv_logger_path)):\n",
    "        os.makedirs(os.path.dirname(csv_logger_path))\n",
    "    csv_logger = CSVLogger(csv_logger_path, append=True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "    callbacks = [lr_scheduler, csv_logger]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 4. 모델 학습시키기\n",
    "    histories[model] = models[model].fit(x_train, y_train, \n",
    "                                         epochs=300, batch_size=batch_size, \n",
    "                                         shuffle=fit_shuffle,\n",
    "                                         workers=4, validation_data=(x_test, y_test),\n",
    "                                         callbacks=callbacks)\n",
    "    \n",
    "    # histories[model] = models[model].fit(x_train, y_train, epochs=300, batch_size=32, workers=4, validation_data=(x_train, y_train))\n",
    "\n",
    "    # 6. 모델 평가하기\n",
    "    evaluates[model] = models[model].evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "TLIdaxrQaJxX",
    "outputId": "1e9b6beb-5ec5-40cc-b9c2-54b0020bcf1c"
   },
   "outputs": [],
   "source": [
    "evaluates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2KkJdfuD8UZ"
   },
   "outputs": [],
   "source": [
    "# 5. 학습과정 살펴보기\n",
    "\n",
    "for history in histories:\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.plot(histories[history].history['loss'], 'y', label=\"train loss\")\n",
    "    ax.plot(histories[history].history['val_loss'], 'r', label=f\"val loss({format(evaluates[history][0], '0.4f')})\")\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"{history}\")\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.plot(histories[history].history['acc'], 'y', label=\"train acc\")\n",
    "    ax.plot(histories[history].history['val_acc'], 'r', label=f\"val acc({format(evaluates[history][1],'0.4f')})\")\n",
    "    ax.set_ylabel('accuray')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"{history}\")\n",
    "\n",
    "    save_type = \"history\"\n",
    "    history_path = f\"{save_path}/{save_type}\"\n",
    "\n",
    "    if not os.path.isdir(history_path):\n",
    "        os.mkdir(history_path)\n",
    "    plt.savefig(history_path + \"/\" + history)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "LNP-CW0ETOVP",
    "outputId": "a7760fac-7115-42f6-9855-3860bf986d2f"
   },
   "outputs": [],
   "source": [
    "*histories['[MLP]1_layer'].history[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apf6Nyv-GpD2"
   },
   "outputs": [],
   "source": [
    "for history in histories:\n",
    "\n",
    "    precs = [x for x in histories[history].history.keys() if \"prec\" in x if \"val\" not in x]\n",
    "    recalls = [x for x in histories[history].history.keys() if \"recall\" in x if \"val\" not in x]\n",
    "\n",
    "    val_precs = [x for x in histories[history].history.keys() if \"prec\" in x if \"val\" in x]\n",
    "    val_recalls = [x for x in histories[history].history.keys() if \"recall\" in x if \"val\" in x]\n",
    "\n",
    "    fig = plt.figure(figsize=(30,5))\n",
    "\n",
    "    for prec in precs:\n",
    "        ax = fig.add_subplot(1,4,1)\n",
    "        ax.plot(histories[history].history[prec], label=prec)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('precision')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{history}\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        \n",
    "    for recall in recalls:\n",
    "        ax = fig.add_subplot(1,4,2)\n",
    "        ax.plot(histories[history].history[recall], label=recall)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('recall')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{history}\")  \n",
    "        ax.set_ylim(0, 1.0)\n",
    "\n",
    "    for prec in val_precs:\n",
    "        ax = fig.add_subplot(1,4,3)\n",
    "        ax.plot(histories[history].history[prec], label=prec)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('val_precision')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{history}\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        \n",
    "    for recall in val_recalls:\n",
    "        ax = fig.add_subplot(1,4,4)\n",
    "        ax.plot(histories[history].history[recall], label=recall)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('val_recall')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{history}\")  \n",
    "        ax.set_ylim(0, 1.0)\n",
    "\n",
    "    save_type = \"precision,recall\"\n",
    "    history_path = f\"{save_path}/{save_type}\"\n",
    "\n",
    "    if not os.path.isdir(history_path):\n",
    "        os.mkdir(history_path)\n",
    "    plt.savefig(history_path + \"/\" + history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTdZoxM_YJL-"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "\n",
    "for id, evaluate in enumerate(evaluates):\n",
    "    # print(evaluates[evaluate])\n",
    "    # print(evaluates[evaluate][2:])\n",
    "    metrics = np.array(evaluates[evaluate][2:])\n",
    "\n",
    "    idx = np.linspace(0, 11, 12) \n",
    "    # print(idx)\n",
    "    precision = metrics[(idx % 2) == 0]\n",
    "    recall = metrics[((idx+1) % 2) == 0]\n",
    "\n",
    "    # print(precision)\n",
    "    # print(recall)\n",
    "    # print(\"\\n\")\n",
    "    # print(\"\\n\")\n",
    "    # print(\"\\n\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "\n",
    "    N = 6\n",
    "    ind = np.arange(N)\n",
    "    width = 0.35\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,id+1)\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    prec_bar = ax.bar(ind, precision, width, color='r')\n",
    "    recall_bar = ax.bar(ind + width, recall, width, color='y')\n",
    "\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Precision and Recall')\n",
    "    ax.set_xticks(ind + width / 2)\n",
    "    ax.set_xticklabels(('C1', 'C2', 'C3', 'C4', 'C5', 'C6'))\n",
    "\n",
    "    ax.legend((prec_bar[0], recall_bar[0]), ('Precision', 'Recall'))\n",
    "    # ax.grid(True)\n",
    "    ax.set_title(evaluate)\n",
    "    ax.set_ylim(0,1.0)\n",
    "\n",
    "        \n",
    "save_type = \"precision,recall_graph\"\n",
    "history_path = f\"{save_path}/{save_type}\"\n",
    "\n",
    "if not os.path.isdir(history_path):\n",
    "    os.mkdir(history_path)\n",
    "plt.savefig(history_path + \"/\" + evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQZwJqsBYF6j"
   },
   "outputs": [],
   "source": [
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8L9-hm1_43vO",
    "outputId": "218eedc8-2af7-49ab-a65a-65a74831d8ac"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 1, 100)\n",
    "x_test  = x_test.reshape(-1, 1, 100)\n",
    "y_train = y_train.reshape(-1,  6)\n",
    "y_test = y_test.reshape(-1, 6)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOrPqCLTDxl4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "m80D0FrrISs3",
    "outputId": "56f6f592-b172-49c0-aec2-d369ef68b2b3"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,100))\n",
    "x = LSTM(10, return_sequences=True, activation=\"relu\")(inputs)\n",
    "y = Dense(6, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "52oawxk5RekY",
    "outputId": "da0de6b0-c54c-47cd-8722-922d0ab5b378"
   },
   "outputs": [],
   "source": [
    "# 2. 모델 구성하기\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128))\n",
    "# # model.add(Flatten())\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "evaluates = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    models[model].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "UMU5OAb_N6Kh",
    "outputId": "7195b0a7-d3c4-43bb-f9b7-5d91d913fef5"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,100))\n",
    "x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(inputs)\n",
    "x = LSTM(10, activation=\"relu\",return_sequences=True)(x)\n",
    "x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "x = LSTM(10, activation=\"relu\")(x)  \n",
    "y = Dense(6, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mXnija4fK0tR",
    "outputId": "d48b7d05-6a3b-4b51-a97d-b92f26bb021c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "evaluates = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(\"\\n\"+model)\n",
    "    models[model].summary()\n",
    "# model.add(Conv1D(50,\n",
    "#                  1,\n",
    "#                  input_shape=(1,100),\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# # model.add(GlobalMaxPooling1D())\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nD8PJcbGWfC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "ax.set_ylabel('loss')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "ax.set_ylabel('accuray')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "# fig, loss_ax = plt.subplots()\n",
    "\n",
    "# acc_ax = loss_ax.twinx()\n",
    "\n",
    "# loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "# loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "# acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "# acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "# acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "# loss_ax.set_xlabel('epoch')\n",
    "# loss_ax.set_ylabel('loss')\n",
    "# acc_ax.set_ylabel('accuray')\n",
    "\n",
    "# loss_ax.legend(loc='upper left')\n",
    "# acc_ax.legend(loc='lower left')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "GPc317BeUxfj",
    "outputId": "adcdb115-a4a3-4860-a48d-1e6e0a2f08c9"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958
    },
    "colab_type": "code",
    "id": "ld0YvmINUxzr",
    "outputId": "04a838bc-8b34-4128-a9fd-6a36931c4e0f"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(50,\n",
    "                 1,\n",
    "                 input_shape=(1,100),\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rf0mstPzcqmF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "fgJET27RjWTv",
    "outputId": "56298d1c-8730-42e1-e35f-e1909b4e2a0e"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "                    # Input(shape=(1,100)),\n",
    "                    Conv1D(50,1, input_shape=(1,100) ,padding='valid',activation='relu',strides=1),\n",
    "                    Conv1D(50,1, padding='valid',activation='relu',strides=1),\n",
    "                    Conv1D(50,1, padding='valid',activation='relu',strides=1),\n",
    "                    Conv1D(50,1, padding='valid',activation='relu',strides=1),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(50, activation='relu'),\n",
    "                    # model.add(Dropout(0.2))\n",
    "                    Dense(6, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CE3nu0OtjWRc",
    "outputId": "e2a4298a-d377-4bef-b534-36963170460c"
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# # model.add(Embedding(10, 128, input_length=100))\n",
    "# # model.add(Dropout(0.2))\n",
    "# model.add(Input(shape=(1,100)))\n",
    "# model.add(Conv1D(100,\n",
    "#                  1,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# # model.add(Dropout(0.2))\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "ax.set_ylabel('loss')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "ax.set_ylabel('accuray')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig, loss_ax = plt.subplots()\n",
    "\n",
    "# acc_ax = loss_ax.twinx()\n",
    "\n",
    "# loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "# loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "# acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "# acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "# acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "# loss_ax.set_xlabel('epoch')\n",
    "# loss_ax.set_ylabel('loss')\n",
    "# acc_ax.set_ylabel('accuray')\n",
    "\n",
    "# loss_ax.legend(loc='upper left')\n",
    "# acc_ax.legend(loc='lower left')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4bNYV8ijYRS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KjCs0sUyFeeV",
    "RfsjvrWu9jjn"
   ],
   "name": "1D DenseNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
