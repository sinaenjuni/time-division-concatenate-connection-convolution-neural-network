{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "pf32MK1QQp6b",
    "outputId": "78d0297e-b627-4a59-e46e-69633420c5b5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/googledrive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "colab_type": "code",
    "id": "wXz2orRxQsB6",
    "outputId": "1a2c1451-1f71-4354-834c-79e381a01f4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip install livelossplot \n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "\n",
    "from keras import backend\n",
    "\n",
    "from keras.layers import AveragePooling1D, Input, GlobalMaxPooling1D\n",
    "from keras import layers, models\n",
    "from keras import backend\n",
    "\n",
    "        \n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AVpGoW9HF3C5"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ByRdciSCZ55"
   },
   "source": [
    "## Load Data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "EwSMiMi2QyBr",
    "outputId": "b272f4f7-f070-4727-978a-dc0addd1a768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 500) (12000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # data_set_path = './data_set/my_data/class_9'\n",
    "# data_set_path = './data_set/nonsvm/'\n",
    "\n",
    "# save_path = './save_file/data_3_100_my_svm'\n",
    "\n",
    "# x_data = np.load(data_set_path+\"/x_data_3_100_my_nonsvm.npy\")\n",
    "# y_data = np.load(data_set_path+\"/y_data_3_100_my_nonsvm.npy\")\n",
    "# print(x_data.shape, y_data.shape)\n",
    "\n",
    "# data_classes = 3\n",
    "\n",
    "\n",
    "# data_set_path = './data_set/my_data/class_9'\n",
    "data_set_path = './data_set/svm'\n",
    "\n",
    "# save_path = './save_file/data_3_100_my_svm'\n",
    "save_path = './save_file/data_6_500_hard_svm_acc'\n",
    "\n",
    "\n",
    "x_data = np.load(data_set_path+\"/x_data_6_500_hard_svm_acc.npy\")\n",
    "y_data = np.load(data_set_path+\"/y_data_6_500_hard_svm_acc.npy\")\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "data_classes = 6\n",
    "\n",
    "\n",
    "# # Normalization\n",
    "# for idx in range(len(x_data)):\n",
    "#     min = np.min(x_data[idx])\n",
    "#     max = np.max(x_data[idx])\n",
    "#     x_data[idx] = (x_data[idx] - min) / (max - min)\n",
    "\n",
    "# fig = plt.figure(figsize=(25,10))\n",
    "\n",
    "# for id in range(4):\n",
    "#     ax = fig.add_subplot(2,4,id+1)\n",
    "#     ax.plot(x_data[id])\n",
    "#     ax.grid(True)\n",
    "#     ax.set_title(f\"x_train{id}\")\n",
    "\n",
    "# # Standardize\n",
    "# for idx in range(len(x_data)):\n",
    "#     standard_deviation = np.std(x_data[idx])\n",
    "#     mean = np.mean(x_data[idx])\n",
    "#     x_data[idx] = (x_data[idx] - mean) / standard_deviation\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(25,10))\n",
    "\n",
    "# for id in range(4):\n",
    "#     ax = fig.add_subplot(2,4,id+1)\n",
    "#     ax.plot(x_data[id])\n",
    "#     ax.grid(True)\n",
    "#     ax.set_title(f\"x_train{id}\")\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "\n",
    "for id in range(4):\n",
    "    ax = fig.add_subplot(2,4,id+1)\n",
    "    ax.plot(x_data[id])\n",
    "    ax.grid(True)\n",
    "    ax.set_title(f\"x_train{id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjCs0sUyFeeV"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2MvkkwRBMkc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesDataAugmentation:\n",
    "\n",
    "    # def __init__(self):\n",
    "        # self.sigma = 0.05\n",
    "\n",
    "    def DA_Jitter(self, X, sigma=0.05):\n",
    "        myNoise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
    "        return X+myNoise\n",
    "\n",
    "    def DA_Scaling(self, X, sigma=0.1):\n",
    "        scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1)) # shape=(1,3)\n",
    "        myNoise = np.matmul(np.ones((X.shape[0],1)), scalingFactor)\n",
    "        return X*myNoise\n",
    "\n",
    "\n",
    "    def GenerateRandomCurves(self, X, sigma=0.2, knot=4):\n",
    "        \n",
    "        xx = np.arange(0, X.shape[0], (X.shape[0]-1) / (knot + 1)) #(3,1) * (1, 6) = (3,6) // (6,3)\n",
    "\n",
    "        yy = np.random.normal(loc = 1.0, scale = sigma, size = (knot + 2)) # (6, 3)\n",
    "        x_range = np.arange(X.shape[0])\n",
    "\n",
    "        cs_x = CubicSpline(xx, yy)\n",
    "\n",
    "        return np.array(cs_x(x_range))\n",
    "\n",
    "\n",
    "    def DA_MagWarp(self, X, sigma=0.2):\n",
    "        return X * self.GenerateRandomCurves(X, sigma)\n",
    "\n",
    "\n",
    "\n",
    "    def DistortTimesteps(self, X, sigma=0.2, knot=4):\n",
    "        tt = self.GenerateRandomCurves(X, sigma, knot) # Regard these samples aroun 1 as time intervals\n",
    "        tt_cum = np.cumsum(tt)        # Add intervals to make a cumulative graph\n",
    "        t_scale = (X.shape[0]-1) / tt_cum[-1] \n",
    "\n",
    "        tt_cum = tt_cum * t_scale\n",
    "\n",
    "        return tt_cum\n",
    "\n",
    "\n",
    "\n",
    "    def DA_TimeWarp(self, X, sigma=0.2):\n",
    "        tt_new = self.DistortTimesteps(X, sigma)\n",
    "\n",
    "        X_new = np.zeros(X.shape) #(3600, 3)      0\n",
    "\n",
    "        x_range = np.arange(X.shape[0]) #(3600,)  0~3599\n",
    "\n",
    "        X_new = np.interp(x_range, tt_new, X)\n",
    "\n",
    "        return X_new\n",
    "\n",
    "\n",
    "\n",
    "    def DA_Permutation(self, X, nPerm=4, minSegLength=10):\n",
    "        data = X\n",
    "        X_new = np.zeros(data.shape)     #(100,)\n",
    "        idx = np.random.permutation(nPerm) \n",
    "\n",
    "        bWhile = True\n",
    "        while bWhile == True:\n",
    "            segs = np.zeros(nPerm+1, dtype=int)\n",
    "            \n",
    "            segs[1:-1] = np.sort(np.random.randint(minSegLength, data.shape[0]-minSegLength, nPerm-1))\n",
    "            segs[-1] = data.shape[0]\n",
    "\n",
    "            if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
    "                bWhile = False\n",
    "\n",
    "        pp = 0\n",
    "        for ii in range(nPerm):\n",
    "            x_temp = data[segs[idx[ii]]:segs[idx[ii]+1]]\n",
    "            X_new[pp:pp+len(x_temp)] = x_temp\n",
    "            pp += len(x_temp)\n",
    "\n",
    "        return X_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def DA_Permutation(self, X, nPerm=4, minSegLength=10):\n",
    "        data = X\n",
    "        X_new = np.zeros(data.shape)     #(100,)\n",
    "        idx = np.random.permutation(nPerm) \n",
    "\n",
    "        bWhile = True\n",
    "        while bWhile == True:\n",
    "            segs = np.zeros(nPerm+1, dtype=int)\n",
    "            \n",
    "            segs[1:-1] = np.sort(np.random.randint(minSegLength, data.shape[0]-minSegLength, nPerm-1))\n",
    "            segs[-1] = data.shape[0]\n",
    "\n",
    "            if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
    "                bWhile = False\n",
    "\n",
    "        pp = 0\n",
    "        for ii in range(nPerm):\n",
    "            x_temp = data[segs[idx[ii]]:segs[idx[ii]+1]]\n",
    "            X_new[pp:pp+len(x_temp)] = x_temp\n",
    "            pp += len(x_temp)\n",
    "\n",
    "        return X_new\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aH3x6zo2FiyT"
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "MNbBVlyURAd8",
    "outputId": "ce63d075-9322-4f3b-8a24-2eeebc8e7be1"
   },
   "outputs": [],
   "source": [
    "\n",
    "#data_set_\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=1, shuffle=False)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "#y_data onehot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, data_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, data_classes)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OAzDrwbFqnn"
   },
   "source": [
    "## Data packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZYWOEh9sR1ss",
    "outputId": "12f68d36-1b16-4a83-f70d-fa64f469218b"
   },
   "outputs": [],
   "source": [
    "isPerdata = True\n",
    "# isPerdata = False\n",
    "\n",
    "if not isPerdata:\n",
    "    x_train = x_train.reshape(-1, 100)\n",
    "    x_test  = x_test.reshape(-1, 100)\n",
    "    y_train = y_train.reshape(-1, 9)\n",
    "    y_test = y_test.reshape(-1, 9)\n",
    "else:\n",
    "    # x_train = x_train.reshape(-1, 1, 100)\n",
    "    # x_test  = x_test.reshape(-1, 1, 100)\n",
    "#     y_train = y_train.reshape(-1, 1, 9)\n",
    "#     y_test = y_test.reshape(-1, 1, 9)\n",
    "    x_train = x_train.reshape(-1, 100,1)\n",
    "    x_test  = x_test.reshape(-1, 100,1)\n",
    "    # y_train = y_train.reshape(-1, 9)\n",
    "    # y_test = y_test.reshape(-1, 9)\n",
    "\n",
    "\n",
    "# x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "# x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 500, 1) (12000, 6)\n"
     ]
    }
   ],
   "source": [
    "#y_data onehot encoding\n",
    "y = keras.utils.to_categorical(y_data, data_classes)\n",
    "x = x_data.reshape(-1, 500,1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YilogMXq358K"
   },
   "outputs": [],
   "source": [
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#    \n",
    "def single_class_precision(interesting_class_id):\n",
    "    def prec(y_true, y_pred):\n",
    "        class_id_true = K.argmax(y_true, axis=-1)\n",
    "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
    "        precision_mask = K.cast(K.equal(class_id_pred, interesting_class_id), 'int32')\n",
    "        class_prec_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * precision_mask\n",
    "        class_prec = K.cast(K.sum(class_prec_tensor), 'float32') / K.cast(K.maximum(K.sum(precision_mask), 1), 'float32')\n",
    "        return class_prec\n",
    "    return prec\n",
    "\n",
    "\n",
    "#    \n",
    "def single_class_recall(interesting_class_id):\n",
    "    def recall(y_true, y_pred):\n",
    "        class_id_true = K.argmax(y_true, axis=-1)\n",
    "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
    "        recall_mask = K.cast(K.equal(class_id_true, interesting_class_id), 'int32')\n",
    "        class_recall_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * recall_mask\n",
    "        class_recall = K.cast(K.sum(class_recall_tensor), 'float32') / K.cast(K.maximum(K.sum(recall_mask), 1), 'float32')\n",
    "        return class_recall\n",
    "    return recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max    \n",
    "    # round : \n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) #  0(Negative)  1(Positive) \n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) #  0(Negative)  1(Positive) \n",
    "\n",
    "    # True Positive      1(Positive) \n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) =   1(Positive) \n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon() 'divide by zero error'    \n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max    \n",
    "    # round : \n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) #  0(Negative)  1(Positive) \n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) #  0(Negative)  1(Positive) \n",
    "\n",
    "    # True Positive      1(Positive) \n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) =   1(Positive) \n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon() 'divide by zero error'    \n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon() 'divide by zero error'    \n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iMdPXqMF-xu"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_1 (ZeroPaddin (None, 106, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1/conv (Conv1D)          (None, 100, 24)           192       \n",
      "_________________________________________________________________\n",
      "1_conv (Conv1D)              (None, 100, 48)           1200      \n",
      "_________________________________________________________________\n",
      "1_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "1_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "2_conv (Conv1D)              (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "2_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "2_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "3_conv (Conv1D)              (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "3_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "3_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "4_conv (Conv1D)              (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "4_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "4_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "5_conv (Conv1D)              (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "5_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "5_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "6_conv (Conv1D)              (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "6_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "6_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "7_conv (Conv1D)              (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "7_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "7_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "8_conv (Conv1D)              (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "8_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "8_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "9_conv (Conv1D)              (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "9_bn (BatchNormalization)    (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "9_relu (Activation)          (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "10_conv (Conv1D)             (None, 100, 48)           2352      \n",
      "_________________________________________________________________\n",
      "10_bn (BatchNormalization)   (None, 100, 48)           192       \n",
      "_________________________________________________________________\n",
      "10_relu (Activation)         (None, 100, 48)           0         \n",
      "_________________________________________________________________\n",
      "avg_pool (GlobalAveragePooli (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 3)                 147       \n",
      "=================================================================\n",
      "Total params: 24,627\n",
      "Trainable params: 23,667\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def createCNNModel(depth=None,\n",
    "            growthRate = None,\n",
    "            input_shape = None,            \n",
    "            pooling = None,\n",
    "            classes = None):\n",
    "    \n",
    "    nChannels = 2 * growthRate\n",
    "    bn_axis = 2\n",
    "\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.ZeroPadding1D(padding=3)(img_input)\n",
    "    x = layers.Conv1D(nChannels, 7, strides=1, use_bias=True, name='conv1/conv')(x)\n",
    "    \n",
    "    \n",
    "    for i in range(depth):\n",
    "        x = layers.Conv1D(4 * growthRate, 1, use_bias=True, kernel_initializer='he_normal', name=f'{i+1}_conv')(x)\n",
    "        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=f'{i+1}_bn')(x)\n",
    "        x = layers.Activation('relu', name=f'{i+1}_relu')(x)\n",
    "    \n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(name='avg_pool')(x)\n",
    "    x = layers.Dense(classes, activation='softmax', name='fc')(x)\n",
    "\n",
    "    model = models.Model(img_input, x, name=f\"{depth}_model\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "\n",
    "def createVGGModel(input_shape = None,            \n",
    "            n_classes = None, l2_regularization=0.0005):\n",
    "    l2_reg = l2_regularization # Make the internal name shorter.\n",
    "\n",
    "    n_boxes = [10,10,10,10,6,6]\n",
    "    bn_axis=-1\n",
    "    \n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    conv1_1 = layers.Conv1D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_1')(img_input)\n",
    "#     conv1_1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1_1_bn')(conv1_1)\n",
    "#     conv1_1 = layers.Activation('relu', name='conv1_1_relu')(conv1_1)\n",
    "    conv1_2 = layers.Conv1D(32, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_2')(conv1_1)\n",
    "#     conv1_2 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1_2_bn')(conv1_2)\n",
    "#     conv1_2 = layers.Activation('relu', name='conv1_2_relu')(conv1_2)\n",
    "    pool1 = layers.MaxPooling1D(pool_size=2, strides=2, padding='same', name='pool1')(conv1_2)\n",
    "\n",
    "    conv2_1 = layers.Conv1D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_1')(pool1)\n",
    "#     conv2_1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv2_1_bn')(conv2_1)\n",
    "#     conv2_1 = layers.Activation('relu', name='conv2_1_relu')(conv2_1)\n",
    "    conv2_2 = layers.Conv1D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_2')(conv2_1)\n",
    "#     conv2_2 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv2_2_bn')(conv2_2)\n",
    "#     conv2_2 = layers.Activation('relu', name='conv2_2_relu')(conv2_2)\n",
    "    pool2 = layers.MaxPooling1D(pool_size=2, strides=2, padding='same', name='pool2')(conv2_2)\n",
    "\n",
    "    conv3_1 = layers.Conv1D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_1')(pool2)\n",
    "#     conv3_1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv3_1_bn')(conv3_1)\n",
    "#     conv3_1 = layers.Activation('relu', name='conv3_1relu')(conv3_1)\n",
    "    conv3_2 = layers.Conv1D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_2')(conv3_1)\n",
    "#     conv3_2 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv3_2_bn')(conv3_2)\n",
    "#     conv3_2 = layers.Activation('relu', name='conv3_2_relu')(conv3_2)\n",
    "    conv3_3 = layers.Conv1D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_3')(conv3_2)\n",
    "#     conv3_3 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv3_3_bn')(conv3_3)\n",
    "#     conv3_3 = layers.Activation('relu', name='conv3_3_relu')(conv3_3)\n",
    "    pool3 = layers.MaxPooling1D(pool_size=2, strides=2, padding='same', name='pool3')(conv3_3)\n",
    "\n",
    "    conv4_1 = layers.Conv1D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_1')(pool3)\n",
    "    conv4_2 = layers.Conv1D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_2')(conv4_1)\n",
    "    conv4_3 = layers.Conv1D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3')(conv4_2)\n",
    "    pool4 = layers.MaxPooling1D(pool_size=2, strides=2, padding='same', name='pool4')(conv4_3)\n",
    "\n",
    "    conv5_1 = layers.Conv1D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_1')(pool4)\n",
    "    conv5_2 = layers.Conv1D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_2')(conv5_1)\n",
    "    conv5_3 = layers.Conv1D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_3')(conv5_2)\n",
    "    pool5 = layers.MaxPooling1D(pool_size=3, strides=1, padding='same', name='pool5')(conv5_3)\n",
    "\n",
    "    fc6 = layers.Conv1D(1024, 3, dilation_rate=6, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc6')(pool5)\n",
    "\n",
    "    fc7 = layers.Conv1D(1024, 1, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7')(fc6)\n",
    "\n",
    "    conv6_1 = layers.Conv1D(128, 1, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_1')(fc7)\n",
    "    conv6_1 = layers.MaxPooling1D( padding='same', name='conv6_padding')(conv6_1)\n",
    "    conv6_2 = layers.Conv1D(256, 3, strides=2, activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2')(conv6_1)\n",
    "\n",
    "#     conv7_1 = layers.Conv1D(128, 1, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_1')(conv6_2)\n",
    "#     conv7_1 = layers.MaxPooling1D( padding='same', name='conv7_padding')(conv7_1)\n",
    "#     conv7_2 = layers.Conv1D(256, 3, strides=2, activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2')(conv7_1)\n",
    "\n",
    "#     conv8_1 = layers.Conv1D(128, 1, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_1')(conv7_2)\n",
    "#     conv8_2 = layers.Conv1D(256, 3, strides=1, activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2')(conv8_1)\n",
    "\n",
    "#     conv9_1 = layers.Conv1D(128, 1, activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_1')(conv8_2)\n",
    "#     conv9_2 = layers.Conv1D(256, 3, strides=1, activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2')(conv9_1)\n",
    "\n",
    "    \n",
    "#     conv4_3_norm = keras.backend.l2_normalize(name='conv4_3_norm')(conv4_3)\n",
    "\n",
    "    ### Build the convolutional predictor layers on top of the base network\n",
    "\n",
    "    # We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`\n",
    "    # Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`\n",
    "    conv4_3_norm_mbox_conf = layers.Conv1D(n_boxes[0] * n_classes, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_conf')(img_input)\n",
    "    fc7_mbox_conf = layers.Conv1D(n_boxes[1] * n_classes, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_conf')(conv2_1)\n",
    "    conv6_2_mbox_conf = layers.Conv1D(n_boxes[2] * n_classes, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_conf')(conv3_1)\n",
    "    conv7_2_mbox_conf = layers.Conv1D(n_boxes[3] * n_classes, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_conf')(conv5_1)\n",
    "    conv8_2_mbox_conf = layers.Conv1D(n_boxes[4] * n_classes, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_conf')(fc7)\n",
    "    conv9_2_mbox_conf = layers.Conv1D(n_boxes[5] * n_classes, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_conf')(conv6_2)\n",
    "    \n",
    "    \n",
    "    conv4_3_norm_mbox_conf_reshape = layers.Reshape((-1, n_classes), name='conv4_3_norm_mbox_conf_reshape')(conv4_3_norm_mbox_conf)\n",
    "    fc7_mbox_conf_reshape = layers.Reshape((-1, n_classes), name='fc7_mbox_conf_reshape')(fc7_mbox_conf)\n",
    "    conv6_2_mbox_conf_reshape = layers.Reshape((-1, n_classes), name='conv6_2_mbox_conf_reshape')(conv6_2_mbox_conf)\n",
    "    conv7_2_mbox_conf_reshape = layers.Reshape((-1, n_classes), name='conv7_2_mbox_conf_reshape')(conv7_2_mbox_conf)\n",
    "    conv8_2_mbox_conf_reshape = layers.Reshape((-1, n_classes), name='conv8_2_mbox_conf_reshape')(conv8_2_mbox_conf)\n",
    "    conv9_2_mbox_conf_reshape = layers.Reshape((-1, n_classes), name='conv9_2_mbox_conf_reshape')(conv9_2_mbox_conf)\n",
    "\n",
    "    \n",
    "    mbox_conf = layers.Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_mbox_conf_reshape,\n",
    "                                                       fc7_mbox_conf_reshape,\n",
    "                                                       conv6_2_mbox_conf_reshape,\n",
    "                                                       conv7_2_mbox_conf_reshape,\n",
    "                                                       conv8_2_mbox_conf_reshape,\n",
    "                                                       conv9_2_mbox_conf_reshape])\n",
    "    \n",
    "    mbox_conf_softmax = Activation('softmax', name='mbox_conf_softmax')(mbox_conf)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D(name='avg_pool')(mbox_conf_softmax)\n",
    "    x = layers.Dense(n_classes, activation='softmax', name='fc')(x)\n",
    "\n",
    "    model = models.Model(img_input, x, name=f\"VGG_model\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           (None, 500, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv1D)                (None, 500, 32)      128         input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2 (Conv1D)                (None, 500, 32)      3104        conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling1D)            (None, 250, 32)      0           conv1_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv1D)                (None, 250, 64)      6208        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2 (Conv1D)                (None, 250, 64)      12352       conv2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling1D)            (None, 125, 64)      0           conv2_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1 (Conv1D)                (None, 125, 128)     24704       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2 (Conv1D)                (None, 125, 128)     49280       conv3_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3 (Conv1D)                (None, 125, 128)     49280       conv3_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling1D)            (None, 63, 128)      0           conv3_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1 (Conv1D)                (None, 63, 256)      98560       pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2 (Conv1D)                (None, 63, 256)      196864      conv4_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3 (Conv1D)                (None, 63, 256)      196864      conv4_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling1D)            (None, 32, 256)      0           conv4_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1 (Conv1D)                (None, 32, 256)      196864      pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2 (Conv1D)                (None, 32, 256)      196864      conv5_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3 (Conv1D)                (None, 32, 256)      196864      conv5_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling1D)            (None, 32, 256)      0           conv5_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Conv1D)                    (None, 32, 1024)     787456      pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fc7 (Conv1D)                    (None, 32, 1024)     1049600     fc6[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv6_1 (Conv1D)                (None, 32, 128)      131200      fc7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv6_padding (MaxPooling1D)    (None, 16, 128)      0           conv6_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2 (Conv1D)                (None, 7, 256)       98560       conv6_padding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf (Conv1D) (None, 500, 60)      240         input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_conf (Conv1D)          (None, 250, 60)      11580       conv2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf (Conv1D)      (None, 125, 60)      23100       conv3_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf (Conv1D)      (None, 32, 60)       46140       conv5_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf (Conv1D)      (None, 32, 36)       110628      fc7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf (Conv1D)      (None, 7, 36)        27684       conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf_reshape  (None, 5000, 6)      0           conv4_3_norm_mbox_conf[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_conf_reshape (Reshape) (None, 2500, 6)      0           fc7_mbox_conf[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf_reshape (Resh (None, 1250, 6)      0           conv6_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf_reshape (Resh (None, 320, 6)       0           conv7_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf_reshape (Resh (None, 192, 6)       0           conv8_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf_reshape (Resh (None, 42, 6)        0           conv9_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf (Concatenate)         (None, 9304, 6)      0           conv4_3_norm_mbox_conf_reshape[0]\n",
      "                                                                 fc7_mbox_conf_reshape[0][0]      \n",
      "                                                                 conv6_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv7_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv8_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv9_2_mbox_conf_reshape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf_softmax (Activation)  (None, 9304, 6)      0           mbox_conf[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling1 (None, 6)            0           mbox_conf_softmax[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "fc (Dense)                      (None, 6)            42          avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,514,166\n",
      "Trainable params: 3,514,166\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createVGGModel(input_shape=(500,1), n_classes=6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def dense_block(x, blocks, growth_rate, name):\n",
    "    for i in range(blocks):\n",
    "        x = conv_block(x, growth_rate, name=name + '_block' + str(i + 1))\n",
    "    return x\n",
    "\n",
    "def conv_block(x, growth_rate, name):\n",
    "    bn_axis = 2\n",
    "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_0_bn')(x)\n",
    "\n",
    "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
    "\n",
    "    x1 = layers.Conv1D(4 * growth_rate, 1, use_bias=True, kernel_initializer='he_normal', name=name + '_1_conv')(x1)\n",
    "\n",
    "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x1)\n",
    "\n",
    "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
    "\n",
    "    x1 = layers.Conv1D(growth_rate, 3, padding='same', use_bias=True, kernel_initializer='he_normal', name=name + '_2_conv')(x1)\n",
    "\n",
    "    x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "\n",
    "    # x = layers.Conv1D(growth_rate, 1, padding='same', use_bias=False, name=name + '_3_conv')(x)\n",
    "    # x = layers.Add(name=name + '_concat')([x, x1])\n",
    "\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def transition_block(x, reduction, name):\n",
    "    bn_axis = 2\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=name + '_bn')(x)\n",
    "\n",
    "    x = layers.Activation('relu', name=name + '_relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(int(backend.int_shape(x)[bn_axis] * reduction), 1, use_bias=False, kernel_initializer='he_normal', name=name + '_conv')(x)\n",
    "                      \n",
    "#     x = layers.AveragePooling1D(2, strides=2, name=name + '_pool')(x)\n",
    "    x = layers.MaxPooling1D(2, strides=2, name=name + '_pool')(x)\n",
    "\n",
    "    # x = spatial_pyramid_pool(x, 3, bn_axis, name)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "class SpatialPyramidPooling(Layer):\n",
    "    \"\"\"Spatial pyramid pooling layer for 2D inputs.\n",
    "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
    "    K. He, X. Zhang, S. Ren, J. Sun\n",
    "    # Arguments\n",
    "        pool_list: list of int\n",
    "            List of pooling regions to use. The length of the list is the number of pooling regions,\n",
    "            each int in the list is the number of regions in that pool. For example [1,2,4] would be 3\n",
    "            regions with 1, 2x2 and 4x4 max pools, so 21 outputs per feature map\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        2D tensor with shape:\n",
    "        `(samples, channels * sum([i * i for i in pool_list])`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_list, **kwargs):\n",
    "\n",
    "        self.pool_list = pool_list\n",
    "\n",
    "        self.num_outputs_per_channel = sum([i for i in pool_list])\n",
    "\n",
    "        super(SpatialPyramidPooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[2]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.nb_channels * self.num_outputs_per_channel)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list}\n",
    "        base_config = super(SpatialPyramidPooling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        input_shape = K.shape(x)\n",
    "\n",
    "        num_rows = input_shape[1]\n",
    "        # num_cols = input_shape[2]\n",
    "\n",
    "        row_length = [K.cast(num_rows, 'float32') / i for i in self.pool_list]\n",
    "        # col_length = [K.cast(num_cols, 'float32') / i for i in self.pool_list]\n",
    "\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "\n",
    "        for pool_num, num_pool_regions in enumerate(self.pool_list):\n",
    "            for jy in range(num_pool_regions):\n",
    "                # for ix in range(num_pool_regions):\n",
    "                #     x1 = ix * col_length[pool_num]\n",
    "                #     x2 = ix * col_length[pool_num] + col_length[pool_num]\n",
    "                    y1 = jy * row_length[pool_num]\n",
    "                    y2 = jy * row_length[pool_num] + row_length[pool_num]\n",
    "\n",
    "                    # x1 = K.cast(K.round(x1), 'int32')\n",
    "                    # x2 = K.cast(K.round(x2), 'int32')\n",
    "                    y1 = K.cast(K.round(y1), 'int32')\n",
    "                    y2 = K.cast(K.round(y2), 'int32')\n",
    "\n",
    "                    # new_shape = [input_shape[0], y2 - y1,\n",
    "                    #                 x2 - x1, input_shape[3]]\n",
    "\n",
    "                    new_shape = [input_shape[0], y2 - y1, input_shape[2]]\n",
    "\n",
    "\n",
    "                    x_crop = x[:, y1:y2, :]\n",
    "                    xm = K.reshape(x_crop, new_shape)\n",
    "#                     pooled_val = K.max(xm, axis=1)\n",
    "                    pooled_val = K.mean(xm, axis=1)\n",
    "                    outputs.append(pooled_val)\n",
    "\n",
    "        #outputs = K.concatenate(outputs,axis = 1)\n",
    "        outputs = K.concatenate(outputs)\n",
    "        #outputs = K.reshape(outputs,(len(self.pool_list),self.num_outputs_per_channel,input_shape[0],input_shape[1]))\n",
    "        #outputs = K.permute_dimensions(outputs,(3,1,0,2))\n",
    "        #outputs = K.reshape(outputs,(input_shape[0], self.num_outputs_per_channel * self.nb_channels))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DcnnNetv3(depth=None,\n",
    "            growthRate = None,\n",
    "            include_top = None,\n",
    "            input_tensor = None,\n",
    "            input_shape = None,            \n",
    "            pooling = None,\n",
    "            classes = None,\n",
    "            **kwargs):\n",
    "    \n",
    "\n",
    "\n",
    "    if type(depth) is list:\n",
    "        if len(depth) == 4:\n",
    "            blocks = depth\n",
    "            depth = sum(blocks) * 2 + 5\n",
    "        else:\n",
    "            assert True, \"custom depth length is 4\"\n",
    "    else:\n",
    "        if depth == 101:\n",
    "            blocks = [6, 12, 18, 12]\n",
    "        elif depth == 151:\n",
    "            blocks = [6, 12, 31, 24]\n",
    "        elif depth == 201:\n",
    "            blocks = [6, 12, 48, 32]\n",
    "        elif depth == 251:\n",
    "            blocks = [6, 12, 55, 50]\n",
    "        else:\n",
    "            print(f'{depth} is not allowed depth, you will use custom_blocks parameter')\n",
    "\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "\n",
    "    nChannels = 2 * growthRate\n",
    "    bn_axis = 2\n",
    "\n",
    "\n",
    "    x = layers.ZeroPadding1D(padding=3)(img_input)\n",
    "    x = layers.Conv1D(nChannels, 7, strides=1, use_bias=True, name='conv1/conv')(x)\n",
    "\n",
    "#     x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "#     x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "\n",
    "#     x = layers.ZeroPadding1D(padding=1)(x)\n",
    "#     x = layers.MaxPooling1D(3, strides=2, name='pool1')(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    # [6, 12, 48, 32]\n",
    "    x = dense_block(x, blocks[0], growthRate, name='conv2')\n",
    "    x = transition_block(x, 0.5, name='pool2')\n",
    "\n",
    "    x = dense_block(x, blocks[1], growthRate, name='conv3')\n",
    "    x = transition_block(x, 0.5, name='pool3')\n",
    "\n",
    "    x = dense_block(x, blocks[2], growthRate, name='conv4')\n",
    "    x = transition_block(x, 0.5, name='pool4')\n",
    "\n",
    "    x = dense_block(x, blocks[3], growthRate, name='conv5')\n",
    "\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='bn')(x)\n",
    "    x = layers.Activation('relu', name='relu')(x)\n",
    "\n",
    "    \n",
    "##################################################\n",
    "\n",
    "\n",
    "# FCN Classification block\n",
    "#     x = layers.MaxPooling1D(2, strides=2, name='max_pool')(x)\n",
    "#     x = layers.Conv1D(4096,6, strides=1, activation='relu',name='fc1',padding='valid')(x)\n",
    "#     x = layers.Conv1D(1440,12, strides=1, activation='relu',name='fc2',padding='valid')(x)\n",
    "#     x = layers.Conv1D(9, 1, strides=1,activation='softmax',name='predictions',padding='valid')(x)\n",
    "\n",
    "################################################\n",
    "    \n",
    "#     x = SpatialPyramidPooling([1,2,4], name='spp')(x)\n",
    "\n",
    "    if include_top:\n",
    "#         # !!!!!\n",
    "        x = layers.GlobalAveragePooling1D(name='avg_pool')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='fc')(x)\n",
    "#     else:\n",
    "#         if pooling == 'avg':\n",
    "#             x = layers.GlobalAveragePooling1D(name='avg_pool')(x)\n",
    "#         elif pooling == 'max':\n",
    "#             x = layers.GlobalMaxPooling1D(name='max_pool')(x)\n",
    "\n",
    "\n",
    "    model = models.Model(img_input, x, name=f'densenet{depth}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createModel(depth=101):\n",
    "    inputs = layers.Input(shape=(100, 1))\n",
    "\n",
    "    modelv3 = DcnnNetv3(depth=depth, growthRate=3, include_top=True, \n",
    "                        input_tensor=inputs, pooling='max', classes=data_classes)\n",
    "    modelv3.summary()\n",
    "    modelv3.name\n",
    "    return modelv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Learning rate:  0.01\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 500, 1)            0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_6 (ZeroPaddin (None, 506, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1/conv (Conv1D)          (None, 500, 24)           192       \n",
      "_________________________________________________________________\n",
      "1_conv (Conv1D)              (None, 500, 48)           1200      \n",
      "_________________________________________________________________\n",
      "1_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "1_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "2_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "2_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "2_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "3_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "3_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "3_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "4_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "4_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "4_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "5_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "5_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "5_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "6_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "6_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "6_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "7_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "7_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "7_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "8_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "8_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "8_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "9_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "9_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "9_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "10_conv (Conv1D)             (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "10_bn (BatchNormalization)   (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "10_relu (Activation)         (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "avg_pool (GlobalAveragePooli (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 6)                 294       \n",
      "=================================================================\n",
      "Total params: 24,774\n",
      "Trainable params: 23,814\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Train on 7998 samples, validate on 4002 samples\n",
      "Epoch 1/300\n",
      "epoch:  0\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 17s 2ms/step - loss: 1.0182 - acc: 0.5819 - prec: 0.7771 - recall: 0.9137 - prec_1: 0.4596 - recall_1: 0.4065 - prec_2: 0.6690 - recall_2: 0.7133 - prec_3: 0.4357 - recall_3: 0.5974 - prec_4: 0.2955 - recall_4: 0.3959 - prec_5: 0.5513 - recall_5: 0.4905 - f1_m: 0.3112 - recall_m: 0.2216 - precision_m: 0.7131 - precision: 0.7131 - recall_6: 0.2216 - f1score: 0.3112 - val_loss: 2.9145 - val_acc: 0.2301 - val_prec: 0.1119 - val_recall: 0.0053 - val_prec_1: 0.1606 - val_recall_1: 0.0536 - val_prec_2: 0.1786 - val_recall_2: 0.1740 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0220 - val_f1_m: 0.1615 - val_recall_m: 0.1484 - val_precision_m: 0.1882 - val_precision: 0.1882 - val_recall_6: 0.1484 - val_f1score: 0.1615\n",
      "Epoch 2/300\n",
      "epoch:  1\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 613us/step - loss: 0.7299 - acc: 0.6849 - prec: 0.8791 - recall: 0.9156 - prec_1: 0.6352 - recall_1: 0.5855 - prec_2: 0.8185 - recall_2: 0.7802 - prec_3: 0.5406 - recall_3: 0.5615 - prec_4: 0.4797 - recall_4: 0.5376 - prec_5: 0.6891 - recall_5: 0.7470 - f1_m: 0.5963 - recall_m: 0.4966 - precision_m: 0.7554 - precision: 0.7554 - recall_6: 0.4966 - f1score: 0.5963 - val_loss: 4.8688 - val_acc: 0.3638 - val_prec: 0.1759 - val_recall: 0.1644 - val_prec_1: 0.1740 - val_recall_1: 0.1555 - val_prec_2: 0.1919 - val_recall_2: 0.0715 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0000e+00 - val_recall_5: 0.0000e+00 - val_f1_m: 0.3471 - val_recall_m: 0.3273 - val_precision_m: 0.3721 - val_precision: 0.3721 - val_recall_6: 0.3273 - val_f1score: 0.3471\n",
      "Epoch 3/300\n",
      "epoch:  2\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 613us/step - loss: 0.6571 - acc: 0.7076 - prec: 0.8974 - recall: 0.9238 - prec_1: 0.6480 - recall_1: 0.6439 - prec_2: 0.8465 - recall_2: 0.7993 - prec_3: 0.5719 - recall_3: 0.5768 - prec_4: 0.5358 - recall_4: 0.5409 - prec_5: 0.7470 - recall_5: 0.7890 - f1_m: 0.6652 - recall_m: 0.5923 - precision_m: 0.7612 - precision: 0.7612 - recall_6: 0.5923 - f1score: 0.6652 - val_loss: 3.2698 - val_acc: 0.4128 - val_prec: 0.1759 - val_recall: 0.1378 - val_prec_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - val_prec_2: 0.1791 - val_recall_2: 0.1054 - val_prec_3: 0.0960 - val_recall_3: 0.0315 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1566 - val_f1_m: 0.4072 - val_recall_m: 0.4025 - val_precision_m: 0.4123 - val_precision: 0.4123 - val_recall_6: 0.4025 - val_f1score: 0.4072\n",
      "Epoch 4/300\n",
      "epoch:  3\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 613us/step - loss: 0.5966 - acc: 0.7438 - prec: 0.9190 - recall: 0.9250 - prec_1: 0.7096 - recall_1: 0.7169 - prec_2: 0.8645 - recall_2: 0.8382 - prec_3: 0.5968 - recall_3: 0.5955 - prec_4: 0.5728 - recall_4: 0.5767 - prec_5: 0.8119 - recall_5: 0.8462 - f1_m: 0.7158 - recall_m: 0.6623 - precision_m: 0.7799 - precision: 0.7799 - recall_6: 0.6623 - f1score: 0.7158 - val_loss: 6.1757 - val_acc: 0.3148 - val_prec: 0.1759 - val_recall: 0.1142 - val_prec_1: 0.1706 - val_recall_1: 0.1689 - val_prec_2: 0.1919 - val_recall_2: 0.0574 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0000e+00 - val_recall_5: 0.0000e+00 - val_f1_m: 0.3133 - val_recall_m: 0.3126 - val_precision_m: 0.3140 - val_precision: 0.3140 - val_recall_6: 0.3126 - val_f1score: 0.3133\n",
      "Epoch 5/300\n",
      "epoch:  4\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 612us/step - loss: 0.5622 - acc: 0.7549 - prec: 0.9242 - recall: 0.9250 - prec_1: 0.7288 - recall_1: 0.7436 - prec_2: 0.8782 - recall_2: 0.8391 - prec_3: 0.6113 - recall_3: 0.5955 - prec_4: 0.5904 - recall_4: 0.5870 - prec_5: 0.8293 - recall_5: 0.8644 - f1_m: 0.7412 - recall_m: 0.7026 - precision_m: 0.7854 - precision: 0.7854 - recall_6: 0.7026 - f1score: 0.7412 - val_loss: 5.0068 - val_acc: 0.3493 - val_prec: 0.1759 - val_recall: 0.1006 - val_prec_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - val_prec_2: 0.1802 - val_recall_2: 0.1665 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1096 - val_f1_m: 0.3466 - val_recall_m: 0.3368 - val_precision_m: 0.3572 - val_precision: 0.3572 - val_recall_6: 0.3368 - val_f1score: 0.3466\n",
      "Epoch 6/300\n",
      "epoch:  5\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 614us/step - loss: 0.5321 - acc: 0.7643 - prec: 0.9190 - recall: 0.9293 - prec_1: 0.7642 - recall_1: 0.7690 - prec_2: 0.8849 - recall_2: 0.8580 - prec_3: 0.6127 - recall_3: 0.5746 - prec_4: 0.5741 - recall_4: 0.5929 - prec_5: 0.8489 - recall_5: 0.8883 - f1_m: 0.7512 - recall_m: 0.7182 - precision_m: 0.7882 - precision: 0.7882 - recall_6: 0.7182 - f1score: 0.7512 - val_loss: 5.8806 - val_acc: 0.3321 - val_prec: 0.1752 - val_recall: 0.1613 - val_prec_1: 0.1703 - val_recall_1: 0.0158 - val_prec_2: 0.1781 - val_recall_2: 0.1840 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0245 - val_recall_5: 0.0036 - val_f1_m: 0.3206 - val_recall_m: 0.3133 - val_precision_m: 0.3292 - val_precision: 0.3292 - val_recall_6: 0.3133 - val_f1score: 0.3206\n",
      "Epoch 7/300\n",
      "epoch:  6\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 613us/step - loss: 0.5041 - acc: 0.7784 - prec: 0.9218 - recall: 0.9380 - prec_1: 0.7834 - recall_1: 0.7999 - prec_2: 0.9020 - recall_2: 0.8658 - prec_3: 0.6395 - recall_3: 0.5718 - prec_4: 0.5977 - recall_4: 0.6295 - prec_5: 0.8681 - recall_5: 0.9040 - f1_m: 0.7691 - recall_m: 0.7408 - precision_m: 0.8002 - precision: 0.8002 - recall_6: 0.7408 - f1score: 0.7691 - val_loss: 5.4323 - val_acc: 0.3693 - val_prec: 0.1734 - val_recall: 0.1717 - val_prec_1: 0.1439 - val_recall_1: 0.0299 - val_prec_2: 0.1828 - val_recall_2: 0.1791 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0235 - val_f1_m: 0.3558 - val_recall_m: 0.3501 - val_precision_m: 0.3630 - val_precision: 0.3630 - val_recall_6: 0.3501 - val_f1score: 0.3558\n",
      "Epoch 8/300\n",
      "epoch:  7\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 609us/step - loss: 0.4886 - acc: 0.7816 - prec: 0.9322 - recall: 0.9435 - prec_1: 0.7733 - recall_1: 0.8042 - prec_2: 0.9040 - recall_2: 0.8611 - prec_3: 0.6188 - recall_3: 0.5967 - prec_4: 0.6077 - recall_4: 0.6167 - prec_5: 0.8539 - recall_5: 0.9000 - f1_m: 0.7748 - recall_m: 0.7476 - precision_m: 0.8047 - precision: 0.8047 - recall_6: 0.7476 - f1score: 0.7748 - val_loss: 6.4897 - val_acc: 0.2919 - val_prec: 0.1684 - val_recall: 0.1737 - val_prec_1: 0.1433 - val_recall_1: 0.0311 - val_prec_2: 0.1887 - val_recall_2: 0.1115 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0320 - val_recall_5: 9.9950e-04 - val_f1_m: 0.2844 - val_recall_m: 0.2776 - val_precision_m: 0.2925 - val_precision: 0.2925 - val_recall_6: 0.2776 - val_f1score: 0.2844\n",
      "Epoch 9/300\n",
      "epoch:  8\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 617us/step - loss: 0.4680 - acc: 0.7888 - prec: 0.9311 - recall: 0.9454 - prec_1: 0.8048 - recall_1: 0.8273 - prec_2: 0.9003 - recall_2: 0.8745 - prec_3: 0.6283 - recall_3: 0.5575 - prec_4: 0.5880 - recall_4: 0.6429 - prec_5: 0.8855 - recall_5: 0.9114 - f1_m: 0.7831 - recall_m: 0.7617 - precision_m: 0.8063 - precision: 0.8063 - recall_6: 0.7617 - f1score: 0.7831 - val_loss: 7.1624 - val_acc: 0.3521 - val_prec: 0.1676 - val_recall: 0.1732 - val_prec_1: 0.0960 - val_recall_1: 0.0202 - val_prec_2: 0.1919 - val_recall_2: 0.1304 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1364 - val_recall_5: 0.0555 - val_f1_m: 0.3472 - val_recall_m: 0.3416 - val_precision_m: 0.3536 - val_precision: 0.3536 - val_recall_6: 0.3416 - val_f1score: 0.3472\n",
      "Epoch 10/300\n",
      "epoch:  9\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 611us/step - loss: 0.4623 - acc: 0.7908 - prec: 0.9304 - recall: 0.9454 - prec_1: 0.8079 - recall_1: 0.8157 - prec_2: 0.9084 - recall_2: 0.8852 - prec_3: 0.6354 - recall_3: 0.5905 - prec_4: 0.6025 - recall_4: 0.6312 - prec_5: 0.8860 - recall_5: 0.9059 - f1_m: 0.7867 - recall_m: 0.7669 - precision_m: 0.8078 - precision: 0.8078 - recall_6: 0.7669 - f1score: 0.7867 - val_loss: 6.5232 - val_acc: 0.4030 - val_prec: 0.1733 - val_recall: 0.1703 - val_prec_1: 0.1740 - val_recall_1: 0.1202 - val_prec_2: 0.1919 - val_recall_2: 0.1493 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0000e+00 - val_recall_5: 0.0000e+00 - val_f1_m: 0.3996 - val_recall_m: 0.3933 - val_precision_m: 0.4066 - val_precision: 0.4066 - val_recall_6: 0.3933 - val_f1score: 0.3996\n",
      "Epoch 11/300\n",
      "epoch:  10\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 612us/step - loss: 0.4502 - acc: 0.7957 - prec: 0.9291 - recall: 0.9511 - prec_1: 0.8242 - recall_1: 0.8339 - prec_2: 0.9056 - recall_2: 0.8877 - prec_3: 0.6436 - recall_3: 0.5439 - prec_4: 0.5979 - recall_4: 0.6858 - prec_5: 0.8875 - recall_5: 0.9157 - f1_m: 0.7919 - recall_m: 0.7712 - precision_m: 0.8141 - precision: 0.8141 - recall_6: 0.7712 - f1score: 0.7919 - val_loss: 1.9596 - val_acc: 0.4810 - val_prec: 0.1759 - val_recall: 0.1640 - val_prec_1: 0.1434 - val_recall_1: 0.0302 - val_prec_2: 0.1808 - val_recall_2: 0.1651 - val_prec_3: 0.0640 - val_recall_3: 0.0225 - val_prec_4: 0.1119 - val_recall_4: 0.0125 - val_prec_5: 0.1684 - val_recall_5: 0.1175 - val_f1_m: 0.4592 - val_recall_m: 0.4438 - val_precision_m: 0.4778 - val_precision: 0.4778 - val_recall_6: 0.4438 - val_f1score: 0.4592\n",
      "Epoch 12/300\n",
      "epoch:  11\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 618us/step - loss: 0.4455 - acc: 0.7884 - prec: 0.9343 - recall: 0.9472 - prec_1: 0.8193 - recall_1: 0.8230 - prec_2: 0.9087 - recall_2: 0.8943 - prec_3: 0.6141 - recall_3: 0.5521 - prec_4: 0.5812 - recall_4: 0.6395 - prec_5: 0.8855 - recall_5: 0.9078 - f1_m: 0.7840 - recall_m: 0.7651 - precision_m: 0.8042 - precision: 0.8042 - recall_6: 0.7651 - f1score: 0.7840 - val_loss: 6.1253 - val_acc: 0.3378 - val_prec: 0.1667 - val_recall: 0.1737 - val_prec_1: 0.1595 - val_recall_1: 0.0613 - val_prec_2: 0.1823 - val_recall_2: 0.1285 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0480 - val_recall_5: 0.0012 - val_f1_m: 0.3368 - val_recall_m: 0.3331 - val_precision_m: 0.3408 - val_precision: 0.3408 - val_recall_6: 0.3331 - val_f1score: 0.3368\n",
      "Epoch 13/300\n",
      "epoch:  12\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 612us/step - loss: 0.4307 - acc: 0.7986 - prec: 0.9400 - recall: 0.9601 - prec_1: 0.8197 - recall_1: 0.8356 - prec_2: 0.9189 - recall_2: 0.8834 - prec_3: 0.6399 - recall_3: 0.5847 - prec_4: 0.6015 - recall_4: 0.6424 - prec_5: 0.8856 - recall_5: 0.9185 - f1_m: 0.7950 - recall_m: 0.7783 - precision_m: 0.8128 - precision: 0.8128 - recall_6: 0.7783 - f1score: 0.7950 - val_loss: 5.0921 - val_acc: 0.3978 - val_prec: 0.1759 - val_recall: 0.1375 - val_prec_1: 0.1599 - val_recall_1: 0.0360 - val_prec_2: 0.1759 - val_recall_2: 0.0789 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1662 - val_f1_m: 0.3948 - val_recall_m: 0.3913 - val_precision_m: 0.3987 - val_precision: 0.3987 - val_recall_6: 0.3913 - val_f1score: 0.3948\n",
      "Epoch 14/300\n",
      "epoch:  13\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 629us/step - loss: 0.4247 - acc: 0.8003 - prec: 0.9372 - recall: 0.9555 - prec_1: 0.8127 - recall_1: 0.8352 - prec_2: 0.9289 - recall_2: 0.8881 - prec_3: 0.6376 - recall_3: 0.5781 - prec_4: 0.5997 - recall_4: 0.6504 - prec_5: 0.8838 - recall_5: 0.9203 - f1_m: 0.7987 - recall_m: 0.7827 - precision_m: 0.8158 - precision: 0.8158 - recall_6: 0.7827 - f1score: 0.7987 - val_loss: 4.5950 - val_acc: 0.4313 - val_prec: 0.1759 - val_recall: 0.1226 - val_prec_1: 0.1279 - val_recall_1: 0.0186 - val_prec_2: 0.1674 - val_recall_2: 0.1828 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1374 - val_f1_m: 0.4262 - val_recall_m: 0.4228 - val_precision_m: 0.4299 - val_precision: 0.4299 - val_recall_6: 0.4228 - val_f1score: 0.4262\n",
      "Epoch 15/300\n",
      "epoch:  14\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 613us/step - loss: 0.4160 - acc: 0.8127 - prec: 0.9412 - recall: 0.9598 - prec_1: 0.8356 - recall_1: 0.8533 - prec_2: 0.9197 - recall_2: 0.8871 - prec_3: 0.6725 - recall_3: 0.5953 - prec_4: 0.6259 - recall_4: 0.6921 - prec_5: 0.9066 - recall_5: 0.9259 - f1_m: 0.8100 - recall_m: 0.7958 - precision_m: 0.8249 - precision: 0.8249 - recall_6: 0.7958 - f1score: 0.8100 - val_loss: 5.9026 - val_acc: 0.3071 - val_prec: 0.1759 - val_recall: 0.0710 - val_prec_1: 0.1279 - val_recall_1: 0.0083 - val_prec_2: 0.1759 - val_recall_2: 0.0766 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1666 - val_recall_5: 0.1679 - val_f1_m: 0.3040 - val_recall_m: 0.3013 - val_precision_m: 0.3070 - val_precision: 0.3070 - val_recall_6: 0.3013 - val_f1score: 0.3040\n",
      "Epoch 16/300\n",
      "epoch:  15\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 615us/step - loss: 0.4152 - acc: 0.8045 - prec: 0.9463 - recall: 0.9614 - prec_1: 0.8233 - recall_1: 0.8368 - prec_2: 0.9038 - recall_2: 0.8954 - prec_3: 0.6577 - recall_3: 0.5879 - prec_4: 0.6272 - recall_4: 0.6688 - prec_5: 0.8956 - recall_5: 0.9168 - f1_m: 0.8033 - recall_m: 0.7899 - precision_m: 0.8173 - precision: 0.8173 - recall_6: 0.7899 - f1score: 0.8033 - val_loss: 6.3508 - val_acc: 0.3351 - val_prec: 0.1671 - val_recall: 0.1734 - val_prec_1: 0.1439 - val_recall_1: 0.0452 - val_prec_2: 0.1919 - val_recall_2: 0.1231 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0237 - val_f1_m: 0.3322 - val_recall_m: 0.3291 - val_precision_m: 0.3355 - val_precision: 0.3355 - val_recall_6: 0.3291 - val_f1score: 0.3322\n",
      "Epoch 17/300\n",
      "epoch:  16\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 620us/step - loss: 0.4125 - acc: 0.8102 - prec: 0.9474 - recall: 0.9616 - prec_1: 0.8368 - recall_1: 0.8507 - prec_2: 0.9105 - recall_2: 0.8942 - prec_3: 0.6570 - recall_3: 0.5927 - prec_4: 0.6174 - recall_4: 0.6666 - prec_5: 0.8940 - recall_5: 0.9210 - f1_m: 0.8070 - recall_m: 0.7931 - precision_m: 0.8217 - precision: 0.8217 - recall_6: 0.7931 - f1score: 0.8070 - val_loss: 1.4174 - val_acc: 0.6292 - val_prec: 0.1747 - val_recall: 0.1674 - val_prec_1: 0.1599 - val_recall_1: 0.0576 - val_prec_2: 0.1890 - val_recall_2: 0.1828 - val_prec_3: 0.1708 - val_recall_3: 0.1248 - val_prec_4: 0.1759 - val_recall_4: 0.0920 - val_prec_5: 0.1364 - val_recall_5: 0.0620 - val_f1_m: 0.6188 - val_recall_m: 0.6067 - val_precision_m: 0.6331 - val_precision: 0.6331 - val_recall_6: 0.6067 - val_f1score: 0.6188\n",
      "Epoch 18/300\n",
      "epoch:  17\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 618us/step - loss: 0.4038 - acc: 0.8116 - prec: 0.9458 - recall: 0.9627 - prec_1: 0.8335 - recall_1: 0.8517 - prec_2: 0.9173 - recall_2: 0.9064 - prec_3: 0.6548 - recall_3: 0.6017 - prec_4: 0.6202 - recall_4: 0.6659 - prec_5: 0.9045 - recall_5: 0.9228 - f1_m: 0.8093 - recall_m: 0.7966 - precision_m: 0.8226 - precision: 0.8226 - recall_6: 0.7966 - f1score: 0.8093 - val_loss: 1.0098 - val_acc: 0.6169 - val_prec: 0.1759 - val_recall: 0.1526 - val_prec_1: 0.1759 - val_recall_1: 0.0755 - val_prec_2: 0.1919 - val_recall_2: 0.1399 - val_prec_3: 0.1759 - val_recall_3: 0.1065 - val_prec_4: 0.0800 - val_recall_4: 0.0165 - val_prec_5: 0.1684 - val_recall_5: 0.1636 - val_f1_m: 0.6084 - val_recall_m: 0.5902 - val_precision_m: 0.6307 - val_precision: 0.6307 - val_recall_6: 0.5902 - val_f1score: 0.6084\n",
      "Epoch 19/300\n",
      "epoch:  18\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 618us/step - loss: 0.3968 - acc: 0.8165 - prec: 0.9451 - recall: 0.9621 - prec_1: 0.8539 - recall_1: 0.8710 - prec_2: 0.9248 - recall_2: 0.9039 - prec_3: 0.6465 - recall_3: 0.6241 - prec_4: 0.6324 - recall_4: 0.6435 - prec_5: 0.9106 - recall_5: 0.9323 - f1_m: 0.8141 - recall_m: 0.8012 - precision_m: 0.8277 - precision: 0.8277 - recall_6: 0.8012 - f1score: 0.8141 - val_loss: 0.6797 - val_acc: 0.7201 - val_prec: 0.1736 - val_recall: 0.1691 - val_prec_1: 0.1740 - val_recall_1: 0.1155 - val_prec_2: 0.1919 - val_recall_2: 0.1633 - val_prec_3: 0.1439 - val_recall_3: 0.0497 - val_prec_4: 0.1759 - val_recall_4: 0.1374 - val_prec_5: 0.1684 - val_recall_5: 0.1363 - val_f1_m: 0.7043 - val_recall_m: 0.6754 - val_precision_m: 0.7427 - val_precision: 0.7427 - val_recall_6: 0.6754 - val_f1score: 0.7043\n",
      "Epoch 20/300\n",
      "epoch:  19\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 609us/step - loss: 0.3970 - acc: 0.8157 - prec: 0.9461 - recall: 0.9748 - prec_1: 0.8510 - recall_1: 0.8646 - prec_2: 0.9187 - recall_2: 0.9021 - prec_3: 0.6568 - recall_3: 0.6296 - prec_4: 0.6361 - recall_4: 0.6407 - prec_5: 0.9064 - recall_5: 0.9277 - f1_m: 0.8114 - recall_m: 0.7987 - precision_m: 0.8247 - precision: 0.8247 - recall_6: 0.7987 - f1score: 0.8114 - val_loss: 1.9083 - val_acc: 0.5885 - val_prec: 0.1753 - val_recall: 0.1714 - val_prec_1: 0.1759 - val_recall_1: 0.0915 - val_prec_2: 0.1919 - val_recall_2: 0.1518 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1439 - val_recall_4: 0.0535 - val_prec_5: 0.1684 - val_recall_5: 0.1618 - val_f1_m: 0.5861 - val_recall_m: 0.5772 - val_precision_m: 0.5957 - val_precision: 0.5957 - val_recall_6: 0.5772 - val_f1score: 0.5861\n",
      "Epoch 21/300\n",
      "epoch:  20\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 617us/step - loss: 0.3908 - acc: 0.8210 - prec: 0.9437 - recall: 0.9681 - prec_1: 0.8570 - recall_1: 0.8580 - prec_2: 0.9191 - recall_2: 0.9088 - prec_3: 0.6659 - recall_3: 0.6449 - prec_4: 0.6533 - recall_4: 0.6574 - prec_5: 0.9027 - recall_5: 0.9260 - f1_m: 0.8185 - recall_m: 0.8065 - precision_m: 0.8312 - precision: 0.8312 - recall_6: 0.8065 - f1score: 0.8185 - val_loss: 6.4966 - val_acc: 0.3558 - val_prec: 0.1667 - val_recall: 0.1754 - val_prec_1: 0.1596 - val_recall_1: 0.0633 - val_prec_2: 0.1919 - val_recall_2: 0.0779 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0569 - val_f1_m: 0.3486 - val_recall_m: 0.3428 - val_precision_m: 0.3550 - val_precision: 0.3550 - val_recall_6: 0.3428 - val_f1score: 0.3486\n",
      "Epoch 22/300\n",
      "epoch:  21\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 620us/step - loss: 0.3871 - acc: 0.8173 - prec: 0.9481 - recall: 0.9682 - prec_1: 0.8450 - recall_1: 0.8601 - prec_2: 0.9163 - recall_2: 0.9117 - prec_3: 0.6526 - recall_3: 0.6158 - prec_4: 0.6382 - recall_4: 0.6643 - prec_5: 0.9062 - recall_5: 0.9237 - f1_m: 0.8151 - recall_m: 0.8036 - precision_m: 0.8272 - precision: 0.8272 - recall_6: 0.8036 - f1score: 0.8151 - val_loss: 6.2300 - val_acc: 0.3956 - val_prec: 0.1759 - val_recall: 0.1325 - val_prec_1: 0.1698 - val_recall_1: 0.1379 - val_prec_2: 0.1919 - val_recall_2: 0.1550 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0725 - val_recall_5: 0.0048 - val_f1_m: 0.3943 - val_recall_m: 0.3923 - val_precision_m: 0.3965 - val_precision: 0.3965 - val_recall_6: 0.3923 - val_f1score: 0.3943\n",
      "Epoch 23/300\n",
      "epoch:  22\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 625us/step - loss: 0.3830 - acc: 0.8185 - prec: 0.9509 - recall: 0.9631 - prec_1: 0.8612 - recall_1: 0.8553 - prec_2: 0.9168 - recall_2: 0.9181 - prec_3: 0.6640 - recall_3: 0.6024 - prec_4: 0.6262 - recall_4: 0.6800 - prec_5: 0.9109 - recall_5: 0.9323 - f1_m: 0.8169 - recall_m: 0.8057 - precision_m: 0.8286 - precision: 0.8286 - recall_6: 0.8057 - f1score: 0.8169 - val_loss: 5.5430 - val_acc: 0.4308 - val_prec: 0.1759 - val_recall: 0.1517 - val_prec_1: 0.1599 - val_recall_1: 0.0457 - val_prec_2: 0.1759 - val_recall_2: 0.0908 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1669 - val_f1_m: 0.4282 - val_recall_m: 0.4263 - val_precision_m: 0.4303 - val_precision: 0.4303 - val_recall_6: 0.4263 - val_f1score: 0.4282\n",
      "Epoch 24/300\n",
      "epoch:  23\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 624us/step - loss: 0.3757 - acc: 0.8285 - prec: 0.9545 - recall: 0.9654 - prec_1: 0.8571 - recall_1: 0.8772 - prec_2: 0.9349 - recall_2: 0.9064 - prec_3: 0.6730 - recall_3: 0.6498 - prec_4: 0.6578 - recall_4: 0.6657 - prec_5: 0.9144 - recall_5: 0.9295 - f1_m: 0.8273 - recall_m: 0.8152 - precision_m: 0.8400 - precision: 0.8400 - recall_6: 0.8152 - f1score: 0.8273 - val_loss: 1.6359 - val_acc: 0.5342 - val_prec: 0.1759 - val_recall: 0.1626 - val_prec_1: 0.1599 - val_recall_1: 0.0528 - val_prec_2: 0.1713 - val_recall_2: 0.1909 - val_prec_3: 0.0960 - val_recall_3: 0.0215 - val_prec_4: 0.1797 - val_recall_4: 0.1119 - val_prec_5: 0.1364 - val_recall_5: 0.0462 - val_f1_m: 0.5032 - val_recall_m: 0.4788 - val_precision_m: 0.5411 - val_precision: 0.5411 - val_recall_6: 0.4788 - val_f1score: 0.5032\n",
      "Epoch 25/300\n",
      "epoch:  24\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 615us/step - loss: 0.3724 - acc: 0.8262 - prec: 0.9487 - recall: 0.9703 - prec_1: 0.8643 - recall_1: 0.8623 - prec_2: 0.9318 - recall_2: 0.9135 - prec_3: 0.6829 - recall_3: 0.6152 - prec_4: 0.6449 - recall_4: 0.6903 - prec_5: 0.9101 - recall_5: 0.9349 - f1_m: 0.8251 - recall_m: 0.8148 - precision_m: 0.8357 - precision: 0.8357 - recall_6: 0.8148 - f1score: 0.8251 - val_loss: 9.7371 - val_acc: 0.2281 - val_prec: 0.1667 - val_recall: 0.1759 - val_prec_1: 0.1599 - val_recall_1: 0.0408 - val_prec_2: 0.1599 - val_recall_2: 0.0214 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0480 - val_recall_5: 0.0025 - val_f1_m: 0.2279 - val_recall_m: 0.2276 - val_precision_m: 0.2283 - val_precision: 0.2283 - val_recall_6: 0.2276 - val_f1score: 0.2279\n",
      "Epoch 26/300\n",
      "epoch:  25\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 632us/step - loss: 0.3761 - acc: 0.8232 - prec: 0.9606 - recall: 0.9732 - prec_1: 0.8563 - recall_1: 0.8583 - prec_2: 0.9252 - recall_2: 0.9126 - prec_3: 0.6557 - recall_3: 0.6261 - prec_4: 0.6334 - recall_4: 0.6509 - prec_5: 0.9148 - recall_5: 0.9335 - f1_m: 0.8215 - recall_m: 0.8112 - precision_m: 0.8323 - precision: 0.8323 - recall_6: 0.8112 - f1score: 0.8215 - val_loss: 0.7992 - val_acc: 0.6947 - val_prec: 0.1759 - val_recall: 0.1404 - val_prec_1: 0.1755 - val_recall_1: 0.0711 - val_prec_2: 0.1844 - val_recall_2: 0.1866 - val_prec_3: 0.1696 - val_recall_3: 0.1215 - val_prec_4: 0.1724 - val_recall_4: 0.1064 - val_prec_5: 0.1684 - val_recall_5: 0.1263 - val_f1_m: 0.6773 - val_recall_m: 0.6487 - val_precision_m: 0.7150 - val_precision: 0.7150 - val_recall_6: 0.6487 - val_f1score: 0.6773\n",
      "Epoch 27/300\n",
      "epoch:  26\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 616us/step - loss: 0.3748 - acc: 0.8195 - prec: 0.9436 - recall: 0.9703 - prec_1: 0.8646 - recall_1: 0.8688 - prec_2: 0.9234 - recall_2: 0.9115 - prec_3: 0.6546 - recall_3: 0.6335 - prec_4: 0.6288 - recall_4: 0.6356 - prec_5: 0.9143 - recall_5: 0.9238 - f1_m: 0.8171 - recall_m: 0.8073 - precision_m: 0.8273 - precision: 0.8273 - recall_6: 0.8073 - f1score: 0.8171 - val_loss: 1.3928 - val_acc: 0.5905 - val_prec: 0.1759 - val_recall: 0.1674 - val_prec_1: 0.1596 - val_recall_1: 0.0741 - val_prec_2: 0.1919 - val_recall_2: 0.0969 - val_prec_3: 0.1709 - val_recall_3: 0.1177 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1641 - val_f1_m: 0.5879 - val_recall_m: 0.5797 - val_precision_m: 0.5969 - val_precision: 0.5969 - val_recall_6: 0.5797 - val_f1score: 0.5879\n",
      "Epoch 28/300\n",
      "epoch:  27\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 6s 740us/step - loss: 0.3659 - acc: 0.8253 - prec: 0.9520 - recall: 0.9713 - prec_1: 0.8619 - recall_1: 0.8810 - prec_2: 0.9301 - recall_2: 0.9176 - prec_3: 0.6558 - recall_3: 0.6335 - prec_4: 0.6483 - recall_4: 0.6598 - prec_5: 0.9272 - recall_5: 0.9347 - f1_m: 0.8243 - recall_m: 0.8137 - precision_m: 0.8353 - precision: 0.8353 - recall_6: 0.8137 - f1score: 0.8243 - val_loss: 0.9761 - val_acc: 0.6674 - val_prec: 0.1716 - val_recall: 0.1569 - val_prec_1: 0.1749 - val_recall_1: 0.0662 - val_prec_2: 0.1804 - val_recall_2: 0.1814 - val_prec_3: 0.1439 - val_recall_3: 0.0719 - val_prec_4: 0.1759 - val_recall_4: 0.1344 - val_prec_5: 0.1684 - val_recall_5: 0.1086 - val_f1_m: 0.6555 - val_recall_m: 0.6347 - val_precision_m: 0.6823 - val_precision: 0.6823 - val_recall_6: 0.6347 - val_f1score: 0.6555\n",
      "Epoch 29/300\n",
      "epoch:  28\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 6s 688us/step - loss: 0.3639 - acc: 0.8286 - prec: 0.9529 - recall: 0.9741 - prec_1: 0.8740 - recall_1: 0.8742 - prec_2: 0.9236 - recall_2: 0.9172 - prec_3: 0.6585 - recall_3: 0.6566 - prec_4: 0.6582 - recall_4: 0.6439 - prec_5: 0.9122 - recall_5: 0.9344 - f1_m: 0.8266 - recall_m: 0.8172 - precision_m: 0.8363 - precision: 0.8363 - recall_6: 0.8172 - f1score: 0.8266 - val_loss: 6.5449 - val_acc: 0.3988 - val_prec: 0.1668 - val_recall: 0.1739 - val_prec_1: 0.1752 - val_recall_1: 0.0727 - val_prec_2: 0.1919 - val_recall_2: 0.1159 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0623 - val_f1_m: 0.3969 - val_recall_m: 0.3911 - val_precision_m: 0.4031 - val_precision: 0.4031 - val_recall_6: 0.3911 - val_f1score: 0.3969\n",
      "Epoch 30/300\n",
      "epoch:  29\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3610 - acc: 0.8296 - prec: 0.9565 - recall: 0.9716 - prec_1: 0.8689 - recall_1: 0.8627 - prec_2: 0.9162 - recall_2: 0.9150 - prec_3: 0.6733 - recall_3: 0.6490 - prec_4: 0.6485 - recall_4: 0.6715 - prec_5: 0.9161 - recall_5: 0.9424 - f1_m: 0.8281 - recall_m: 0.8187 - precision_m: 0.8379 - precision: 0.8379 - recall_6: 0.8187 - f1score: 0.8281 - val_loss: 3.2797 - val_acc: 0.5097 - val_prec: 0.1747 - val_recall: 0.1695 - val_prec_1: 0.1742 - val_recall_1: 0.1425 - val_prec_2: 0.1599 - val_recall_2: 0.0563 - val_prec_3: 0.0960 - val_recall_3: 0.0277 - val_prec_4: 0.0800 - val_recall_4: 0.0117 - val_prec_5: 0.1684 - val_recall_5: 0.1242 - val_f1_m: 0.5030 - val_recall_m: 0.4990 - val_precision_m: 0.5076 - val_precision: 0.5076 - val_recall_6: 0.4990 - val_f1score: 0.5030\n",
      "Epoch 31/300\n",
      "epoch:  30\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3650 - acc: 0.8245 - prec: 0.9578 - recall: 0.9740 - prec_1: 0.8681 - recall_1: 0.8707 - prec_2: 0.9250 - recall_2: 0.9093 - prec_3: 0.6446 - recall_3: 0.6449 - prec_4: 0.6546 - recall_4: 0.6475 - prec_5: 0.9127 - recall_5: 0.9318 - f1_m: 0.8234 - recall_m: 0.8136 - precision_m: 0.8335 - precision: 0.8335 - recall_6: 0.8136 - f1score: 0.8234 - val_loss: 3.2601 - val_acc: 0.4710 - val_prec: 0.1664 - val_recall: 0.1725 - val_prec_1: 0.1599 - val_recall_1: 0.0346 - val_prec_2: 0.1828 - val_recall_2: 0.1418 - val_prec_3: 0.0640 - val_recall_3: 0.0080 - val_prec_4: 0.1649 - val_recall_4: 0.0990 - val_prec_5: 0.1364 - val_recall_5: 0.0510 - val_f1_m: 0.4565 - val_recall_m: 0.4488 - val_precision_m: 0.4673 - val_precision: 0.4673 - val_recall_6: 0.4488 - val_f1score: 0.4565\n",
      "Epoch 32/300\n",
      "epoch:  31\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3599 - acc: 0.8267 - prec: 0.9539 - recall: 0.9668 - prec_1: 0.8635 - recall_1: 0.8752 - prec_2: 0.9275 - recall_2: 0.9156 - prec_3: 0.6606 - recall_3: 0.6245 - prec_4: 0.6375 - recall_4: 0.6705 - prec_5: 0.9177 - recall_5: 0.9303 - f1_m: 0.8261 - recall_m: 0.8177 - precision_m: 0.8348 - precision: 0.8348 - recall_6: 0.8177 - f1score: 0.8261 - val_loss: 1.7925 - val_acc: 0.6414 - val_prec: 0.1728 - val_recall: 0.1710 - val_prec_1: 0.1752 - val_recall_1: 0.1003 - val_prec_2: 0.1839 - val_recall_2: 0.1809 - val_prec_3: 0.1599 - val_recall_3: 0.0872 - val_prec_4: 0.0160 - val_recall_4: 4.9975e-04 - val_prec_5: 0.1684 - val_recall_5: 0.1440 - val_f1_m: 0.6338 - val_recall_m: 0.6219 - val_precision_m: 0.6476 - val_precision: 0.6476 - val_recall_6: 0.6219 - val_f1score: 0.6338\n",
      "Epoch 33/300\n",
      "epoch:  32\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3533 - acc: 0.8360 - prec: 0.9611 - recall: 0.9768 - prec_1: 0.8725 - recall_1: 0.8911 - prec_2: 0.9260 - recall_2: 0.9160 - prec_3: 0.6634 - recall_3: 0.6504 - prec_4: 0.6549 - recall_4: 0.6620 - prec_5: 0.9337 - recall_5: 0.9390 - f1_m: 0.8348 - recall_m: 0.8256 - precision_m: 0.8443 - precision: 0.8443 - recall_6: 0.8256 - f1score: 0.8348 - val_loss: 1.0577 - val_acc: 0.5915 - val_prec: 0.1759 - val_recall: 0.0838 - val_prec_1: 0.1744 - val_recall_1: 0.1484 - val_prec_2: 0.1823 - val_recall_2: 0.1568 - val_prec_3: 0.0800 - val_recall_3: 0.0275 - val_prec_4: 0.1649 - val_recall_4: 0.1509 - val_prec_5: 0.1524 - val_recall_5: 0.0698 - val_f1_m: 0.5865 - val_recall_m: 0.5742 - val_precision_m: 0.6022 - val_precision: 0.6022 - val_recall_6: 0.5742 - val_f1score: 0.5865\n",
      "Epoch 34/300\n",
      "epoch:  33\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 6s 768us/step - loss: 0.3475 - acc: 0.8343 - prec: 0.9560 - recall: 0.9752 - prec_1: 0.8733 - recall_1: 0.8893 - prec_2: 0.9341 - recall_2: 0.9229 - prec_3: 0.6624 - recall_3: 0.6402 - prec_4: 0.6540 - recall_4: 0.6612 - prec_5: 0.9316 - recall_5: 0.9358 - f1_m: 0.8334 - recall_m: 0.8257 - precision_m: 0.8414 - precision: 0.8414 - recall_6: 0.8257 - f1score: 0.8334 - val_loss: 6.4917 - val_acc: 0.4268 - val_prec: 0.1713 - val_recall: 0.1724 - val_prec_1: 0.1735 - val_recall_1: 0.1277 - val_prec_2: 0.1759 - val_recall_2: 0.0388 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1098 - val_f1_m: 0.4242 - val_recall_m: 0.4213 - val_precision_m: 0.4274 - val_precision: 0.4274 - val_recall_6: 0.4213 - val_f1score: 0.4242\n",
      "Epoch 35/300\n",
      "epoch:  34\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 616us/step - loss: 0.3579 - acc: 0.8291 - prec: 0.9592 - recall: 0.9742 - prec_1: 0.8600 - recall_1: 0.8860 - prec_2: 0.9273 - recall_2: 0.9119 - prec_3: 0.6617 - recall_3: 0.6410 - prec_4: 0.6445 - recall_4: 0.6632 - prec_5: 0.9244 - recall_5: 0.9374 - f1_m: 0.8294 - recall_m: 0.8216 - precision_m: 0.8374 - precision: 0.8374 - recall_6: 0.8216 - f1score: 0.8294 - val_loss: 4.3711 - val_acc: 0.4250 - val_prec: 0.1667 - val_recall: 0.1757 - val_prec_1: 0.1595 - val_recall_1: 0.0624 - val_prec_2: 0.1919 - val_recall_2: 0.1349 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0800 - val_f1_m: 0.4212 - val_recall_m: 0.4135 - val_precision_m: 0.4298 - val_precision: 0.4298 - val_recall_6: 0.4135 - val_f1score: 0.4212\n",
      "Epoch 36/300\n",
      "epoch:  35\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 657us/step - loss: 0.3518 - acc: 0.8321 - prec: 0.9590 - recall: 0.9765 - prec_1: 0.8851 - recall_1: 0.8922 - prec_2: 0.9199 - recall_2: 0.9143 - prec_3: 0.6487 - recall_3: 0.6639 - prec_4: 0.6644 - recall_4: 0.6375 - prec_5: 0.9274 - recall_5: 0.9433 - f1_m: 0.8312 - recall_m: 0.8223 - precision_m: 0.8404 - precision: 0.8404 - recall_6: 0.8223 - f1score: 0.8312 - val_loss: 4.4719 - val_acc: 0.4243 - val_prec: 0.1667 - val_recall: 0.1742 - val_prec_1: 0.1599 - val_recall_1: 0.0439 - val_prec_2: 0.1919 - val_recall_2: 0.1081 - val_prec_3: 0.1709 - val_recall_3: 0.0777 - val_prec_4: 0.0480 - val_recall_4: 0.0012 - val_prec_5: 0.1364 - val_recall_5: 0.0492 - val_f1_m: 0.4204 - val_recall_m: 0.4155 - val_precision_m: 0.4265 - val_precision: 0.4265 - val_recall_6: 0.4155 - val_f1score: 0.4204\n",
      "Epoch 37/300\n",
      "epoch:  36\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 6s 692us/step - loss: 0.3509 - acc: 0.8298 - prec: 0.9563 - recall: 0.9826 - prec_1: 0.8768 - recall_1: 0.8848 - prec_2: 0.9344 - recall_2: 0.9262 - prec_3: 0.6422 - recall_3: 0.6375 - prec_4: 0.6489 - recall_4: 0.6440 - prec_5: 0.9274 - recall_5: 0.9350 - f1_m: 0.8291 - recall_m: 0.8203 - precision_m: 0.8382 - precision: 0.8382 - recall_6: 0.8203 - f1score: 0.8291 - val_loss: 3.5915 - val_acc: 0.5220 - val_prec: 0.1753 - val_recall: 0.1606 - val_prec_1: 0.1279 - val_recall_1: 0.0287 - val_prec_2: 0.1814 - val_recall_2: 0.1494 - val_prec_3: 0.1279 - val_recall_3: 0.0524 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1598 - val_f1_m: 0.4955 - val_recall_m: 0.4893 - val_precision_m: 0.5040 - val_precision: 0.5040 - val_recall_6: 0.4893 - val_f1score: 0.4955\n",
      "Epoch 38/300\n",
      "epoch:  37\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.3492 - acc: 0.8316 - prec: 0.9636 - recall: 0.9757 - prec_1: 0.8815 - recall_1: 0.8852 - prec_2: 0.9238 - recall_2: 0.9174 - prec_3: 0.6589 - recall_3: 0.6560 - prec_4: 0.6611 - recall_4: 0.6484 - prec_5: 0.9242 - recall_5: 0.9402 - f1_m: 0.8314 - recall_m: 0.8235 - precision_m: 0.8395 - precision: 0.8395 - recall_6: 0.8235 - f1score: 0.8314 - val_loss: 0.9061 - val_acc: 0.6749 - val_prec: 0.1732 - val_recall: 0.1664 - val_prec_1: 0.1734 - val_recall_1: 0.1147 - val_prec_2: 0.1812 - val_recall_2: 0.1733 - val_prec_3: 0.1759 - val_recall_3: 0.0835 - val_prec_4: 0.1599 - val_recall_4: 0.0672 - val_prec_5: 0.1684 - val_recall_5: 0.1229 - val_f1_m: 0.6515 - val_recall_m: 0.6279 - val_precision_m: 0.6887 - val_precision: 0.6887 - val_recall_6: 0.6279 - val_f1score: 0.6515\n",
      "Epoch 39/300\n",
      "epoch:  38\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3422 - acc: 0.8403 - prec: 0.9624 - recall: 0.9747 - prec_1: 0.8788 - recall_1: 0.8927 - prec_2: 0.9298 - recall_2: 0.9233 - prec_3: 0.6713 - recall_3: 0.6784 - prec_4: 0.6676 - recall_4: 0.6547 - prec_5: 0.9356 - recall_5: 0.9461 - f1_m: 0.8395 - recall_m: 0.8316 - precision_m: 0.8476 - precision: 0.8476 - recall_6: 0.8316 - f1score: 0.8395 - val_loss: 3.1900 - val_acc: 0.4850 - val_prec: 0.1662 - val_recall: 0.1626 - val_prec_1: 0.1599 - val_recall_1: 0.0443 - val_prec_2: 0.1839 - val_recall_2: 0.1360 - val_prec_3: 0.1709 - val_recall_3: 0.1399 - val_prec_4: 0.1439 - val_recall_4: 0.0177 - val_prec_5: 0.0885 - val_recall_5: 0.0190 - val_f1_m: 0.4799 - val_recall_m: 0.4750 - val_precision_m: 0.4855 - val_precision: 0.4855 - val_recall_6: 0.4750 - val_f1score: 0.4799\n",
      "Epoch 40/300\n",
      "epoch:  39\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.3440 - acc: 0.8332 - prec: 0.9565 - recall: 0.9775 - prec_1: 0.8802 - recall_1: 0.8965 - prec_2: 0.9340 - recall_2: 0.9206 - prec_3: 0.6549 - recall_3: 0.6503 - prec_4: 0.6580 - recall_4: 0.6556 - prec_5: 0.9254 - recall_5: 0.9406 - f1_m: 0.8334 - recall_m: 0.8253 - precision_m: 0.8418 - precision: 0.8418 - recall_6: 0.8253 - f1score: 0.8334 - val_loss: 5.7672 - val_acc: 0.5375 - val_prec: 0.1734 - val_recall: 0.1727 - val_prec_1: 0.1739 - val_recall_1: 0.1467 - val_prec_2: 0.1919 - val_recall_2: 0.1270 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1224 - val_f1_m: 0.5357 - val_recall_m: 0.5292 - val_precision_m: 0.5426 - val_precision: 0.5426 - val_recall_6: 0.5292 - val_f1score: 0.5357\n",
      "Epoch 41/300\n",
      "epoch:  40\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.3426 - acc: 0.8382 - prec: 0.9615 - recall: 0.9780 - prec_1: 0.8828 - recall_1: 0.9067 - prec_2: 0.9355 - recall_2: 0.9253 - prec_3: 0.6659 - recall_3: 0.6523 - prec_4: 0.6491 - recall_4: 0.6578 - prec_5: 0.9340 - recall_5: 0.9439 - f1_m: 0.8371 - recall_m: 0.8301 - precision_m: 0.8444 - precision: 0.8444 - recall_6: 0.8301 - f1score: 0.8371 - val_loss: 5.5326 - val_acc: 0.4173 - val_prec: 0.1676 - val_recall: 0.1729 - val_prec_1: 0.1596 - val_recall_1: 0.0875 - val_prec_2: 0.1806 - val_recall_2: 0.1140 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0696 - val_f1_m: 0.4153 - val_recall_m: 0.4103 - val_precision_m: 0.4211 - val_precision: 0.4211 - val_recall_6: 0.4103 - val_f1score: 0.4153\n",
      "Epoch 42/300\n",
      "epoch:  41\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.3393 - acc: 0.8391 - prec: 0.9593 - recall: 0.9796 - prec_1: 0.8852 - recall_1: 0.8978 - prec_2: 0.9392 - recall_2: 0.9239 - prec_3: 0.6692 - recall_3: 0.6729 - prec_4: 0.6679 - recall_4: 0.6498 - prec_5: 0.9330 - recall_5: 0.9398 - f1_m: 0.8394 - recall_m: 0.8321 - precision_m: 0.8470 - precision: 0.8470 - recall_6: 0.8321 - f1score: 0.8394 - val_loss: 3.6048 - val_acc: 0.4468 - val_prec: 0.1759 - val_recall: 0.0848 - val_prec_1: 0.1599 - val_recall_1: 0.0619 - val_prec_2: 0.1759 - val_recall_2: 0.0534 - val_prec_3: 0.1759 - val_recall_3: 0.1050 - val_prec_4: 0.0320 - val_recall_4: 4.9975e-04 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.4441 - val_recall_m: 0.4408 - val_precision_m: 0.4477 - val_precision: 0.4477 - val_recall_6: 0.4408 - val_f1score: 0.4441\n",
      "Epoch 43/300\n",
      "epoch:  42\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.3380 - acc: 0.8432 - prec: 0.9644 - recall: 0.9781 - prec_1: 0.8879 - recall_1: 0.9005 - prec_2: 0.9423 - recall_2: 0.9276 - prec_3: 0.6671 - recall_3: 0.6692 - prec_4: 0.6748 - recall_4: 0.6614 - prec_5: 0.9280 - recall_5: 0.9457 - f1_m: 0.8429 - recall_m: 0.8363 - precision_m: 0.8496 - precision: 0.8496 - recall_6: 0.8363 - f1score: 0.8429 - val_loss: 5.7693 - val_acc: 0.4733 - val_prec: 0.1599 - val_recall: 0.1031 - val_prec_1: 0.1756 - val_recall_1: 0.0923 - val_prec_2: 0.1823 - val_recall_2: 0.1419 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1666 - val_recall_5: 0.1629 - val_f1_m: 0.4725 - val_recall_m: 0.4663 - val_precision_m: 0.4794 - val_precision: 0.4794 - val_recall_6: 0.4663 - val_f1score: 0.4725\n",
      "Epoch 44/300\n",
      "epoch:  43\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.3355 - acc: 0.8367 - prec: 0.9622 - recall: 0.9790 - prec_1: 0.8818 - recall_1: 0.8999 - prec_2: 0.9301 - recall_2: 0.9239 - prec_3: 0.6643 - recall_3: 0.6385 - prec_4: 0.6569 - recall_4: 0.6727 - prec_5: 0.9317 - recall_5: 0.9408 - f1_m: 0.8361 - recall_m: 0.8283 - precision_m: 0.8442 - precision: 0.8442 - recall_6: 0.8283 - f1score: 0.8361 - val_loss: 5.6407 - val_acc: 0.4148 - val_prec: 0.1667 - val_recall: 0.1757 - val_prec_1: 0.1599 - val_recall_1: 0.0330 - val_prec_2: 0.1919 - val_recall_2: 0.1337 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0976 - val_f1_m: 0.4122 - val_recall_m: 0.4098 - val_precision_m: 0.4147 - val_precision: 0.4147 - val_recall_6: 0.4098 - val_f1score: 0.4122\n",
      "Epoch 45/300\n",
      "epoch:  44\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.3291 - acc: 0.8483 - prec: 0.9715 - recall: 0.9801 - prec_1: 0.8982 - recall_1: 0.9139 - prec_2: 0.9463 - recall_2: 0.9277 - prec_3: 0.6693 - recall_3: 0.6738 - prec_4: 0.6704 - recall_4: 0.6633 - prec_5: 0.9378 - recall_5: 0.9531 - f1_m: 0.8470 - recall_m: 0.8390 - precision_m: 0.8553 - precision: 0.8553 - recall_6: 0.8390 - f1score: 0.8470 - val_loss: 0.5599 - val_acc: 0.7239 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1759 - val_recall_1: 0.0934 - val_prec_2: 0.1919 - val_recall_2: 0.1552 - val_prec_3: 0.1709 - val_recall_3: 0.1712 - val_prec_4: 0.1119 - val_recall_4: 0.0060 - val_prec_5: 0.1684 - val_recall_5: 0.1648 - val_f1_m: 0.7145 - val_recall_m: 0.7034 - val_precision_m: 0.7271 - val_precision: 0.7271 - val_recall_6: 0.7034 - val_f1score: 0.7145\n",
      "Epoch 46/300\n",
      "epoch:  45\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.3254 - acc: 0.8411 - prec: 0.9689 - recall: 0.9764 - prec_1: 0.8941 - recall_1: 0.9027 - prec_2: 0.9374 - recall_2: 0.9355 - prec_3: 0.6509 - recall_3: 0.6459 - prec_4: 0.6513 - recall_4: 0.6483 - prec_5: 0.9408 - recall_5: 0.9533 - f1_m: 0.8404 - recall_m: 0.8327 - precision_m: 0.8483 - precision: 0.8483 - recall_6: 0.8327 - f1score: 0.8404 - val_loss: 2.1465 - val_acc: 0.4883 - val_prec: 0.1759 - val_recall: 0.0823 - val_prec_1: 0.1759 - val_recall_1: 0.0611 - val_prec_2: 0.1792 - val_recall_2: 0.1750 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1759 - val_recall_4: 0.0464 - val_prec_5: 0.1676 - val_recall_5: 0.1632 - val_f1_m: 0.4853 - val_recall_m: 0.4793 - val_precision_m: 0.4918 - val_precision: 0.4918 - val_recall_6: 0.4793 - val_f1score: 0.4853\n",
      "Epoch 47/300\n",
      "epoch:  46\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.3331 - acc: 0.8393 - prec: 0.9567 - recall: 0.9723 - prec_1: 0.8898 - recall_1: 0.9101 - prec_2: 0.9270 - recall_2: 0.9308 - prec_3: 0.6581 - recall_3: 0.6846 - prec_4: 0.6696 - recall_4: 0.6352 - prec_5: 0.9439 - recall_5: 0.9386 - f1_m: 0.8379 - recall_m: 0.8300 - precision_m: 0.8461 - precision: 0.8461 - recall_6: 0.8300 - f1score: 0.8379 - val_loss: 0.7099 - val_acc: 0.7076 - val_prec: 0.1759 - val_recall: 0.1499 - val_prec_1: 0.1756 - val_recall_1: 0.0752 - val_prec_2: 0.1919 - val_recall_2: 0.1785 - val_prec_3: 0.1279 - val_recall_3: 0.0256 - val_prec_4: 0.1739 - val_recall_4: 0.1632 - val_prec_5: 0.1684 - val_recall_5: 0.1648 - val_f1_m: 0.7019 - val_recall_m: 0.6892 - val_precision_m: 0.7162 - val_precision: 0.7162 - val_recall_6: 0.6892 - val_f1score: 0.7019\n",
      "Epoch 48/300\n",
      "epoch:  47\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 675us/step - loss: 0.3307 - acc: 0.8395 - prec: 0.9637 - recall: 0.9818 - prec_1: 0.8902 - recall_1: 0.9112 - prec_2: 0.9514 - recall_2: 0.9325 - prec_3: 0.6518 - recall_3: 0.6537 - prec_4: 0.6504 - recall_4: 0.6408 - prec_5: 0.9421 - recall_5: 0.9455 - f1_m: 0.8381 - recall_m: 0.8313 - precision_m: 0.8451 - precision: 0.8451 - recall_6: 0.8313 - f1score: 0.8381 - val_loss: 0.8295 - val_acc: 0.6957 - val_prec: 0.1759 - val_recall: 0.1449 - val_prec_1: 0.1740 - val_recall_1: 0.1326 - val_prec_2: 0.1804 - val_recall_2: 0.1714 - val_prec_3: 0.1439 - val_recall_3: 0.0717 - val_prec_4: 0.1759 - val_recall_4: 0.1349 - val_prec_5: 0.1684 - val_recall_5: 0.0916 - val_f1_m: 0.6904 - val_recall_m: 0.6772 - val_precision_m: 0.7057 - val_precision: 0.7057 - val_recall_6: 0.6772 - val_f1score: 0.6904\n",
      "Epoch 49/300\n",
      "epoch:  48\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3260 - acc: 0.8441 - prec: 0.9660 - recall: 0.9867 - prec_1: 0.8946 - recall_1: 0.9094 - prec_2: 0.9422 - recall_2: 0.9312 - prec_3: 0.6659 - recall_3: 0.6444 - prec_4: 0.6577 - recall_4: 0.6705 - prec_5: 0.9389 - recall_5: 0.9432 - f1_m: 0.8425 - recall_m: 0.8356 - precision_m: 0.8497 - precision: 0.8497 - recall_6: 0.8356 - f1score: 0.8425 - val_loss: 2.0430 - val_acc: 0.5572 - val_prec: 0.1662 - val_recall: 0.1724 - val_prec_1: 0.1439 - val_recall_1: 0.0552 - val_prec_2: 0.1839 - val_recall_2: 0.1525 - val_prec_3: 0.1759 - val_recall_3: 0.1007 - val_prec_4: 0.1439 - val_recall_4: 0.0407 - val_prec_5: 0.1524 - val_recall_5: 0.0801 - val_f1_m: 0.5548 - val_recall_m: 0.5475 - val_precision_m: 0.5630 - val_precision: 0.5630 - val_recall_6: 0.5475 - val_f1score: 0.5548\n",
      "Epoch 50/300\n",
      "epoch:  49\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.3265 - acc: 0.8415 - prec: 0.9663 - recall: 0.9800 - prec_1: 0.8894 - recall_1: 0.8984 - prec_2: 0.9269 - recall_2: 0.9230 - prec_3: 0.6723 - recall_3: 0.6641 - prec_4: 0.6629 - recall_4: 0.6674 - prec_5: 0.9386 - recall_5: 0.9466 - f1_m: 0.8417 - recall_m: 0.8343 - precision_m: 0.8493 - precision: 0.8493 - recall_6: 0.8343 - f1score: 0.8417 - val_loss: 4.6007 - val_acc: 0.4310 - val_prec: 0.1759 - val_recall: 0.1627 - val_prec_1: 0.1724 - val_recall_1: 0.1724 - val_prec_2: 0.1599 - val_recall_2: 0.0327 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0840 - val_f1_m: 0.4306 - val_recall_m: 0.4305 - val_precision_m: 0.4306 - val_precision: 0.4306 - val_recall_6: 0.4305 - val_f1score: 0.4306\n",
      "Epoch 51/300\n",
      "epoch:  50\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3323 - acc: 0.8413 - prec: 0.9640 - recall: 0.9810 - prec_1: 0.8762 - recall_1: 0.9056 - prec_2: 0.9387 - recall_2: 0.9179 - prec_3: 0.6719 - recall_3: 0.6732 - prec_4: 0.6709 - recall_4: 0.6586 - prec_5: 0.9370 - recall_5: 0.9471 - f1_m: 0.8407 - recall_m: 0.8332 - precision_m: 0.8484 - precision: 0.8484 - recall_6: 0.8332 - f1score: 0.8407 - val_loss: 0.9649 - val_acc: 0.6629 - val_prec: 0.1753 - val_recall: 0.1734 - val_prec_1: 0.1744 - val_recall_1: 0.1527 - val_prec_2: 0.1919 - val_recall_2: 0.1329 - val_prec_3: 0.1708 - val_recall_3: 0.0802 - val_prec_4: 0.1599 - val_recall_4: 0.0305 - val_prec_5: 0.1684 - val_recall_5: 0.1334 - val_f1_m: 0.6274 - val_recall_m: 0.6132 - val_precision_m: 0.6512 - val_precision: 0.6512 - val_recall_6: 0.6132 - val_f1score: 0.6274\n",
      "Epoch 52/300\n",
      "epoch:  51\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3162 - acc: 0.8492 - prec: 0.9641 - recall: 0.9811 - prec_1: 0.9059 - recall_1: 0.9097 - prec_2: 0.9393 - recall_2: 0.9395 - prec_3: 0.6712 - recall_3: 0.6802 - prec_4: 0.6687 - recall_4: 0.6578 - prec_5: 0.9381 - recall_5: 0.9533 - f1_m: 0.8481 - recall_m: 0.8418 - precision_m: 0.8546 - precision: 0.8546 - recall_6: 0.8418 - f1score: 0.8481 - val_loss: 1.2020 - val_acc: 0.6829 - val_prec: 0.1759 - val_recall: 0.1687 - val_prec_1: 0.1759 - val_recall_1: 0.1063 - val_prec_2: 0.1919 - val_recall_2: 0.1502 - val_prec_3: 0.1709 - val_recall_3: 0.1357 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1601 - val_f1_m: 0.6773 - val_recall_m: 0.6687 - val_precision_m: 0.6883 - val_precision: 0.6883 - val_recall_6: 0.6687 - val_f1score: 0.6773\n",
      "Epoch 53/300\n",
      "epoch:  52\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.3136 - acc: 0.8483 - prec: 0.9664 - recall: 0.9796 - prec_1: 0.9018 - recall_1: 0.9163 - prec_2: 0.9452 - recall_2: 0.9331 - prec_3: 0.6752 - recall_3: 0.6674 - prec_4: 0.6643 - recall_4: 0.6817 - prec_5: 0.9434 - recall_5: 0.9522 - f1_m: 0.8477 - recall_m: 0.8408 - precision_m: 0.8547 - precision: 0.8547 - recall_6: 0.8408 - f1score: 0.8477 - val_loss: 1.1778 - val_acc: 0.6807 - val_prec: 0.1712 - val_recall: 0.1706 - val_prec_1: 0.1747 - val_recall_1: 0.1384 - val_prec_2: 0.1919 - val_recall_2: 0.1660 - val_prec_3: 0.1599 - val_recall_3: 0.0925 - val_prec_4: 0.1599 - val_recall_4: 0.0702 - val_prec_5: 0.1684 - val_recall_5: 0.0831 - val_f1_m: 0.6783 - val_recall_m: 0.6709 - val_precision_m: 0.6861 - val_precision: 0.6861 - val_recall_6: 0.6709 - val_f1score: 0.6783\n",
      "Epoch 54/300\n",
      "epoch:  53\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.3171 - acc: 0.8495 - prec: 0.9631 - recall: 0.9767 - prec_1: 0.9038 - recall_1: 0.9109 - prec_2: 0.9446 - recall_2: 0.9364 - prec_3: 0.6717 - recall_3: 0.6863 - prec_4: 0.6807 - recall_4: 0.6527 - prec_5: 0.9457 - recall_5: 0.9563 - f1_m: 0.8482 - recall_m: 0.8420 - precision_m: 0.8546 - precision: 0.8546 - recall_6: 0.8420 - f1score: 0.8482 - val_loss: 1.9780 - val_acc: 0.6832 - val_prec: 0.1759 - val_recall: 0.1417 - val_prec_1: 0.1759 - val_recall_1: 0.0744 - val_prec_2: 0.1828 - val_recall_2: 0.1891 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1809 - val_recall_4: 0.1914 - val_prec_5: 0.1684 - val_recall_5: 0.1491 - val_f1_m: 0.6806 - val_recall_m: 0.6684 - val_precision_m: 0.6948 - val_precision: 0.6948 - val_recall_6: 0.6684 - val_f1score: 0.6806\n",
      "Epoch 55/300\n",
      "epoch:  54\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.3235 - acc: 0.8413 - prec: 0.9649 - recall: 0.9807 - prec_1: 0.8985 - recall_1: 0.9048 - prec_2: 0.9352 - recall_2: 0.9323 - prec_3: 0.6578 - recall_3: 0.6579 - prec_4: 0.6597 - recall_4: 0.6465 - prec_5: 0.9403 - recall_5: 0.9518 - f1_m: 0.8402 - recall_m: 0.8333 - precision_m: 0.8472 - precision: 0.8472 - recall_6: 0.8333 - f1score: 0.8402 - val_loss: 5.5654 - val_acc: 0.3471 - val_prec: 0.1665 - val_recall: 0.1618 - val_prec_1: 0.0960 - val_recall_1: 0.0138 - val_prec_2: 0.1839 - val_recall_2: 0.1217 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1439 - val_recall_4: 0.0332 - val_prec_5: 0.1204 - val_recall_5: 0.0415 - val_f1_m: 0.3458 - val_recall_m: 0.3446 - val_precision_m: 0.3470 - val_precision: 0.3470 - val_recall_6: 0.3446 - val_f1score: 0.3458\n",
      "Epoch 56/300\n",
      "epoch:  55\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.3212 - acc: 0.8486 - prec: 0.9710 - recall: 0.9823 - prec_1: 0.9025 - recall_1: 0.9092 - prec_2: 0.9403 - recall_2: 0.9356 - prec_3: 0.6667 - recall_3: 0.6758 - prec_4: 0.6651 - recall_4: 0.6520 - prec_5: 0.9408 - recall_5: 0.9508 - f1_m: 0.8474 - recall_m: 0.8411 - precision_m: 0.8539 - precision: 0.8539 - recall_6: 0.8411 - f1score: 0.8474 - val_loss: 1.2487 - val_acc: 0.6514 - val_prec: 0.1759 - val_recall: 0.1526 - val_prec_1: 0.1752 - val_recall_1: 0.0673 - val_prec_2: 0.1852 - val_recall_2: 0.1821 - val_prec_3: 0.1439 - val_recall_3: 0.0682 - val_prec_4: 0.1599 - val_recall_4: 0.0540 - val_prec_5: 0.1666 - val_recall_5: 0.1661 - val_f1_m: 0.6480 - val_recall_m: 0.6414 - val_precision_m: 0.6551 - val_precision: 0.6551 - val_recall_6: 0.6414 - val_f1score: 0.6480\n",
      "Epoch 57/300\n",
      "epoch:  56\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.3208 - acc: 0.8503 - prec: 0.9643 - recall: 0.9790 - prec_1: 0.8995 - recall_1: 0.9164 - prec_2: 0.9436 - recall_2: 0.9375 - prec_3: 0.6714 - recall_3: 0.6864 - prec_4: 0.6826 - recall_4: 0.6586 - prec_5: 0.9471 - recall_5: 0.9474 - f1_m: 0.8483 - recall_m: 0.8415 - precision_m: 0.8553 - precision: 0.8553 - recall_6: 0.8415 - f1score: 0.8483 - val_loss: 4.3190 - val_acc: 0.5462 - val_prec: 0.1759 - val_recall: 0.1552 - val_prec_1: 0.1753 - val_recall_1: 0.1173 - val_prec_2: 0.1800 - val_recall_2: 0.1758 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1335 - val_f1_m: 0.5433 - val_recall_m: 0.5365 - val_precision_m: 0.5509 - val_precision: 0.5509 - val_recall_6: 0.5365 - val_f1score: 0.5433\n",
      "Epoch 58/300\n",
      "epoch:  57\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 657us/step - loss: 0.3127 - acc: 0.8470 - prec: 0.9737 - recall: 0.9828 - prec_1: 0.9096 - recall_1: 0.9200 - prec_2: 0.9496 - recall_2: 0.9439 - prec_3: 0.6638 - recall_3: 0.6548 - prec_4: 0.6509 - recall_4: 0.6514 - prec_5: 0.9470 - recall_5: 0.9529 - f1_m: 0.8460 - recall_m: 0.8400 - precision_m: 0.8522 - precision: 0.8522 - recall_6: 0.8400 - f1score: 0.8460 - val_loss: 5.9211 - val_acc: 0.4448 - val_prec: 0.1667 - val_recall: 0.1742 - val_prec_1: 0.1599 - val_recall_1: 0.0431 - val_prec_2: 0.1848 - val_recall_2: 0.1287 - val_prec_3: 0.1439 - val_recall_3: 0.0674 - val_prec_4: 0.0800 - val_recall_4: 0.0027 - val_prec_5: 0.1364 - val_recall_5: 0.0587 - val_f1_m: 0.4422 - val_recall_m: 0.4398 - val_precision_m: 0.4447 - val_precision: 0.4447 - val_recall_6: 0.4398 - val_f1score: 0.4422\n",
      "Epoch 59/300\n",
      "epoch:  58\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3154 - acc: 0.8467 - prec: 0.9746 - recall: 0.9795 - prec_1: 0.8970 - recall_1: 0.9090 - prec_2: 0.9381 - recall_2: 0.9398 - prec_3: 0.6678 - recall_3: 0.6661 - prec_4: 0.6675 - recall_4: 0.6680 - prec_5: 0.9456 - recall_5: 0.9501 - f1_m: 0.8464 - recall_m: 0.8393 - precision_m: 0.8536 - precision: 0.8536 - recall_6: 0.8393 - f1score: 0.8464 - val_loss: 1.6765 - val_acc: 0.6059 - val_prec: 0.1673 - val_recall: 0.1725 - val_prec_1: 0.1742 - val_recall_1: 0.1080 - val_prec_2: 0.1759 - val_recall_2: 0.1170 - val_prec_3: 0.1709 - val_recall_3: 0.1709 - val_prec_4: 0.0480 - val_recall_4: 0.0042 - val_prec_5: 0.1524 - val_recall_5: 0.0660 - val_f1_m: 0.6040 - val_recall_m: 0.6007 - val_precision_m: 0.6075 - val_precision: 0.6075 - val_recall_6: 0.6007 - val_f1score: 0.6040\n",
      "Epoch 60/300\n",
      "epoch:  59\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 676us/step - loss: 0.3133 - acc: 0.8523 - prec: 0.9654 - recall: 0.9839 - prec_1: 0.9090 - recall_1: 0.9282 - prec_2: 0.9553 - recall_2: 0.9360 - prec_3: 0.6717 - recall_3: 0.6891 - prec_4: 0.6816 - recall_4: 0.6583 - prec_5: 0.9471 - recall_5: 0.9546 - f1_m: 0.8504 - recall_m: 0.8441 - precision_m: 0.8569 - precision: 0.8569 - recall_6: 0.8441 - f1score: 0.8504 - val_loss: 1.9003 - val_acc: 0.6269 - val_prec: 0.1671 - val_recall: 0.1734 - val_prec_1: 0.1744 - val_recall_1: 0.0942 - val_prec_2: 0.1839 - val_recall_2: 0.1253 - val_prec_3: 0.1706 - val_recall_3: 0.1230 - val_prec_4: 0.1599 - val_recall_4: 0.0412 - val_prec_5: 0.1524 - val_recall_5: 0.1036 - val_f1_m: 0.6224 - val_recall_m: 0.6152 - val_precision_m: 0.6302 - val_precision: 0.6302 - val_recall_6: 0.6152 - val_f1score: 0.6224\n",
      "Epoch 61/300\n",
      "epoch:  60\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.3197 - acc: 0.8433 - prec: 0.9634 - recall: 0.9763 - prec_1: 0.8981 - recall_1: 0.9087 - prec_2: 0.9381 - recall_2: 0.9390 - prec_3: 0.6654 - recall_3: 0.6716 - prec_4: 0.6594 - recall_4: 0.6527 - prec_5: 0.9366 - recall_5: 0.9429 - f1_m: 0.8446 - recall_m: 0.8375 - precision_m: 0.8520 - precision: 0.8520 - recall_6: 0.8375 - f1score: 0.8446 - val_loss: 5.2618 - val_acc: 0.4848 - val_prec: 0.1734 - val_recall: 0.1739 - val_prec_1: 0.1759 - val_recall_1: 0.1002 - val_prec_2: 0.1919 - val_recall_2: 0.1839 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0663 - val_f1_m: 0.4857 - val_recall_m: 0.4815 - val_precision_m: 0.4902 - val_precision: 0.4902 - val_recall_6: 0.4815 - val_f1score: 0.4857\n",
      "Epoch 62/300\n",
      "epoch:  61\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 676us/step - loss: 0.3113 - acc: 0.8508 - prec: 0.9730 - recall: 0.9787 - prec_1: 0.9007 - recall_1: 0.9190 - prec_2: 0.9458 - recall_2: 0.9382 - prec_3: 0.6773 - recall_3: 0.6719 - prec_4: 0.6680 - recall_4: 0.6612 - prec_5: 0.9443 - recall_5: 0.9533 - f1_m: 0.8504 - recall_m: 0.8450 - precision_m: 0.8561 - precision: 0.8561 - recall_6: 0.8450 - f1score: 0.8504 - val_loss: 5.8333 - val_acc: 0.4998 - val_prec: 0.1737 - val_recall: 0.1720 - val_prec_1: 0.1599 - val_recall_1: 0.0444 - val_prec_2: 0.1791 - val_recall_2: 0.1490 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.4961 - val_recall_m: 0.4920 - val_precision_m: 0.5005 - val_precision: 0.5005 - val_recall_6: 0.4920 - val_f1score: 0.4961\n",
      "Epoch 63/300\n",
      "epoch:  62\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3078 - acc: 0.8507 - prec: 0.9643 - recall: 0.9825 - prec_1: 0.9154 - recall_1: 0.9218 - prec_2: 0.9525 - recall_2: 0.9499 - prec_3: 0.6586 - recall_3: 0.6694 - prec_4: 0.6686 - recall_4: 0.6583 - prec_5: 0.9441 - recall_5: 0.9557 - f1_m: 0.8496 - recall_m: 0.8440 - precision_m: 0.8554 - precision: 0.8554 - recall_6: 0.8440 - f1score: 0.8496 - val_loss: 2.5918 - val_acc: 0.5710 - val_prec: 0.1753 - val_recall: 0.1727 - val_prec_1: 0.1756 - val_recall_1: 0.1101 - val_prec_2: 0.1919 - val_recall_2: 0.1791 - val_prec_3: 0.1439 - val_recall_3: 0.0541 - val_prec_4: 0.0640 - val_recall_4: 0.0145 - val_prec_5: 0.1684 - val_recall_5: 0.0798 - val_f1_m: 0.5601 - val_recall_m: 0.5497 - val_precision_m: 0.5722 - val_precision: 0.5722 - val_recall_6: 0.5497 - val_f1score: 0.5601\n",
      "Epoch 64/300\n",
      "epoch:  63\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.3093 - acc: 0.8576 - prec: 0.9661 - recall: 0.9805 - prec_1: 0.9145 - recall_1: 0.9268 - prec_2: 0.9507 - recall_2: 0.9381 - prec_3: 0.6803 - recall_3: 0.6891 - prec_4: 0.6813 - recall_4: 0.6659 - prec_5: 0.9556 - recall_5: 0.9624 - f1_m: 0.8577 - recall_m: 0.8517 - precision_m: 0.8638 - precision: 0.8638 - recall_6: 0.8517 - f1score: 0.8577 - val_loss: 1.7326 - val_acc: 0.6504 - val_prec: 0.1726 - val_recall: 0.1712 - val_prec_1: 0.1755 - val_recall_1: 0.0982 - val_prec_2: 0.1849 - val_recall_2: 0.1756 - val_prec_3: 0.1708 - val_recall_3: 0.1076 - val_prec_4: 0.1599 - val_recall_4: 0.0347 - val_prec_5: 0.1684 - val_recall_5: 0.1127 - val_f1_m: 0.6424 - val_recall_m: 0.6304 - val_precision_m: 0.6561 - val_precision: 0.6561 - val_recall_6: 0.6304 - val_f1score: 0.6424\n",
      "Epoch 65/300\n",
      "epoch:  64\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3128 - acc: 0.8500 - prec: 0.9684 - recall: 0.9794 - prec_1: 0.9044 - recall_1: 0.9190 - prec_2: 0.9527 - recall_2: 0.9368 - prec_3: 0.6713 - recall_3: 0.6766 - prec_4: 0.6687 - recall_4: 0.6603 - prec_5: 0.9507 - recall_5: 0.9546 - f1_m: 0.8495 - recall_m: 0.8435 - precision_m: 0.8558 - precision: 0.8558 - recall_6: 0.8435 - f1score: 0.8495 - val_loss: 1.2297 - val_acc: 0.6292 - val_prec: 0.1753 - val_recall: 0.1700 - val_prec_1: 0.1599 - val_recall_1: 0.0458 - val_prec_2: 0.1807 - val_recall_2: 0.1855 - val_prec_3: 0.1759 - val_recall_3: 0.1152 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1522 - val_f1_m: 0.6228 - val_recall_m: 0.6134 - val_precision_m: 0.6383 - val_precision: 0.6383 - val_recall_6: 0.6134 - val_f1score: 0.6228\n",
      "Epoch 66/300\n",
      "epoch:  65\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3067 - acc: 0.8555 - prec: 0.9677 - recall: 0.9826 - prec_1: 0.9095 - recall_1: 0.9239 - prec_2: 0.9464 - recall_2: 0.9401 - prec_3: 0.6793 - recall_3: 0.6962 - prec_4: 0.6903 - recall_4: 0.6691 - prec_5: 0.9497 - recall_5: 0.9561 - f1_m: 0.8533 - recall_m: 0.8463 - precision_m: 0.8605 - precision: 0.8605 - recall_6: 0.8463 - f1score: 0.8533 - val_loss: 3.9238 - val_acc: 0.4998 - val_prec: 0.1669 - val_recall: 0.1659 - val_prec_1: 0.1759 - val_recall_1: 0.0682 - val_prec_2: 0.1819 - val_recall_2: 0.1725 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1119 - val_recall_4: 0.0555 - val_prec_5: 0.1684 - val_recall_5: 0.0825 - val_f1_m: 0.5007 - val_recall_m: 0.4950 - val_precision_m: 0.5072 - val_precision: 0.5072 - val_recall_6: 0.4950 - val_f1score: 0.5007\n",
      "Epoch 67/300\n",
      "epoch:  66\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.3025 - acc: 0.8535 - prec: 0.9727 - recall: 0.9841 - prec_1: 0.9027 - recall_1: 0.9153 - prec_2: 0.9472 - recall_2: 0.9395 - prec_3: 0.6663 - recall_3: 0.6875 - prec_4: 0.6879 - recall_4: 0.6625 - prec_5: 0.9447 - recall_5: 0.9552 - f1_m: 0.8531 - recall_m: 0.8480 - precision_m: 0.8585 - precision: 0.8585 - recall_6: 0.8480 - f1score: 0.8531 - val_loss: 1.6297 - val_acc: 0.6157 - val_prec: 0.1703 - val_recall: 0.1631 - val_prec_1: 0.1759 - val_recall_1: 0.0259 - val_prec_2: 0.1803 - val_recall_2: 0.1835 - val_prec_3: 0.0640 - val_recall_3: 0.0260 - val_prec_4: 0.1649 - val_recall_4: 0.1492 - val_prec_5: 0.1684 - val_recall_5: 0.1146 - val_f1_m: 0.6145 - val_recall_m: 0.6119 - val_precision_m: 0.6173 - val_precision: 0.6173 - val_recall_6: 0.6119 - val_f1score: 0.6145\n",
      "Epoch 68/300\n",
      "epoch:  67\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.3079 - acc: 0.8543 - prec: 0.9655 - recall: 0.9843 - prec_1: 0.9111 - recall_1: 0.9275 - prec_2: 0.9560 - recall_2: 0.9444 - prec_3: 0.6735 - recall_3: 0.6715 - prec_4: 0.6664 - recall_4: 0.6695 - prec_5: 0.9551 - recall_5: 0.9547 - f1_m: 0.8537 - recall_m: 0.8476 - precision_m: 0.8600 - precision: 0.8600 - recall_6: 0.8476 - f1score: 0.8537 - val_loss: 1.7193 - val_acc: 0.6129 - val_prec: 0.1753 - val_recall: 0.1702 - val_prec_1: 0.1750 - val_recall_1: 0.1288 - val_prec_2: 0.1896 - val_recall_2: 0.1715 - val_prec_3: 0.1439 - val_recall_3: 0.0522 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1300 - val_f1_m: 0.6130 - val_recall_m: 0.6062 - val_precision_m: 0.6202 - val_precision: 0.6202 - val_recall_6: 0.6062 - val_f1score: 0.6130\n",
      "Epoch 69/300\n",
      "epoch:  68\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2976 - acc: 0.8610 - prec: 0.9741 - recall: 0.9812 - prec_1: 0.9143 - recall_1: 0.9281 - prec_2: 0.9492 - recall_2: 0.9454 - prec_3: 0.6881 - recall_3: 0.6936 - prec_4: 0.6881 - recall_4: 0.6813 - prec_5: 0.9549 - recall_5: 0.9609 - f1_m: 0.8602 - recall_m: 0.8545 - precision_m: 0.8660 - precision: 0.8660 - recall_6: 0.8545 - f1score: 0.8602 - val_loss: 3.3222 - val_acc: 0.5300 - val_prec: 0.1719 - val_recall: 0.1576 - val_prec_1: 0.1599 - val_recall_1: 0.0681 - val_prec_2: 0.1829 - val_recall_2: 0.1887 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0800 - val_recall_4: 0.0100 - val_prec_5: 0.1665 - val_recall_5: 0.1425 - val_f1_m: 0.5279 - val_recall_m: 0.5237 - val_precision_m: 0.5323 - val_precision: 0.5323 - val_recall_6: 0.5237 - val_f1score: 0.5279\n",
      "Epoch 70/300\n",
      "epoch:  69\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3061 - acc: 0.8531 - prec: 0.9688 - recall: 0.9866 - prec_1: 0.9143 - recall_1: 0.9277 - prec_2: 0.9516 - recall_2: 0.9476 - prec_3: 0.6668 - recall_3: 0.6851 - prec_4: 0.6744 - recall_4: 0.6531 - prec_5: 0.9520 - recall_5: 0.9571 - f1_m: 0.8517 - recall_m: 0.8462 - precision_m: 0.8574 - precision: 0.8574 - recall_6: 0.8462 - f1score: 0.8517 - val_loss: 1.2932 - val_acc: 0.5960 - val_prec: 0.1759 - val_recall: 0.1208 - val_prec_1: 0.1737 - val_recall_1: 0.1719 - val_prec_2: 0.1919 - val_recall_2: 0.1066 - val_prec_3: 0.1759 - val_recall_3: 0.1722 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0659 - val_f1_m: 0.5924 - val_recall_m: 0.5847 - val_precision_m: 0.6019 - val_precision: 0.6019 - val_recall_6: 0.5847 - val_f1score: 0.5924\n",
      "Epoch 71/300\n",
      "epoch:  70\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.3040 - acc: 0.8587 - prec: 0.9670 - recall: 0.9793 - prec_1: 0.9188 - recall_1: 0.9172 - prec_2: 0.9402 - recall_2: 0.9456 - prec_3: 0.6879 - recall_3: 0.6754 - prec_4: 0.6871 - recall_4: 0.6964 - prec_5: 0.9421 - recall_5: 0.9574 - f1_m: 0.8585 - recall_m: 0.8530 - precision_m: 0.8641 - precision: 0.8641 - recall_6: 0.8530 - f1score: 0.8585 - val_loss: 2.2109 - val_acc: 0.5367 - val_prec: 0.1759 - val_recall: 0.1614 - val_prec_1: 0.1599 - val_recall_1: 0.0721 - val_prec_2: 0.1843 - val_recall_2: 0.1884 - val_prec_3: 0.1390 - val_recall_3: 0.0519 - val_prec_4: 0.0640 - val_recall_4: 0.0020 - val_prec_5: 0.1684 - val_recall_5: 0.1015 - val_f1_m: 0.5072 - val_recall_m: 0.5002 - val_precision_m: 0.5154 - val_precision: 0.5154 - val_recall_6: 0.5002 - val_f1score: 0.5072\n",
      "Epoch 72/300\n",
      "epoch:  71\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2998 - acc: 0.8585 - prec: 0.9758 - recall: 0.9830 - prec_1: 0.9222 - recall_1: 0.9300 - prec_2: 0.9498 - recall_2: 0.9488 - prec_3: 0.6676 - recall_3: 0.6779 - prec_4: 0.6811 - recall_4: 0.6661 - prec_5: 0.9608 - recall_5: 0.9621 - f1_m: 0.8583 - recall_m: 0.8531 - precision_m: 0.8637 - precision: 0.8637 - recall_6: 0.8531 - f1score: 0.8583 - val_loss: 2.3695 - val_acc: 0.6477 - val_prec: 0.1759 - val_recall: 0.1559 - val_prec_1: 0.1759 - val_recall_1: 0.1036 - val_prec_2: 0.1919 - val_recall_2: 0.1879 - val_prec_3: 0.1119 - val_recall_3: 0.0460 - val_prec_4: 0.1599 - val_recall_4: 0.0492 - val_prec_5: 0.1666 - val_recall_5: 0.1593 - val_f1_m: 0.6463 - val_recall_m: 0.6367 - val_precision_m: 0.6570 - val_precision: 0.6570 - val_recall_6: 0.6367 - val_f1score: 0.6463\n",
      "Epoch 73/300\n",
      "epoch:  72\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 658us/step - loss: 0.2949 - acc: 0.8603 - prec: 0.9702 - recall: 0.9823 - prec_1: 0.9133 - recall_1: 0.9341 - prec_2: 0.9571 - recall_2: 0.9443 - prec_3: 0.6736 - recall_3: 0.6904 - prec_4: 0.6853 - recall_4: 0.6711 - prec_5: 0.9596 - recall_5: 0.9629 - f1_m: 0.8597 - recall_m: 0.8541 - precision_m: 0.8654 - precision: 0.8654 - recall_6: 0.8541 - f1score: 0.8597 - val_loss: 3.2124 - val_acc: 0.4943 - val_prec: 0.1670 - val_recall: 0.1732 - val_prec_1: 0.1759 - val_recall_1: 0.0567 - val_prec_2: 0.1805 - val_recall_2: 0.1733 - val_prec_3: 0.0800 - val_recall_3: 0.0117 - val_prec_4: 0.1599 - val_recall_4: 0.0535 - val_prec_5: 0.1524 - val_recall_5: 0.0573 - val_f1_m: 0.4912 - val_recall_m: 0.4875 - val_precision_m: 0.4952 - val_precision: 0.4952 - val_recall_6: 0.4875 - val_f1score: 0.4912\n",
      "Epoch 74/300\n",
      "epoch:  73\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.3011 - acc: 0.8566 - prec: 0.9705 - recall: 0.9831 - prec_1: 0.9141 - recall_1: 0.9211 - prec_2: 0.9365 - recall_2: 0.9443 - prec_3: 0.6825 - recall_3: 0.6858 - prec_4: 0.6809 - recall_4: 0.6733 - prec_5: 0.9478 - recall_5: 0.9625 - f1_m: 0.8566 - recall_m: 0.8522 - precision_m: 0.8611 - precision: 0.8611 - recall_6: 0.8522 - f1score: 0.8566 - val_loss: 1.7063 - val_acc: 0.6587 - val_prec: 0.1759 - val_recall: 0.1306 - val_prec_1: 0.1599 - val_recall_1: 0.0591 - val_prec_2: 0.1879 - val_recall_2: 0.1705 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1789 - val_recall_4: 0.1919 - val_prec_5: 0.1684 - val_recall_5: 0.1636 - val_f1_m: 0.6553 - val_recall_m: 0.6487 - val_precision_m: 0.6625 - val_precision: 0.6625 - val_recall_6: 0.6487 - val_f1score: 0.6553\n",
      "Epoch 75/300\n",
      "epoch:  74\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3005 - acc: 0.8530 - prec: 0.9746 - recall: 0.9856 - prec_1: 0.9213 - recall_1: 0.9286 - prec_2: 0.9518 - recall_2: 0.9498 - prec_3: 0.6711 - recall_3: 0.6781 - prec_4: 0.6801 - recall_4: 0.6521 - prec_5: 0.9540 - recall_5: 0.9602 - f1_m: 0.8523 - recall_m: 0.8467 - precision_m: 0.8581 - precision: 0.8581 - recall_6: 0.8467 - f1score: 0.8523 - val_loss: 1.3265 - val_acc: 0.6272 - val_prec: 0.1759 - val_recall: 0.1719 - val_prec_1: 0.1599 - val_recall_1: 0.0822 - val_prec_2: 0.1843 - val_recall_2: 0.1904 - val_prec_3: 0.1439 - val_recall_3: 0.0702 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1544 - val_f1_m: 0.6249 - val_recall_m: 0.6199 - val_precision_m: 0.6301 - val_precision: 0.6301 - val_recall_6: 0.6199 - val_f1score: 0.6249\n",
      "Epoch 76/300\n",
      "epoch:  75\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2999 - acc: 0.8552 - prec: 0.9682 - recall: 0.9798 - prec_1: 0.9246 - recall_1: 0.9294 - prec_2: 0.9515 - recall_2: 0.9517 - prec_3: 0.6690 - recall_3: 0.6660 - prec_4: 0.6661 - recall_4: 0.6661 - prec_5: 0.9529 - recall_5: 0.9592 - f1_m: 0.8548 - recall_m: 0.8493 - precision_m: 0.8604 - precision: 0.8604 - recall_6: 0.8493 - f1score: 0.8548 - val_loss: 4.9758 - val_acc: 0.5442 - val_prec: 0.1707 - val_recall: 0.1737 - val_prec_1: 0.1748 - val_recall_1: 0.1004 - val_prec_2: 0.1919 - val_recall_2: 0.1284 - val_prec_3: 0.0640 - val_recall_3: 0.0212 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1531 - val_f1_m: 0.5423 - val_recall_m: 0.5377 - val_precision_m: 0.5473 - val_precision: 0.5473 - val_recall_6: 0.5377 - val_f1score: 0.5423\n",
      "Epoch 77/300\n",
      "epoch:  76\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2921 - acc: 0.8622 - prec: 0.9662 - recall: 0.9806 - prec_1: 0.9217 - recall_1: 0.9393 - prec_2: 0.9467 - recall_2: 0.9434 - prec_3: 0.6896 - recall_3: 0.6868 - prec_4: 0.6955 - recall_4: 0.6964 - prec_5: 0.9573 - recall_5: 0.9609 - f1_m: 0.8615 - recall_m: 0.8563 - precision_m: 0.8668 - precision: 0.8668 - recall_6: 0.8563 - f1score: 0.8615 - val_loss: 4.4101 - val_acc: 0.4765 - val_prec: 0.1667 - val_recall: 0.1737 - val_prec_1: 0.1599 - val_recall_1: 0.0568 - val_prec_2: 0.1919 - val_recall_2: 0.1074 - val_prec_3: 0.1709 - val_recall_3: 0.1417 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0305 - val_f1_m: 0.4754 - val_recall_m: 0.4740 - val_precision_m: 0.4769 - val_precision: 0.4769 - val_recall_6: 0.4740 - val_f1score: 0.4754\n",
      "Epoch 78/300\n",
      "epoch:  77\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 656us/step - loss: 0.2959 - acc: 0.8623 - prec: 0.9781 - recall: 0.9867 - prec_1: 0.9157 - recall_1: 0.9255 - prec_2: 0.9542 - recall_2: 0.9504 - prec_3: 0.6924 - recall_3: 0.7073 - prec_4: 0.7084 - recall_4: 0.6874 - prec_5: 0.9404 - recall_5: 0.9589 - f1_m: 0.8615 - recall_m: 0.8561 - precision_m: 0.8671 - precision: 0.8671 - recall_6: 0.8561 - f1score: 0.8615 - val_loss: 4.6546 - val_acc: 0.4243 - val_prec: 0.1669 - val_recall: 0.1759 - val_prec_1: 0.1748 - val_recall_1: 0.0624 - val_prec_2: 0.1919 - val_recall_2: 0.1128 - val_prec_3: 0.0640 - val_recall_3: 0.0097 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0873 - val_f1_m: 0.4226 - val_recall_m: 0.4203 - val_precision_m: 0.4251 - val_precision: 0.4251 - val_recall_6: 0.4203 - val_f1score: 0.4226\n",
      "Epoch 79/300\n",
      "epoch:  78\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2942 - acc: 0.8601 - prec: 0.9752 - recall: 0.9778 - prec_1: 0.9174 - recall_1: 0.9302 - prec_2: 0.9556 - recall_2: 0.9508 - prec_3: 0.6789 - recall_3: 0.6904 - prec_4: 0.6792 - recall_4: 0.6762 - prec_5: 0.9468 - recall_5: 0.9597 - f1_m: 0.8597 - recall_m: 0.8547 - precision_m: 0.8649 - precision: 0.8649 - recall_6: 0.8547 - f1score: 0.8597 - val_loss: 9.1245 - val_acc: 0.2411 - val_prec: 0.0640 - val_recall: 0.0050 - val_prec_1: 0.1673 - val_recall_1: 0.1757 - val_prec_2: 0.1919 - val_recall_2: 0.0640 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0885 - val_recall_5: 0.0142 - val_f1_m: 0.2410 - val_recall_m: 0.2409 - val_precision_m: 0.2411 - val_precision: 0.2411 - val_recall_6: 0.2409 - val_f1score: 0.2410\n",
      "Epoch 80/300\n",
      "epoch:  79\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2849 - acc: 0.8672 - prec: 0.9751 - recall: 0.9858 - prec_1: 0.9309 - recall_1: 0.9455 - prec_2: 0.9666 - recall_2: 0.9515 - prec_3: 0.6847 - recall_3: 0.6873 - prec_4: 0.6919 - recall_4: 0.6827 - prec_5: 0.9607 - recall_5: 0.9634 - f1_m: 0.8667 - recall_m: 0.8617 - precision_m: 0.8719 - precision: 0.8719 - recall_6: 0.8617 - f1score: 0.8667 - val_loss: 2.5388 - val_acc: 0.6192 - val_prec: 0.1688 - val_recall: 0.1494 - val_prec_1: 0.1753 - val_recall_1: 0.0484 - val_prec_2: 0.1802 - val_recall_2: 0.1816 - val_prec_3: 0.0640 - val_recall_3: 0.0322 - val_prec_4: 0.1911 - val_recall_4: 0.1254 - val_prec_5: 0.1678 - val_recall_5: 0.1432 - val_f1_m: 0.5815 - val_recall_m: 0.5675 - val_precision_m: 0.6034 - val_precision: 0.6034 - val_recall_6: 0.5675 - val_f1score: 0.5815\n",
      "Epoch 81/300\n",
      "epoch:  80\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2967 - acc: 0.8573 - prec: 0.9710 - recall: 0.9853 - prec_1: 0.9211 - recall_1: 0.9335 - prec_2: 0.9517 - recall_2: 0.9478 - prec_3: 0.6826 - recall_3: 0.6704 - prec_4: 0.6779 - recall_4: 0.6774 - prec_5: 0.9517 - recall_5: 0.9594 - f1_m: 0.8569 - recall_m: 0.8518 - precision_m: 0.8621 - precision: 0.8621 - recall_6: 0.8518 - f1score: 0.8569 - val_loss: 4.8308 - val_acc: 0.5292 - val_prec: 0.1759 - val_recall: 0.1477 - val_prec_1: 0.1759 - val_recall_1: 0.0847 - val_prec_2: 0.1826 - val_recall_2: 0.1805 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1513 - val_f1_m: 0.5266 - val_recall_m: 0.5205 - val_precision_m: 0.5331 - val_precision: 0.5331 - val_recall_6: 0.5205 - val_f1score: 0.5266\n",
      "Epoch 82/300\n",
      "epoch:  81\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2861 - acc: 0.8627 - prec: 0.9729 - recall: 0.9855 - prec_1: 0.9325 - recall_1: 0.9344 - prec_2: 0.9550 - recall_2: 0.9583 - prec_3: 0.6879 - recall_3: 0.6769 - prec_4: 0.6782 - recall_4: 0.6694 - prec_5: 0.9582 - recall_5: 0.9637 - f1_m: 0.8617 - recall_m: 0.8568 - precision_m: 0.8666 - precision: 0.8666 - recall_6: 0.8568 - f1score: 0.8617 - val_loss: 2.4905 - val_acc: 0.6519 - val_prec: 0.1759 - val_recall: 0.1722 - val_prec_1: 0.1750 - val_recall_1: 0.1490 - val_prec_2: 0.1919 - val_recall_2: 0.1643 - val_prec_3: 0.0640 - val_recall_3: 0.0207 - val_prec_4: 0.1439 - val_recall_4: 0.0505 - val_prec_5: 0.1684 - val_recall_5: 0.1437 - val_f1_m: 0.6503 - val_recall_m: 0.6467 - val_precision_m: 0.6540 - val_precision: 0.6540 - val_recall_6: 0.6467 - val_f1score: 0.6503\n",
      "Epoch 83/300\n",
      "epoch:  82\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.2883 - acc: 0.8645 - prec: 0.9690 - recall: 0.9852 - prec_1: 0.9216 - recall_1: 0.9398 - prec_2: 0.9574 - recall_2: 0.9583 - prec_3: 0.6886 - recall_3: 0.7108 - prec_4: 0.6975 - recall_4: 0.6733 - prec_5: 0.9534 - recall_5: 0.9567 - f1_m: 0.8642 - recall_m: 0.8587 - precision_m: 0.8698 - precision: 0.8698 - recall_6: 0.8587 - f1score: 0.8642 - val_loss: 0.8652 - val_acc: 0.7241 - val_prec: 0.1753 - val_recall: 0.1714 - val_prec_1: 0.1753 - val_recall_1: 0.1285 - val_prec_2: 0.1896 - val_recall_2: 0.1718 - val_prec_3: 0.1709 - val_recall_3: 0.1429 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1554 - val_f1_m: 0.7242 - val_recall_m: 0.7179 - val_precision_m: 0.7308 - val_precision: 0.7308 - val_recall_6: 0.7179 - val_f1score: 0.7242\n",
      "Epoch 84/300\n",
      "epoch:  83\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2873 - acc: 0.8653 - prec: 0.9742 - recall: 0.9838 - prec_1: 0.9282 - recall_1: 0.9307 - prec_2: 0.9562 - recall_2: 0.9511 - prec_3: 0.7027 - recall_3: 0.6907 - prec_4: 0.6849 - recall_4: 0.7001 - prec_5: 0.9494 - recall_5: 0.9684 - f1_m: 0.8652 - recall_m: 0.8601 - precision_m: 0.8705 - precision: 0.8705 - recall_6: 0.8601 - f1score: 0.8652 - val_loss: 0.7508 - val_acc: 0.6862 - val_prec: 0.1713 - val_recall: 0.1720 - val_prec_1: 0.1751 - val_recall_1: 0.1471 - val_prec_2: 0.1919 - val_recall_2: 0.1587 - val_prec_3: 0.1599 - val_recall_3: 0.0999 - val_prec_4: 0.1599 - val_recall_4: 0.0502 - val_prec_5: 0.1684 - val_recall_5: 0.0969 - val_f1_m: 0.6503 - val_recall_m: 0.6357 - val_precision_m: 0.6701 - val_precision: 0.6701 - val_recall_6: 0.6357 - val_f1score: 0.6503\n",
      "Epoch 85/300\n",
      "epoch:  84\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2881 - acc: 0.8616 - prec: 0.9734 - recall: 0.9806 - prec_1: 0.9233 - recall_1: 0.9359 - prec_2: 0.9557 - recall_2: 0.9567 - prec_3: 0.6830 - recall_3: 0.6762 - prec_4: 0.6767 - recall_4: 0.6802 - prec_5: 0.9536 - recall_5: 0.9629 - f1_m: 0.8607 - recall_m: 0.8552 - precision_m: 0.8663 - precision: 0.8663 - recall_6: 0.8552 - f1score: 0.8607 - val_loss: 4.5119 - val_acc: 0.4995 - val_prec: 0.1668 - val_recall: 0.1730 - val_prec_1: 0.1753 - val_recall_1: 0.0986 - val_prec_2: 0.1919 - val_recall_2: 0.1531 - val_prec_3: 0.0960 - val_recall_3: 0.0252 - val_prec_4: 0.0160 - val_recall_4: 0.0022 - val_prec_5: 0.1524 - val_recall_5: 0.0763 - val_f1_m: 0.4960 - val_recall_m: 0.4923 - val_precision_m: 0.4999 - val_precision: 0.4999 - val_recall_6: 0.4923 - val_f1score: 0.4960\n",
      "Epoch 86/300\n",
      "epoch:  85\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2862 - acc: 0.8593 - prec: 0.9716 - recall: 0.9819 - prec_1: 0.9288 - recall_1: 0.9269 - prec_2: 0.9508 - recall_2: 0.9527 - prec_3: 0.6740 - recall_3: 0.6824 - prec_4: 0.6789 - recall_4: 0.6706 - prec_5: 0.9507 - recall_5: 0.9645 - f1_m: 0.8593 - recall_m: 0.8537 - precision_m: 0.8651 - precision: 0.8651 - recall_6: 0.8537 - f1score: 0.8593 - val_loss: 5.4406 - val_acc: 0.4498 - val_prec: 0.1689 - val_recall: 0.1684 - val_prec_1: 0.1759 - val_recall_1: 0.0699 - val_prec_2: 0.1738 - val_recall_2: 0.1720 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0710 - val_f1_m: 0.4497 - val_recall_m: 0.4468 - val_precision_m: 0.4529 - val_precision: 0.4529 - val_recall_6: 0.4468 - val_f1score: 0.4497\n",
      "Epoch 87/300\n",
      "epoch:  86\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2880 - acc: 0.8642 - prec: 0.9705 - recall: 0.9834 - prec_1: 0.9229 - recall_1: 0.9344 - prec_2: 0.9523 - recall_2: 0.9575 - prec_3: 0.6882 - recall_3: 0.6988 - prec_4: 0.6898 - recall_4: 0.6759 - prec_5: 0.9520 - recall_5: 0.9643 - f1_m: 0.8641 - recall_m: 0.8592 - precision_m: 0.8691 - precision: 0.8691 - recall_6: 0.8592 - f1score: 0.8641 - val_loss: 8.5845 - val_acc: 0.3096 - val_prec: 0.1667 - val_recall: 0.1754 - val_prec_1: 0.1275 - val_recall_1: 0.0289 - val_prec_2: 0.1759 - val_recall_2: 0.1008 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0230 - val_f1_m: 0.3076 - val_recall_m: 0.3066 - val_precision_m: 0.3087 - val_precision: 0.3087 - val_recall_6: 0.3066 - val_f1score: 0.3076\n",
      "Epoch 88/300\n",
      "epoch:  87\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2840 - acc: 0.8643 - prec: 0.9776 - recall: 0.9860 - prec_1: 0.9345 - recall_1: 0.9410 - prec_2: 0.9643 - recall_2: 0.9577 - prec_3: 0.6897 - recall_3: 0.6662 - prec_4: 0.6753 - recall_4: 0.6952 - prec_5: 0.9552 - recall_5: 0.9654 - f1_m: 0.8636 - recall_m: 0.8586 - precision_m: 0.8686 - precision: 0.8686 - recall_6: 0.8586 - f1score: 0.8636 - val_loss: 4.5348 - val_acc: 0.5072 - val_prec: 0.1669 - val_recall: 0.1526 - val_prec_1: 0.1703 - val_recall_1: 0.0988 - val_prec_2: 0.1859 - val_recall_2: 0.1627 - val_prec_3: 0.1119 - val_recall_3: 0.0417 - val_prec_4: 0.1279 - val_recall_4: 0.0310 - val_prec_5: 0.1524 - val_recall_5: 0.0655 - val_f1_m: 0.5056 - val_recall_m: 0.5002 - val_precision_m: 0.5112 - val_precision: 0.5112 - val_recall_6: 0.5002 - val_f1score: 0.5056\n",
      "Epoch 89/300\n",
      "epoch:  88\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2831 - acc: 0.8673 - prec: 0.9758 - recall: 0.9845 - prec_1: 0.9268 - recall_1: 0.9356 - prec_2: 0.9559 - recall_2: 0.9562 - prec_3: 0.6938 - recall_3: 0.7039 - prec_4: 0.6985 - recall_4: 0.6855 - prec_5: 0.9596 - recall_5: 0.9650 - f1_m: 0.8668 - recall_m: 0.8615 - precision_m: 0.8723 - precision: 0.8723 - recall_6: 0.8615 - f1score: 0.8668 - val_loss: 2.5295 - val_acc: 0.5520 - val_prec: 0.1670 - val_recall: 0.1734 - val_prec_1: 0.1759 - val_recall_1: 0.0596 - val_prec_2: 0.1896 - val_recall_2: 0.1480 - val_prec_3: 0.0960 - val_recall_3: 0.0216 - val_prec_4: 0.1661 - val_recall_4: 0.0930 - val_prec_5: 0.1524 - val_recall_5: 0.1001 - val_f1_m: 0.5480 - val_recall_m: 0.5432 - val_precision_m: 0.5533 - val_precision: 0.5533 - val_recall_6: 0.5432 - val_f1score: 0.5480\n",
      "Epoch 90/300\n",
      "epoch:  89\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2804 - acc: 0.8686 - prec: 0.9782 - recall: 0.9835 - prec_1: 0.9224 - recall_1: 0.9357 - prec_2: 0.9502 - recall_2: 0.9523 - prec_3: 0.7026 - recall_3: 0.7044 - prec_4: 0.6976 - recall_4: 0.6978 - prec_5: 0.9602 - recall_5: 0.9647 - f1_m: 0.8674 - recall_m: 0.8615 - precision_m: 0.8735 - precision: 0.8735 - recall_6: 0.8615 - f1score: 0.8674 - val_loss: 1.2309 - val_acc: 0.6589 - val_prec: 0.1759 - val_recall: 0.1352 - val_prec_1: 0.1439 - val_recall_1: 0.0387 - val_prec_2: 0.1848 - val_recall_2: 0.1849 - val_prec_3: 0.1439 - val_recall_3: 0.0537 - val_prec_4: 0.1809 - val_recall_4: 0.1602 - val_prec_5: 0.1684 - val_recall_5: 0.1524 - val_f1_m: 0.6443 - val_recall_m: 0.6319 - val_precision_m: 0.6618 - val_precision: 0.6618 - val_recall_6: 0.6319 - val_f1score: 0.6443\n",
      "Epoch 91/300\n",
      "epoch:  90\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 657us/step - loss: 0.2800 - acc: 0.8705 - prec: 0.9749 - recall: 0.9851 - prec_1: 0.9351 - recall_1: 0.9380 - prec_2: 0.9621 - recall_2: 0.9522 - prec_3: 0.6941 - recall_3: 0.7102 - prec_4: 0.7029 - recall_4: 0.6887 - prec_5: 0.9573 - recall_5: 0.9699 - f1_m: 0.8706 - recall_m: 0.8658 - precision_m: 0.8755 - precision: 0.8755 - recall_6: 0.8658 - f1score: 0.8706 - val_loss: 3.9866 - val_acc: 0.5015 - val_prec: 0.1747 - val_recall: 0.1731 - val_prec_1: 0.1743 - val_recall_1: 0.1631 - val_prec_2: 0.1919 - val_recall_2: 0.1041 - val_prec_3: 0.0480 - val_recall_3: 0.0112 - val_prec_4: 0.1119 - val_recall_4: 0.0317 - val_prec_5: 0.1444 - val_recall_5: 0.0580 - val_f1_m: 0.4984 - val_recall_m: 0.4975 - val_precision_m: 0.4993 - val_precision: 0.4993 - val_recall_6: 0.4975 - val_f1score: 0.4984\n",
      "Epoch 92/300\n",
      "epoch:  91\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2767 - acc: 0.8703 - prec: 0.9779 - recall: 0.9878 - prec_1: 0.9225 - recall_1: 0.9477 - prec_2: 0.9638 - recall_2: 0.9529 - prec_3: 0.6982 - recall_3: 0.6925 - prec_4: 0.7033 - recall_4: 0.7034 - prec_5: 0.9559 - recall_5: 0.9608 - f1_m: 0.8700 - recall_m: 0.8652 - precision_m: 0.8750 - precision: 0.8750 - recall_6: 0.8652 - f1score: 0.8700 - val_loss: 1.5017 - val_acc: 0.6667 - val_prec: 0.1759 - val_recall: 0.1690 - val_prec_1: 0.1743 - val_recall_1: 0.1635 - val_prec_2: 0.1919 - val_recall_2: 0.1185 - val_prec_3: 0.1740 - val_recall_3: 0.1030 - val_prec_4: 0.1119 - val_recall_4: 0.0337 - val_prec_5: 0.1684 - val_recall_5: 0.1282 - val_f1_m: 0.6649 - val_recall_m: 0.6614 - val_precision_m: 0.6685 - val_precision: 0.6685 - val_recall_6: 0.6614 - val_f1score: 0.6649\n",
      "Epoch 93/300\n",
      "epoch:  92\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2923 - acc: 0.8605 - prec: 0.9726 - recall: 0.9809 - prec_1: 0.9262 - recall_1: 0.9408 - prec_2: 0.9483 - recall_2: 0.9529 - prec_3: 0.6764 - recall_3: 0.6858 - prec_4: 0.6735 - recall_4: 0.6648 - prec_5: 0.9563 - recall_5: 0.9642 - f1_m: 0.8597 - recall_m: 0.8552 - precision_m: 0.8644 - precision: 0.8644 - recall_6: 0.8552 - f1score: 0.8597 - val_loss: 0.6361 - val_acc: 0.7739 - val_prec: 0.1759 - val_recall: 0.1636 - val_prec_1: 0.1746 - val_recall_1: 0.1666 - val_prec_2: 0.1919 - val_recall_2: 0.1572 - val_prec_3: 0.1709 - val_recall_3: 0.1744 - val_prec_4: 0.0960 - val_recall_4: 0.0150 - val_prec_5: 0.1684 - val_recall_5: 0.1428 - val_f1_m: 0.7755 - val_recall_m: 0.7711 - val_precision_m: 0.7801 - val_precision: 0.7801 - val_recall_6: 0.7711 - val_f1score: 0.7755\n",
      "Epoch 94/300\n",
      "epoch:  93\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2830 - acc: 0.8636 - prec: 0.9697 - recall: 0.9871 - prec_1: 0.9396 - recall_1: 0.9321 - prec_2: 0.9496 - recall_2: 0.9589 - prec_3: 0.6894 - recall_3: 0.6799 - prec_4: 0.6773 - recall_4: 0.6735 - prec_5: 0.9512 - recall_5: 0.9647 - f1_m: 0.8641 - recall_m: 0.8595 - precision_m: 0.8688 - precision: 0.8688 - recall_6: 0.8595 - f1score: 0.8641 - val_loss: 2.7714 - val_acc: 0.6287 - val_prec: 0.1736 - val_recall: 0.1651 - val_prec_1: 0.1745 - val_recall_1: 0.1535 - val_prec_2: 0.1839 - val_recall_2: 0.1033 - val_prec_3: 0.1439 - val_recall_3: 0.0525 - val_prec_4: 0.1789 - val_recall_4: 0.1441 - val_prec_5: 0.1674 - val_recall_5: 0.0596 - val_f1_m: 0.6255 - val_recall_m: 0.6217 - val_precision_m: 0.6295 - val_precision: 0.6295 - val_recall_6: 0.6217 - val_f1score: 0.6255\n",
      "Epoch 95/300\n",
      "epoch:  94\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2820 - acc: 0.8630 - prec: 0.9752 - recall: 0.9855 - prec_1: 0.9185 - recall_1: 0.9379 - prec_2: 0.9503 - recall_2: 0.9490 - prec_3: 0.6824 - recall_3: 0.7071 - prec_4: 0.7031 - recall_4: 0.6698 - prec_5: 0.9534 - recall_5: 0.9558 - f1_m: 0.8631 - recall_m: 0.8575 - precision_m: 0.8689 - precision: 0.8689 - recall_6: 0.8575 - f1score: 0.8631 - val_loss: 0.7085 - val_acc: 0.7331 - val_prec: 0.1759 - val_recall: 0.1591 - val_prec_1: 0.1756 - val_recall_1: 0.1132 - val_prec_2: 0.1859 - val_recall_2: 0.1697 - val_prec_3: 0.1759 - val_recall_3: 0.1137 - val_prec_4: 0.1599 - val_recall_4: 0.0650 - val_prec_5: 0.1684 - val_recall_5: 0.1628 - val_f1_m: 0.7167 - val_recall_m: 0.6974 - val_precision_m: 0.7475 - val_precision: 0.7475 - val_recall_6: 0.6974 - val_f1score: 0.7167\n",
      "Epoch 96/300\n",
      "epoch:  95\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2808 - acc: 0.8692 - prec: 0.9705 - recall: 0.9837 - prec_1: 0.9224 - recall_1: 0.9391 - prec_2: 0.9518 - recall_2: 0.9530 - prec_3: 0.6976 - recall_3: 0.7135 - prec_4: 0.7086 - recall_4: 0.6839 - prec_5: 0.9649 - recall_5: 0.9639 - f1_m: 0.8699 - recall_m: 0.8647 - precision_m: 0.8753 - precision: 0.8753 - recall_6: 0.8647 - f1score: 0.8699 - val_loss: 1.2785 - val_acc: 0.7101 - val_prec: 0.1759 - val_recall: 0.1682 - val_prec_1: 0.1756 - val_recall_1: 0.1359 - val_prec_2: 0.1901 - val_recall_2: 0.1827 - val_prec_3: 0.1599 - val_recall_3: 0.0725 - val_prec_4: 0.1119 - val_recall_4: 0.0552 - val_prec_5: 0.1684 - val_recall_5: 0.1544 - val_f1_m: 0.7096 - val_recall_m: 0.7039 - val_precision_m: 0.7156 - val_precision: 0.7156 - val_recall_6: 0.7039 - val_f1score: 0.7096\n",
      "Epoch 97/300\n",
      "epoch:  96\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 658us/step - loss: 0.2760 - acc: 0.8692 - prec: 0.9786 - recall: 0.9844 - prec_1: 0.9393 - recall_1: 0.9406 - prec_2: 0.9521 - recall_2: 0.9613 - prec_3: 0.6902 - recall_3: 0.7000 - prec_4: 0.6969 - recall_4: 0.6848 - prec_5: 0.9662 - recall_5: 0.9674 - f1_m: 0.8681 - recall_m: 0.8635 - precision_m: 0.8728 - precision: 0.8728 - recall_6: 0.8635 - f1score: 0.8681 - val_loss: 0.3679 - val_acc: 0.8446 - val_prec: 0.1759 - val_recall: 0.1697 - val_prec_1: 0.1748 - val_recall_1: 0.1657 - val_prec_2: 0.1919 - val_recall_2: 0.1607 - val_prec_3: 0.1701 - val_recall_3: 0.1377 - val_prec_4: 0.1759 - val_recall_4: 0.1114 - val_prec_5: 0.1684 - val_recall_5: 0.1574 - val_f1_m: 0.8413 - val_recall_m: 0.8333 - val_precision_m: 0.8501 - val_precision: 0.8501 - val_recall_6: 0.8333 - val_f1score: 0.8413\n",
      "Epoch 98/300\n",
      "epoch:  97\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2740 - acc: 0.8655 - prec: 0.9780 - recall: 0.9825 - prec_1: 0.9305 - recall_1: 0.9461 - prec_2: 0.9585 - recall_2: 0.9571 - prec_3: 0.6807 - recall_3: 0.6877 - prec_4: 0.6771 - recall_4: 0.6759 - prec_5: 0.9671 - recall_5: 0.9705 - f1_m: 0.8652 - recall_m: 0.8602 - precision_m: 0.8703 - precision: 0.8703 - recall_6: 0.8602 - f1score: 0.8652 - val_loss: 2.8768 - val_acc: 0.5700 - val_prec: 0.1743 - val_recall: 0.1754 - val_prec_1: 0.1740 - val_recall_1: 0.1330 - val_prec_2: 0.1819 - val_recall_2: 0.1470 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0160 - val_recall_4: 2.4988e-04 - val_prec_5: 0.1684 - val_recall_5: 0.1481 - val_f1_m: 0.5691 - val_recall_m: 0.5642 - val_precision_m: 0.5743 - val_precision: 0.5743 - val_recall_6: 0.5642 - val_f1score: 0.5691\n",
      "Epoch 99/300\n",
      "epoch:  98\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2751 - acc: 0.8710 - prec: 0.9680 - recall: 0.9887 - prec_1: 0.9422 - recall_1: 0.9556 - prec_2: 0.9631 - recall_2: 0.9571 - prec_3: 0.6920 - recall_3: 0.7043 - prec_4: 0.7021 - recall_4: 0.6896 - prec_5: 0.9668 - recall_5: 0.9676 - f1_m: 0.8709 - recall_m: 0.8663 - precision_m: 0.8756 - precision: 0.8756 - recall_6: 0.8663 - f1score: 0.8709 - val_loss: 2.4646 - val_acc: 0.5272 - val_prec: 0.1759 - val_recall: 0.0960 - val_prec_1: 0.1599 - val_recall_1: 0.0695 - val_prec_2: 0.1839 - val_recall_2: 0.1866 - val_prec_3: 0.1439 - val_recall_3: 0.0717 - val_prec_4: 0.0480 - val_recall_4: 0.0035 - val_prec_5: 0.1684 - val_recall_5: 0.1399 - val_f1_m: 0.5228 - val_recall_m: 0.5207 - val_precision_m: 0.5250 - val_precision: 0.5250 - val_recall_6: 0.5207 - val_f1score: 0.5228\n",
      "Epoch 100/300\n",
      "epoch:  99\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2756 - acc: 0.8690 - prec: 0.9770 - recall: 0.9854 - prec_1: 0.9283 - recall_1: 0.9429 - prec_2: 0.9510 - recall_2: 0.9485 - prec_3: 0.6949 - recall_3: 0.7044 - prec_4: 0.7051 - recall_4: 0.6846 - prec_5: 0.9585 - recall_5: 0.9644 - f1_m: 0.8683 - recall_m: 0.8641 - precision_m: 0.8727 - precision: 0.8727 - recall_6: 0.8641 - f1score: 0.8683 - val_loss: 2.2907 - val_acc: 0.5147 - val_prec: 0.1759 - val_recall: 0.1530 - val_prec_1: 0.1756 - val_recall_1: 0.1343 - val_prec_2: 0.1759 - val_recall_2: 0.0695 - val_prec_3: 0.0800 - val_recall_3: 0.0265 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1541 - val_f1_m: 0.5134 - val_recall_m: 0.5122 - val_precision_m: 0.5147 - val_precision: 0.5147 - val_recall_6: 0.5122 - val_f1score: 0.5134\n",
      "Epoch 101/300\n",
      "epoch:  100\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2771 - acc: 0.8672 - prec: 0.9706 - recall: 0.9817 - prec_1: 0.9239 - recall_1: 0.9432 - prec_2: 0.9600 - recall_2: 0.9527 - prec_3: 0.6872 - recall_3: 0.7061 - prec_4: 0.7005 - recall_4: 0.6792 - prec_5: 0.9626 - recall_5: 0.9638 - f1_m: 0.8673 - recall_m: 0.8630 - precision_m: 0.8717 - precision: 0.8717 - recall_6: 0.8630 - f1score: 0.8673 - val_loss: 0.6533 - val_acc: 0.7479 - val_prec: 0.1747 - val_recall: 0.1706 - val_prec_1: 0.1735 - val_recall_1: 0.1502 - val_prec_2: 0.1919 - val_recall_2: 0.1554 - val_prec_3: 0.1493 - val_recall_3: 0.0658 - val_prec_4: 0.1599 - val_recall_4: 0.0917 - val_prec_5: 0.1684 - val_recall_5: 0.1561 - val_f1_m: 0.7463 - val_recall_m: 0.7414 - val_precision_m: 0.7516 - val_precision: 0.7516 - val_recall_6: 0.7414 - val_f1score: 0.7463\n",
      "Epoch 102/300\n",
      "epoch:  101\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2737 - acc: 0.8665 - prec: 0.9796 - recall: 0.9836 - prec_1: 0.9258 - recall_1: 0.9412 - prec_2: 0.9596 - recall_2: 0.9579 - prec_3: 0.6852 - recall_3: 0.7073 - prec_4: 0.6937 - recall_4: 0.6668 - prec_5: 0.9581 - recall_5: 0.9624 - f1_m: 0.8673 - recall_m: 0.8626 - precision_m: 0.8721 - precision: 0.8721 - recall_6: 0.8626 - f1score: 0.8673 - val_loss: 4.3179 - val_acc: 0.5527 - val_prec: 0.1702 - val_recall: 0.1757 - val_prec_1: 0.1753 - val_recall_1: 0.1303 - val_prec_2: 0.1802 - val_recall_2: 0.1512 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1293 - val_f1_m: 0.5523 - val_recall_m: 0.5482 - val_precision_m: 0.5566 - val_precision: 0.5566 - val_recall_6: 0.5482 - val_f1score: 0.5523\n",
      "Epoch 103/300\n",
      "epoch:  102\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2667 - acc: 0.8750 - prec: 0.9788 - recall: 0.9888 - prec_1: 0.9379 - recall_1: 0.9478 - prec_2: 0.9597 - recall_2: 0.9595 - prec_3: 0.7003 - recall_3: 0.7185 - prec_4: 0.7085 - recall_4: 0.6895 - prec_5: 0.9674 - recall_5: 0.9720 - f1_m: 0.8741 - recall_m: 0.8692 - precision_m: 0.8792 - precision: 0.8792 - recall_6: 0.8692 - f1score: 0.8741 - val_loss: 5.6231 - val_acc: 0.4313 - val_prec: 0.1668 - val_recall: 0.1754 - val_prec_1: 0.1759 - val_recall_1: 0.0441 - val_prec_2: 0.1859 - val_recall_2: 0.1386 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1279 - val_recall_4: 0.0497 - val_prec_5: 0.1364 - val_recall_5: 0.0620 - val_f1_m: 0.4296 - val_recall_m: 0.4280 - val_precision_m: 0.4312 - val_precision: 0.4312 - val_recall_6: 0.4280 - val_f1score: 0.4296\n",
      "Epoch 104/300\n",
      "epoch:  103\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2638 - acc: 0.8772 - prec: 0.9799 - recall: 0.9873 - prec_1: 0.9369 - recall_1: 0.9559 - prec_2: 0.9698 - recall_2: 0.9619 - prec_3: 0.7093 - recall_3: 0.7171 - prec_4: 0.7108 - recall_4: 0.7055 - prec_5: 0.9702 - recall_5: 0.9664 - f1_m: 0.8762 - recall_m: 0.8716 - precision_m: 0.8810 - precision: 0.8810 - recall_6: 0.8716 - f1score: 0.8762 - val_loss: 4.6198 - val_acc: 0.5160 - val_prec: 0.1759 - val_recall: 0.1502 - val_prec_1: 0.1756 - val_recall_1: 0.1024 - val_prec_2: 0.1890 - val_recall_2: 0.1833 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1173 - val_f1_m: 0.5161 - val_recall_m: 0.5120 - val_precision_m: 0.5204 - val_precision: 0.5204 - val_recall_6: 0.5120 - val_f1score: 0.5161\n",
      "Epoch 105/300\n",
      "epoch:  104\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2650 - acc: 0.8710 - prec: 0.9767 - recall: 0.9828 - prec_1: 0.9489 - recall_1: 0.9513 - prec_2: 0.9709 - recall_2: 0.9651 - prec_3: 0.6876 - recall_3: 0.7020 - prec_4: 0.6843 - recall_4: 0.6735 - prec_5: 0.9640 - recall_5: 0.9733 - f1_m: 0.8711 - recall_m: 0.8673 - precision_m: 0.8749 - precision: 0.8749 - recall_6: 0.8673 - f1score: 0.8711 - val_loss: 1.4181 - val_acc: 0.5957 - val_prec: 0.1686 - val_recall: 0.1725 - val_prec_1: 0.1742 - val_recall_1: 0.1242 - val_prec_2: 0.1839 - val_recall_2: 0.1348 - val_prec_3: 0.1759 - val_recall_3: 0.0975 - val_prec_4: 0.1119 - val_recall_4: 0.0292 - val_prec_5: 0.1524 - val_recall_5: 0.0698 - val_f1_m: 0.5895 - val_recall_m: 0.5830 - val_precision_m: 0.5970 - val_precision: 0.5970 - val_recall_6: 0.5830 - val_f1score: 0.5895\n",
      "Epoch 106/300\n",
      "epoch:  105\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.2703 - acc: 0.8693 - prec: 0.9763 - recall: 0.9863 - prec_1: 0.9371 - recall_1: 0.9476 - prec_2: 0.9563 - recall_2: 0.9577 - prec_3: 0.6933 - recall_3: 0.6801 - prec_4: 0.6898 - recall_4: 0.6944 - prec_5: 0.9696 - recall_5: 0.9675 - f1_m: 0.8685 - recall_m: 0.8638 - precision_m: 0.8733 - precision: 0.8733 - recall_6: 0.8638 - f1score: 0.8685 - val_loss: 5.1163 - val_acc: 0.5892 - val_prec: 0.1759 - val_recall: 0.1704 - val_prec_1: 0.1753 - val_recall_1: 0.1549 - val_prec_2: 0.1896 - val_recall_2: 0.1630 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1392 - val_f1_m: 0.5887 - val_recall_m: 0.5865 - val_precision_m: 0.5911 - val_precision: 0.5911 - val_recall_6: 0.5865 - val_f1score: 0.5887\n",
      "Epoch 107/300\n",
      "epoch:  106\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2739 - acc: 0.8678 - prec: 0.9729 - recall: 0.9838 - prec_1: 0.9364 - recall_1: 0.9455 - prec_2: 0.9614 - recall_2: 0.9583 - prec_3: 0.6896 - recall_3: 0.7062 - prec_4: 0.6827 - recall_4: 0.6677 - prec_5: 0.9559 - recall_5: 0.9658 - f1_m: 0.8672 - recall_m: 0.8623 - precision_m: 0.8723 - precision: 0.8723 - recall_6: 0.8623 - f1score: 0.8672 - val_loss: 0.7048 - val_acc: 0.7334 - val_prec: 0.1759 - val_recall: 0.1445 - val_prec_1: 0.1759 - val_recall_1: 0.1335 - val_prec_2: 0.1919 - val_recall_2: 0.1548 - val_prec_3: 0.1709 - val_recall_3: 0.1632 - val_prec_4: 0.1119 - val_recall_4: 0.0175 - val_prec_5: 0.1684 - val_recall_5: 0.1598 - val_f1_m: 0.7287 - val_recall_m: 0.7221 - val_precision_m: 0.7370 - val_precision: 0.7370 - val_recall_6: 0.7221 - val_f1score: 0.7287\n",
      "Epoch 108/300\n",
      "epoch:  107\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 658us/step - loss: 0.2731 - acc: 0.8685 - prec: 0.9755 - recall: 0.9849 - prec_1: 0.9320 - recall_1: 0.9375 - prec_2: 0.9548 - recall_2: 0.9531 - prec_3: 0.6970 - recall_3: 0.6993 - prec_4: 0.6961 - recall_4: 0.6881 - prec_5: 0.9602 - recall_5: 0.9677 - f1_m: 0.8689 - recall_m: 0.8640 - precision_m: 0.8741 - precision: 0.8741 - recall_6: 0.8640 - f1score: 0.8689 - val_loss: 0.6074 - val_acc: 0.7491 - val_prec: 0.1753 - val_recall: 0.1717 - val_prec_1: 0.1746 - val_recall_1: 0.1349 - val_prec_2: 0.1919 - val_recall_2: 0.1715 - val_prec_3: 0.1759 - val_recall_3: 0.1259 - val_prec_4: 0.1439 - val_recall_4: 0.0460 - val_prec_5: 0.1684 - val_recall_5: 0.1572 - val_f1_m: 0.7338 - val_recall_m: 0.7136 - val_precision_m: 0.7667 - val_precision: 0.7667 - val_recall_6: 0.7136 - val_f1score: 0.7338\n",
      "Epoch 109/300\n",
      "epoch:  108\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2686 - acc: 0.8732 - prec: 0.9765 - recall: 0.9874 - prec_1: 0.9443 - recall_1: 0.9474 - prec_2: 0.9581 - recall_2: 0.9640 - prec_3: 0.6824 - recall_3: 0.7135 - prec_4: 0.7082 - recall_4: 0.6725 - prec_5: 0.9666 - recall_5: 0.9758 - f1_m: 0.8724 - recall_m: 0.8677 - precision_m: 0.8773 - precision: 0.8773 - recall_6: 0.8677 - f1score: 0.8724 - val_loss: 2.0679 - val_acc: 0.6327 - val_prec: 0.1679 - val_recall: 0.1737 - val_prec_1: 0.1759 - val_recall_1: 0.0754 - val_prec_2: 0.1855 - val_recall_2: 0.1625 - val_prec_3: 0.1709 - val_recall_3: 0.1392 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1249 - val_f1_m: 0.6311 - val_recall_m: 0.6269 - val_precision_m: 0.6356 - val_precision: 0.6356 - val_recall_6: 0.6269 - val_f1score: 0.6311\n",
      "Epoch 110/300\n",
      "epoch:  109\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 658us/step - loss: 0.2666 - acc: 0.8743 - prec: 0.9761 - recall: 0.9888 - prec_1: 0.9284 - recall_1: 0.9388 - prec_2: 0.9553 - recall_2: 0.9610 - prec_3: 0.7052 - recall_3: 0.7195 - prec_4: 0.7126 - recall_4: 0.6996 - prec_5: 0.9613 - recall_5: 0.9650 - f1_m: 0.8738 - recall_m: 0.8698 - precision_m: 0.8778 - precision: 0.8778 - recall_6: 0.8698 - f1score: 0.8738 - val_loss: 1.3294 - val_acc: 0.6672 - val_prec: 0.1753 - val_recall: 0.1705 - val_prec_1: 0.1759 - val_recall_1: 0.0607 - val_prec_2: 0.1800 - val_recall_2: 0.1862 - val_prec_3: 0.1759 - val_recall_3: 0.1444 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1491 - val_f1_m: 0.6586 - val_recall_m: 0.6529 - val_precision_m: 0.6690 - val_precision: 0.6690 - val_recall_6: 0.6529 - val_f1score: 0.6586\n",
      "Epoch 111/300\n",
      "epoch:  110\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2625 - acc: 0.8750 - prec: 0.9741 - recall: 0.9877 - prec_1: 0.9398 - recall_1: 0.9473 - prec_2: 0.9627 - recall_2: 0.9659 - prec_3: 0.7114 - recall_3: 0.6928 - prec_4: 0.6985 - recall_4: 0.7080 - prec_5: 0.9695 - recall_5: 0.9758 - f1_m: 0.8744 - recall_m: 0.8697 - precision_m: 0.8793 - precision: 0.8793 - recall_6: 0.8697 - f1score: 0.8744 - val_loss: 1.6032 - val_acc: 0.7164 - val_prec: 0.1759 - val_recall: 0.1417 - val_prec_1: 0.1734 - val_recall_1: 0.1477 - val_prec_2: 0.1826 - val_recall_2: 0.1689 - val_prec_3: 0.1439 - val_recall_3: 0.0582 - val_prec_4: 0.1599 - val_recall_4: 0.0832 - val_prec_5: 0.1684 - val_recall_5: 0.1562 - val_f1_m: 0.7156 - val_recall_m: 0.7094 - val_precision_m: 0.7221 - val_precision: 0.7221 - val_recall_6: 0.7094 - val_f1score: 0.7156\n",
      "Epoch 112/300\n",
      "epoch:  111\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.2615 - acc: 0.8758 - prec: 0.9794 - recall: 0.9880 - prec_1: 0.9440 - recall_1: 0.9583 - prec_2: 0.9721 - recall_2: 0.9590 - prec_3: 0.6937 - recall_3: 0.7119 - prec_4: 0.7080 - recall_4: 0.6926 - prec_5: 0.9679 - recall_5: 0.9680 - f1_m: 0.8758 - recall_m: 0.8705 - precision_m: 0.8812 - precision: 0.8812 - recall_6: 0.8705 - f1score: 0.8758 - val_loss: 2.2079 - val_acc: 0.5432 - val_prec: 0.1759 - val_recall: 0.1632 - val_prec_1: 0.1759 - val_recall_1: 0.0810 - val_prec_2: 0.1708 - val_recall_2: 0.1914 - val_prec_3: 0.0640 - val_recall_3: 0.0230 - val_prec_4: 0.0480 - val_recall_4: 0.0092 - val_prec_5: 0.1684 - val_recall_5: 0.1127 - val_f1_m: 0.5276 - val_recall_m: 0.5235 - val_precision_m: 0.5325 - val_precision: 0.5325 - val_recall_6: 0.5235 - val_f1score: 0.5276\n",
      "Epoch 113/300\n",
      "epoch:  112\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2619 - acc: 0.8740 - prec: 0.9800 - recall: 0.9853 - prec_1: 0.9429 - recall_1: 0.9580 - prec_2: 0.9645 - recall_2: 0.9573 - prec_3: 0.6947 - recall_3: 0.7029 - prec_4: 0.6985 - recall_4: 0.6951 - prec_5: 0.9709 - recall_5: 0.9729 - f1_m: 0.8738 - recall_m: 0.8692 - precision_m: 0.8784 - precision: 0.8784 - recall_6: 0.8692 - f1score: 0.8738 - val_loss: 0.7087 - val_acc: 0.7401 - val_prec: 0.1742 - val_recall: 0.1725 - val_prec_1: 0.1751 - val_recall_1: 0.1548 - val_prec_2: 0.1919 - val_recall_2: 0.1678 - val_prec_3: 0.1709 - val_recall_3: 0.0964 - val_prec_4: 0.1649 - val_recall_4: 0.0839 - val_prec_5: 0.1684 - val_recall_5: 0.1158 - val_f1_m: 0.7400 - val_recall_m: 0.7304 - val_precision_m: 0.7512 - val_precision: 0.7512 - val_recall_6: 0.7304 - val_f1score: 0.7400\n",
      "Epoch 114/300\n",
      "epoch:  113\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2633 - acc: 0.8782 - prec: 0.9817 - recall: 0.9824 - prec_1: 0.9342 - recall_1: 0.9486 - prec_2: 0.9548 - recall_2: 0.9592 - prec_3: 0.7192 - recall_3: 0.7236 - prec_4: 0.7176 - recall_4: 0.7133 - prec_5: 0.9620 - recall_5: 0.9658 - f1_m: 0.8781 - recall_m: 0.8735 - precision_m: 0.8828 - precision: 0.8828 - recall_6: 0.8735 - f1score: 0.8781 - val_loss: 3.5241 - val_acc: 0.5847 - val_prec: 0.1759 - val_recall: 0.1652 - val_prec_1: 0.1759 - val_recall_1: 0.1193 - val_prec_2: 0.1919 - val_recall_2: 0.1831 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1557 - val_f1_m: 0.5843 - val_recall_m: 0.5815 - val_precision_m: 0.5873 - val_precision: 0.5873 - val_recall_6: 0.5815 - val_f1score: 0.5843\n",
      "Epoch 115/300\n",
      "epoch:  114\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2628 - acc: 0.8783 - prec: 0.9779 - recall: 0.9854 - prec_1: 0.9392 - recall_1: 0.9585 - prec_2: 0.9667 - recall_2: 0.9636 - prec_3: 0.7021 - recall_3: 0.7128 - prec_4: 0.7123 - recall_4: 0.6939 - prec_5: 0.9751 - recall_5: 0.9728 - f1_m: 0.8781 - recall_m: 0.8730 - precision_m: 0.8833 - precision: 0.8833 - recall_6: 0.8730 - f1score: 0.8781 - val_loss: 0.9363 - val_acc: 0.7094 - val_prec: 0.1724 - val_recall: 0.1715 - val_prec_1: 0.1750 - val_recall_1: 0.1322 - val_prec_2: 0.1919 - val_recall_2: 0.1391 - val_prec_3: 0.1709 - val_recall_3: 0.1719 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1389 - val_f1_m: 0.7081 - val_recall_m: 0.7031 - val_precision_m: 0.7134 - val_precision: 0.7134 - val_recall_6: 0.7031 - val_f1score: 0.7081\n",
      "Epoch 116/300\n",
      "epoch:  115\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2658 - acc: 0.8758 - prec: 0.9778 - recall: 0.9839 - prec_1: 0.9463 - recall_1: 0.9446 - prec_2: 0.9596 - recall_2: 0.9635 - prec_3: 0.7053 - recall_3: 0.7188 - prec_4: 0.7053 - recall_4: 0.6936 - prec_5: 0.9685 - recall_5: 0.9744 - f1_m: 0.8744 - recall_m: 0.8700 - precision_m: 0.8790 - precision: 0.8790 - recall_6: 0.8700 - f1score: 0.8744 - val_loss: 0.7924 - val_acc: 0.6924 - val_prec: 0.1706 - val_recall: 0.1631 - val_prec_1: 0.1759 - val_recall_1: 0.0907 - val_prec_2: 0.1803 - val_recall_2: 0.1860 - val_prec_3: 0.1119 - val_recall_3: 0.0330 - val_prec_4: 0.1809 - val_recall_4: 0.1555 - val_prec_5: 0.1684 - val_recall_5: 0.1160 - val_f1_m: 0.6889 - val_recall_m: 0.6817 - val_precision_m: 0.6968 - val_precision: 0.6968 - val_recall_6: 0.6817 - val_f1score: 0.6889\n",
      "Epoch 117/300\n",
      "epoch:  116\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2569 - acc: 0.8782 - prec: 0.9733 - recall: 0.9881 - prec_1: 0.9465 - recall_1: 0.9486 - prec_2: 0.9687 - recall_2: 0.9634 - prec_3: 0.7045 - recall_3: 0.7270 - prec_4: 0.7164 - recall_4: 0.6842 - prec_5: 0.9651 - recall_5: 0.9708 - f1_m: 0.8779 - recall_m: 0.8732 - precision_m: 0.8828 - precision: 0.8828 - recall_6: 0.8732 - f1score: 0.8779 - val_loss: 5.7516 - val_acc: 0.5465 - val_prec: 0.1747 - val_recall: 0.1671 - val_prec_1: 0.1729 - val_recall_1: 0.1675 - val_prec_2: 0.1919 - val_recall_2: 0.1428 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1039 - val_f1_m: 0.5454 - val_recall_m: 0.5442 - val_precision_m: 0.5466 - val_precision: 0.5466 - val_recall_6: 0.5442 - val_f1score: 0.5454\n",
      "Epoch 118/300\n",
      "epoch:  117\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2690 - acc: 0.8733 - prec: 0.9781 - recall: 0.9882 - prec_1: 0.9344 - recall_1: 0.9519 - prec_2: 0.9691 - recall_2: 0.9620 - prec_3: 0.6948 - recall_3: 0.7013 - prec_4: 0.7050 - recall_4: 0.6930 - prec_5: 0.9692 - recall_5: 0.9660 - f1_m: 0.8726 - recall_m: 0.8688 - precision_m: 0.8765 - precision: 0.8765 - recall_6: 0.8688 - f1score: 0.8726 - val_loss: 0.7707 - val_acc: 0.7754 - val_prec: 0.1759 - val_recall: 0.1614 - val_prec_1: 0.1756 - val_recall_1: 0.1345 - val_prec_2: 0.1899 - val_recall_2: 0.1809 - val_prec_3: 0.1709 - val_recall_3: 0.1563 - val_prec_4: 0.1119 - val_recall_4: 0.0250 - val_prec_5: 0.1684 - val_recall_5: 0.1618 - val_f1_m: 0.7747 - val_recall_m: 0.7686 - val_precision_m: 0.7812 - val_precision: 0.7812 - val_recall_6: 0.7686 - val_f1score: 0.7747\n",
      "Epoch 119/300\n",
      "epoch:  118\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2617 - acc: 0.8761 - prec: 0.9820 - recall: 0.9906 - prec_1: 0.9491 - recall_1: 0.9643 - prec_2: 0.9720 - recall_2: 0.9662 - prec_3: 0.6973 - recall_3: 0.6936 - prec_4: 0.6952 - recall_4: 0.6928 - prec_5: 0.9730 - recall_5: 0.9760 - f1_m: 0.8749 - recall_m: 0.8703 - precision_m: 0.8796 - precision: 0.8796 - recall_6: 0.8703 - f1score: 0.8749 - val_loss: 1.7798 - val_acc: 0.7379 - val_prec: 0.1747 - val_recall: 0.1664 - val_prec_1: 0.1755 - val_recall_1: 0.1237 - val_prec_2: 0.1828 - val_recall_2: 0.1873 - val_prec_3: 0.1439 - val_recall_3: 0.0602 - val_prec_4: 0.1759 - val_recall_4: 0.1427 - val_prec_5: 0.1684 - val_recall_5: 0.1171 - val_f1_m: 0.7354 - val_recall_m: 0.7274 - val_precision_m: 0.7440 - val_precision: 0.7440 - val_recall_6: 0.7274 - val_f1score: 0.7354\n",
      "Epoch 120/300\n",
      "epoch:  119\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2562 - acc: 0.8762 - prec: 0.9788 - recall: 0.9873 - prec_1: 0.9415 - recall_1: 0.9550 - prec_2: 0.9705 - recall_2: 0.9635 - prec_3: 0.6987 - recall_3: 0.7162 - prec_4: 0.7064 - recall_4: 0.6873 - prec_5: 0.9670 - recall_5: 0.9713 - f1_m: 0.8763 - recall_m: 0.8725 - precision_m: 0.8802 - precision: 0.8802 - recall_6: 0.8725 - f1score: 0.8763 - val_loss: 3.0971 - val_acc: 0.5830 - val_prec: 0.1759 - val_recall: 0.1614 - val_prec_1: 0.1756 - val_recall_1: 0.1529 - val_prec_2: 0.1919 - val_recall_2: 0.1548 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1517 - val_f1_m: 0.5833 - val_recall_m: 0.5817 - val_precision_m: 0.5849 - val_precision: 0.5849 - val_recall_6: 0.5817 - val_f1score: 0.5833\n",
      "Epoch 121/300\n",
      "epoch:  120\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2602 - acc: 0.8771 - prec: 0.9778 - recall: 0.9845 - prec_1: 0.9457 - recall_1: 0.9551 - prec_2: 0.9632 - recall_2: 0.9611 - prec_3: 0.7015 - recall_3: 0.7091 - prec_4: 0.7023 - recall_4: 0.6986 - prec_5: 0.9722 - recall_5: 0.9762 - f1_m: 0.8763 - recall_m: 0.8725 - precision_m: 0.8801 - precision: 0.8801 - recall_6: 0.8725 - f1score: 0.8763 - val_loss: 3.3380 - val_acc: 0.6224 - val_prec: 0.1753 - val_recall: 0.1747 - val_prec_1: 0.1750 - val_recall_1: 0.1565 - val_prec_2: 0.1804 - val_recall_2: 0.1786 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1528 - val_f1_m: 0.6227 - val_recall_m: 0.6199 - val_precision_m: 0.6256 - val_precision: 0.6256 - val_recall_6: 0.6199 - val_f1score: 0.6227\n",
      "Epoch 122/300\n",
      "epoch:  121\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2555 - acc: 0.8790 - prec: 0.9823 - recall: 0.9887 - prec_1: 0.9500 - recall_1: 0.9606 - prec_2: 0.9701 - recall_2: 0.9667 - prec_3: 0.7053 - recall_3: 0.6935 - prec_4: 0.7016 - recall_4: 0.7127 - prec_5: 0.9660 - recall_5: 0.9755 - f1_m: 0.8772 - recall_m: 0.8731 - precision_m: 0.8814 - precision: 0.8814 - recall_6: 0.8731 - f1score: 0.8772 - val_loss: 2.1354 - val_acc: 0.6354 - val_prec: 0.1742 - val_recall: 0.1737 - val_prec_1: 0.1747 - val_recall_1: 0.1506 - val_prec_2: 0.1919 - val_recall_2: 0.1648 - val_prec_3: 0.1279 - val_recall_3: 0.0495 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1389 - val_f1_m: 0.6340 - val_recall_m: 0.6299 - val_precision_m: 0.6382 - val_precision: 0.6382 - val_recall_6: 0.6299 - val_f1score: 0.6340\n",
      "Epoch 123/300\n",
      "epoch:  122\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2577 - acc: 0.8781 - prec: 0.9749 - recall: 0.9869 - prec_1: 0.9428 - recall_1: 0.9508 - prec_2: 0.9648 - recall_2: 0.9609 - prec_3: 0.7101 - recall_3: 0.7198 - prec_4: 0.7236 - recall_4: 0.7036 - prec_5: 0.9640 - recall_5: 0.9707 - f1_m: 0.8784 - recall_m: 0.8745 - precision_m: 0.8825 - precision: 0.8825 - recall_6: 0.8745 - f1score: 0.8784 - val_loss: 0.9594 - val_acc: 0.6569 - val_prec: 0.1759 - val_recall: 0.0741 - val_prec_1: 0.1716 - val_recall_1: 0.1112 - val_prec_2: 0.1866 - val_recall_2: 0.1635 - val_prec_3: 0.1119 - val_recall_3: 0.0182 - val_prec_4: 0.1809 - val_recall_4: 0.1827 - val_prec_5: 0.1684 - val_recall_5: 0.1646 - val_f1_m: 0.6495 - val_recall_m: 0.6412 - val_precision_m: 0.6593 - val_precision: 0.6593 - val_recall_6: 0.6412 - val_f1score: 0.6495\n",
      "Epoch 124/300\n",
      "epoch:  123\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2644 - acc: 0.8751 - prec: 0.9805 - recall: 0.9847 - prec_1: 0.9399 - recall_1: 0.9565 - prec_2: 0.9613 - recall_2: 0.9622 - prec_3: 0.6991 - recall_3: 0.7228 - prec_4: 0.7104 - recall_4: 0.6778 - prec_5: 0.9682 - recall_5: 0.9725 - f1_m: 0.8754 - recall_m: 0.8713 - precision_m: 0.8796 - precision: 0.8796 - recall_6: 0.8713 - f1score: 0.8754 - val_loss: 2.0843 - val_acc: 0.6069 - val_prec: 0.1753 - val_recall: 0.1691 - val_prec_1: 0.1747 - val_recall_1: 0.1197 - val_prec_2: 0.1919 - val_recall_2: 0.1158 - val_prec_3: 0.1279 - val_recall_3: 0.0485 - val_prec_4: 0.1119 - val_recall_4: 0.0402 - val_prec_5: 0.1681 - val_recall_5: 0.1619 - val_f1_m: 0.6048 - val_recall_m: 0.6017 - val_precision_m: 0.6081 - val_precision: 0.6081 - val_recall_6: 0.6017 - val_f1score: 0.6048\n",
      "Epoch 125/300\n",
      "epoch:  124\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2557 - acc: 0.8775 - prec: 0.9799 - recall: 0.9844 - prec_1: 0.9538 - recall_1: 0.9541 - prec_2: 0.9663 - recall_2: 0.9639 - prec_3: 0.6996 - recall_3: 0.7063 - prec_4: 0.6945 - recall_4: 0.6938 - prec_5: 0.9681 - recall_5: 0.9758 - f1_m: 0.8765 - recall_m: 0.8721 - precision_m: 0.8810 - precision: 0.8810 - recall_6: 0.8721 - f1score: 0.8765 - val_loss: 3.6119 - val_acc: 0.5232 - val_prec: 0.1662 - val_recall: 0.1674 - val_prec_1: 0.1587 - val_recall_1: 0.0830 - val_prec_2: 0.1759 - val_recall_2: 0.1178 - val_prec_3: 0.0640 - val_recall_3: 0.0087 - val_prec_4: 0.1759 - val_recall_4: 0.1372 - val_prec_5: 0.1524 - val_recall_5: 0.0443 - val_f1_m: 0.5202 - val_recall_m: 0.5160 - val_precision_m: 0.5246 - val_precision: 0.5246 - val_recall_6: 0.5160 - val_f1score: 0.5202\n",
      "Epoch 126/300\n",
      "epoch:  125\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2499 - acc: 0.8808 - prec: 0.9795 - recall: 0.9871 - prec_1: 0.9466 - recall_1: 0.9523 - prec_2: 0.9724 - recall_2: 0.9714 - prec_3: 0.7067 - recall_3: 0.7134 - prec_4: 0.7134 - recall_4: 0.7034 - prec_5: 0.9728 - recall_5: 0.9730 - f1_m: 0.8809 - recall_m: 0.8765 - precision_m: 0.8855 - precision: 0.8855 - recall_6: 0.8765 - f1score: 0.8809 - val_loss: 5.3502 - val_acc: 0.4730 - val_prec: 0.1669 - val_recall: 0.1754 - val_prec_1: 0.1747 - val_recall_1: 0.0551 - val_prec_2: 0.1828 - val_recall_2: 0.1374 - val_prec_3: 0.1439 - val_recall_3: 0.0574 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0767 - val_f1_m: 0.4707 - val_recall_m: 0.4680 - val_precision_m: 0.4735 - val_precision: 0.4735 - val_recall_6: 0.4680 - val_f1score: 0.4707\n",
      "Epoch 127/300\n",
      "epoch:  126\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 655us/step - loss: 0.2561 - acc: 0.8776 - prec: 0.9768 - recall: 0.9874 - prec_1: 0.9432 - recall_1: 0.9546 - prec_2: 0.9712 - recall_2: 0.9650 - prec_3: 0.7020 - recall_3: 0.7114 - prec_4: 0.7110 - recall_4: 0.6964 - prec_5: 0.9676 - recall_5: 0.9739 - f1_m: 0.8769 - recall_m: 0.8732 - precision_m: 0.8808 - precision: 0.8808 - recall_6: 0.8732 - f1score: 0.8769 - val_loss: 2.1101 - val_acc: 0.6972 - val_prec: 0.1747 - val_recall: 0.1725 - val_prec_1: 0.1747 - val_recall_1: 0.1386 - val_prec_2: 0.1919 - val_recall_2: 0.1770 - val_prec_3: 0.1279 - val_recall_3: 0.0412 - val_prec_4: 0.1503 - val_recall_4: 0.0682 - val_prec_5: 0.1684 - val_recall_5: 0.1533 - val_f1_m: 0.6935 - val_recall_m: 0.6854 - val_precision_m: 0.7025 - val_precision: 0.7025 - val_recall_6: 0.6854 - val_f1score: 0.6935\n",
      "Epoch 128/300\n",
      "epoch:  127\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2528 - acc: 0.8807 - prec: 0.9812 - recall: 0.9889 - prec_1: 0.9444 - recall_1: 0.9504 - prec_2: 0.9651 - recall_2: 0.9625 - prec_3: 0.7208 - recall_3: 0.7057 - prec_4: 0.7033 - recall_4: 0.7170 - prec_5: 0.9724 - recall_5: 0.9767 - f1_m: 0.8798 - recall_m: 0.8761 - precision_m: 0.8836 - precision: 0.8836 - recall_6: 0.8761 - f1score: 0.8798 - val_loss: 1.4977 - val_acc: 0.6604 - val_prec: 0.1748 - val_recall: 0.1734 - val_prec_1: 0.1742 - val_recall_1: 0.1544 - val_prec_2: 0.1919 - val_recall_2: 0.1104 - val_prec_3: 0.1709 - val_recall_3: 0.1392 - val_prec_4: 0.1439 - val_recall_4: 0.0135 - val_prec_5: 0.1684 - val_recall_5: 0.1111 - val_f1_m: 0.6470 - val_recall_m: 0.6404 - val_precision_m: 0.6543 - val_precision: 0.6543 - val_recall_6: 0.6404 - val_f1score: 0.6470\n",
      "Epoch 129/300\n",
      "epoch:  128\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2523 - acc: 0.8827 - prec: 0.9794 - recall: 0.9892 - prec_1: 0.9565 - recall_1: 0.9627 - prec_2: 0.9735 - recall_2: 0.9716 - prec_3: 0.7035 - recall_3: 0.7299 - prec_4: 0.7123 - recall_4: 0.6823 - prec_5: 0.9719 - recall_5: 0.9744 - f1_m: 0.8814 - recall_m: 0.8776 - precision_m: 0.8854 - precision: 0.8854 - recall_6: 0.8776 - f1score: 0.8814 - val_loss: 2.3428 - val_acc: 0.5942 - val_prec: 0.1753 - val_recall: 0.1744 - val_prec_1: 0.1745 - val_recall_1: 0.1590 - val_prec_2: 0.1919 - val_recall_2: 0.1328 - val_prec_3: 0.0960 - val_recall_3: 0.0240 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1393 - val_f1_m: 0.5913 - val_recall_m: 0.5895 - val_precision_m: 0.5932 - val_precision: 0.5932 - val_recall_6: 0.5895 - val_f1score: 0.5913\n",
      "Epoch 130/300\n",
      "epoch:  129\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2511 - acc: 0.8822 - prec: 0.9814 - recall: 0.9871 - prec_1: 0.9539 - recall_1: 0.9686 - prec_2: 0.9700 - recall_2: 0.9673 - prec_3: 0.7068 - recall_3: 0.7204 - prec_4: 0.7154 - recall_4: 0.6907 - prec_5: 0.9734 - recall_5: 0.9724 - f1_m: 0.8824 - recall_m: 0.8782 - precision_m: 0.8867 - precision: 0.8867 - recall_6: 0.8782 - f1score: 0.8824 - val_loss: 2.7346 - val_acc: 0.5882 - val_prec: 0.1670 - val_recall: 0.1737 - val_prec_1: 0.1756 - val_recall_1: 0.0666 - val_prec_2: 0.1919 - val_recall_2: 0.1619 - val_prec_3: 0.1709 - val_recall_3: 0.1427 - val_prec_4: 0.0160 - val_recall_4: 4.9975e-04 - val_prec_5: 0.1524 - val_recall_5: 0.0826 - val_f1_m: 0.5845 - val_recall_m: 0.5815 - val_precision_m: 0.5878 - val_precision: 0.5878 - val_recall_6: 0.5815 - val_f1score: 0.5845\n",
      "Epoch 131/300\n",
      "epoch:  130\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2505 - acc: 0.8831 - prec: 0.9753 - recall: 0.9885 - prec_1: 0.9571 - recall_1: 0.9602 - prec_2: 0.9720 - recall_2: 0.9711 - prec_3: 0.7193 - recall_3: 0.7055 - prec_4: 0.7023 - recall_4: 0.7135 - prec_5: 0.9747 - recall_5: 0.9756 - f1_m: 0.8835 - recall_m: 0.8806 - precision_m: 0.8865 - precision: 0.8865 - recall_6: 0.8806 - f1score: 0.8835 - val_loss: 1.5105 - val_acc: 0.5825 - val_prec: 0.1759 - val_recall: 0.1607 - val_prec_1: 0.1759 - val_recall_1: 0.0763 - val_prec_2: 0.1716 - val_recall_2: 0.1888 - val_prec_3: 0.1119 - val_recall_3: 0.0137 - val_prec_4: 0.1119 - val_recall_4: 0.0392 - val_prec_5: 0.1684 - val_recall_5: 0.1541 - val_f1_m: 0.5704 - val_recall_m: 0.5670 - val_precision_m: 0.5742 - val_precision: 0.5742 - val_recall_6: 0.5670 - val_f1score: 0.5704\n",
      "Epoch 132/300\n",
      "epoch:  131\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2566 - acc: 0.8797 - prec: 0.9810 - recall: 0.9891 - prec_1: 0.9465 - recall_1: 0.9613 - prec_2: 0.9643 - recall_2: 0.9630 - prec_3: 0.7110 - recall_3: 0.7112 - prec_4: 0.7154 - recall_4: 0.7078 - prec_5: 0.9690 - recall_5: 0.9717 - f1_m: 0.8803 - recall_m: 0.8763 - precision_m: 0.8844 - precision: 0.8844 - recall_6: 0.8763 - f1score: 0.8803 - val_loss: 0.9187 - val_acc: 0.7361 - val_prec: 0.1759 - val_recall: 0.1637 - val_prec_1: 0.1744 - val_recall_1: 0.1346 - val_prec_2: 0.1903 - val_recall_2: 0.1846 - val_prec_3: 0.1708 - val_recall_3: 0.1257 - val_prec_4: 0.1599 - val_recall_4: 0.0495 - val_prec_5: 0.1684 - val_recall_5: 0.1268 - val_f1_m: 0.7336 - val_recall_m: 0.7289 - val_precision_m: 0.7385 - val_precision: 0.7385 - val_recall_6: 0.7289 - val_f1score: 0.7336\n",
      "Epoch 133/300\n",
      "epoch:  132\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2576 - acc: 0.8777 - prec: 0.9767 - recall: 0.9877 - prec_1: 0.9475 - recall_1: 0.9595 - prec_2: 0.9647 - recall_2: 0.9653 - prec_3: 0.7004 - recall_3: 0.7125 - prec_4: 0.7076 - recall_4: 0.6876 - prec_5: 0.9713 - recall_5: 0.9726 - f1_m: 0.8781 - recall_m: 0.8741 - precision_m: 0.8823 - precision: 0.8823 - recall_6: 0.8741 - f1score: 0.8781 - val_loss: 0.7041 - val_acc: 0.7284 - val_prec: 0.1699 - val_recall: 0.1722 - val_prec_1: 0.1753 - val_recall_1: 0.1289 - val_prec_2: 0.1919 - val_recall_2: 0.1509 - val_prec_3: 0.1709 - val_recall_3: 0.1714 - val_prec_4: 0.1439 - val_recall_4: 0.0250 - val_prec_5: 0.1684 - val_recall_5: 0.1233 - val_f1_m: 0.7250 - val_recall_m: 0.7189 - val_precision_m: 0.7315 - val_precision: 0.7315 - val_recall_6: 0.7189 - val_f1score: 0.7250\n",
      "Epoch 134/300\n",
      "epoch:  133\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2523 - acc: 0.8787 - prec: 0.9808 - recall: 0.9859 - prec_1: 0.9459 - recall_1: 0.9641 - prec_2: 0.9711 - recall_2: 0.9632 - prec_3: 0.6931 - recall_3: 0.7282 - prec_4: 0.7155 - recall_4: 0.6845 - prec_5: 0.9715 - recall_5: 0.9742 - f1_m: 0.8786 - recall_m: 0.8755 - precision_m: 0.8818 - precision: 0.8818 - recall_6: 0.8755 - f1score: 0.8786 - val_loss: 2.4681 - val_acc: 0.6969 - val_prec: 0.1736 - val_recall: 0.1611 - val_prec_1: 0.1759 - val_recall_1: 0.0622 - val_prec_2: 0.1858 - val_recall_2: 0.1840 - val_prec_3: 0.0160 - val_recall_3: 7.4963e-04 - val_prec_4: 0.1809 - val_recall_4: 0.1811 - val_prec_5: 0.1676 - val_recall_5: 0.1638 - val_f1_m: 0.6947 - val_recall_m: 0.6902 - val_precision_m: 0.6997 - val_precision: 0.6997 - val_recall_6: 0.6902 - val_f1score: 0.6947\n",
      "Epoch 135/300\n",
      "epoch:  134\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2472 - acc: 0.8858 - prec: 0.9826 - recall: 0.9888 - prec_1: 0.9501 - recall_1: 0.9599 - prec_2: 0.9689 - recall_2: 0.9713 - prec_3: 0.7195 - recall_3: 0.7209 - prec_4: 0.7212 - recall_4: 0.7229 - prec_5: 0.9734 - recall_5: 0.9735 - f1_m: 0.8855 - recall_m: 0.8821 - precision_m: 0.8889 - precision: 0.8889 - recall_6: 0.8821 - f1score: 0.8855 - val_loss: 2.4828 - val_acc: 0.6219 - val_prec: 0.1683 - val_recall: 0.1699 - val_prec_1: 0.1755 - val_recall_1: 0.0864 - val_prec_2: 0.1879 - val_recall_2: 0.1745 - val_prec_3: 0.0480 - val_recall_3: 0.0032 - val_prec_4: 0.1759 - val_recall_4: 0.1414 - val_prec_5: 0.1684 - val_recall_5: 0.0932 - val_f1_m: 0.6199 - val_recall_m: 0.6114 - val_precision_m: 0.6291 - val_precision: 0.6291 - val_recall_6: 0.6114 - val_f1score: 0.6199\n",
      "Epoch 136/300\n",
      "epoch:  135\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2510 - acc: 0.8823 - prec: 0.9833 - recall: 0.9877 - prec_1: 0.9519 - recall_1: 0.9475 - prec_2: 0.9567 - recall_2: 0.9617 - prec_3: 0.7198 - recall_3: 0.7213 - prec_4: 0.7224 - recall_4: 0.7241 - prec_5: 0.9631 - recall_5: 0.9774 - f1_m: 0.8817 - recall_m: 0.8783 - precision_m: 0.8852 - precision: 0.8852 - recall_6: 0.8783 - f1score: 0.8817 - val_loss: 1.0038 - val_acc: 0.6997 - val_prec: 0.1759 - val_recall: 0.1317 - val_prec_1: 0.1759 - val_recall_1: 0.1302 - val_prec_2: 0.1839 - val_recall_2: 0.1572 - val_prec_3: 0.1709 - val_recall_3: 0.1719 - val_prec_4: 0.0640 - val_recall_4: 0.0045 - val_prec_5: 0.1684 - val_recall_5: 0.1448 - val_f1_m: 0.6965 - val_recall_m: 0.6939 - val_precision_m: 0.6993 - val_precision: 0.6993 - val_recall_6: 0.6939 - val_f1score: 0.6965\n",
      "Epoch 137/300\n",
      "epoch:  136\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2529 - acc: 0.8805 - prec: 0.9726 - recall: 0.9901 - prec_1: 0.9549 - recall_1: 0.9618 - prec_2: 0.9702 - recall_2: 0.9730 - prec_3: 0.7006 - recall_3: 0.7057 - prec_4: 0.7081 - recall_4: 0.6944 - prec_5: 0.9769 - recall_5: 0.9758 - f1_m: 0.8799 - recall_m: 0.8757 - precision_m: 0.8841 - precision: 0.8841 - recall_6: 0.8757 - f1score: 0.8799 - val_loss: 0.6554 - val_acc: 0.7349 - val_prec: 0.1737 - val_recall: 0.1722 - val_prec_1: 0.1747 - val_recall_1: 0.1511 - val_prec_2: 0.1919 - val_recall_2: 0.1351 - val_prec_3: 0.1709 - val_recall_3: 0.1526 - val_prec_4: 0.1279 - val_recall_4: 0.0222 - val_prec_5: 0.1684 - val_recall_5: 0.1430 - val_f1_m: 0.7271 - val_recall_m: 0.7146 - val_precision_m: 0.7430 - val_precision: 0.7430 - val_recall_6: 0.7146 - val_f1score: 0.7271\n",
      "Epoch 138/300\n",
      "epoch:  137\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2473 - acc: 0.8836 - prec: 0.9842 - recall: 0.9900 - prec_1: 0.9444 - recall_1: 0.9619 - prec_2: 0.9760 - recall_2: 0.9668 - prec_3: 0.7179 - recall_3: 0.7282 - prec_4: 0.7170 - recall_4: 0.7036 - prec_5: 0.9703 - recall_5: 0.9756 - f1_m: 0.8837 - recall_m: 0.8802 - precision_m: 0.8872 - precision: 0.8872 - recall_6: 0.8802 - f1score: 0.8837 - val_loss: 1.5011 - val_acc: 0.6739 - val_prec: 0.1759 - val_recall: 0.1456 - val_prec_1: 0.1759 - val_recall_1: 0.0923 - val_prec_2: 0.1802 - val_recall_2: 0.1744 - val_prec_3: 0.1439 - val_recall_3: 0.0542 - val_prec_4: 0.1649 - val_recall_4: 0.1154 - val_prec_5: 0.1666 - val_recall_5: 0.1399 - val_f1_m: 0.6725 - val_recall_m: 0.6649 - val_precision_m: 0.6805 - val_precision: 0.6805 - val_recall_6: 0.6649 - val_f1score: 0.6725\n",
      "Epoch 139/300\n",
      "epoch:  138\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2469 - acc: 0.8833 - prec: 0.9845 - recall: 0.9902 - prec_1: 0.9559 - recall_1: 0.9664 - prec_2: 0.9760 - recall_2: 0.9724 - prec_3: 0.7137 - recall_3: 0.7164 - prec_4: 0.7108 - recall_4: 0.7059 - prec_5: 0.9653 - recall_5: 0.9701 - f1_m: 0.8833 - recall_m: 0.8800 - precision_m: 0.8866 - precision: 0.8866 - recall_6: 0.8800 - f1score: 0.8833 - val_loss: 0.7747 - val_acc: 0.6854 - val_prec: 0.1759 - val_recall: 0.1700 - val_prec_1: 0.1755 - val_recall_1: 0.1325 - val_prec_2: 0.1812 - val_recall_2: 0.1835 - val_prec_3: 0.1169 - val_recall_3: 0.0318 - val_prec_4: 0.1759 - val_recall_4: 0.0892 - val_prec_5: 0.1684 - val_recall_5: 0.1339 - val_f1_m: 0.6843 - val_recall_m: 0.6777 - val_precision_m: 0.6917 - val_precision: 0.6917 - val_recall_6: 0.6777 - val_f1score: 0.6843\n",
      "Epoch 140/300\n",
      "epoch:  139\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2442 - acc: 0.8851 - prec: 0.9815 - recall: 0.9866 - prec_1: 0.9606 - recall_1: 0.9586 - prec_2: 0.9703 - recall_2: 0.9689 - prec_3: 0.7134 - recall_3: 0.7182 - prec_4: 0.7164 - recall_4: 0.7110 - prec_5: 0.9714 - recall_5: 0.9829 - f1_m: 0.8843 - recall_m: 0.8802 - precision_m: 0.8885 - precision: 0.8885 - recall_6: 0.8802 - f1score: 0.8843 - val_loss: 5.1403 - val_acc: 0.5897 - val_prec: 0.1759 - val_recall: 0.1651 - val_prec_1: 0.1731 - val_recall_1: 0.1685 - val_prec_2: 0.1910 - val_recall_2: 0.1538 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0160 - val_recall_4: 0.0020 - val_prec_5: 0.1684 - val_recall_5: 0.1361 - val_f1_m: 0.5873 - val_recall_m: 0.5832 - val_precision_m: 0.5921 - val_precision: 0.5921 - val_recall_6: 0.5832 - val_f1score: 0.5873\n",
      "Epoch 141/300\n",
      "epoch:  140\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2435 - acc: 0.8856 - prec: 0.9819 - recall: 0.9882 - prec_1: 0.9550 - recall_1: 0.9617 - prec_2: 0.9748 - recall_2: 0.9721 - prec_3: 0.7177 - recall_3: 0.7237 - prec_4: 0.7149 - recall_4: 0.7044 - prec_5: 0.9688 - recall_5: 0.9775 - f1_m: 0.8849 - recall_m: 0.8812 - precision_m: 0.8887 - precision: 0.8887 - recall_6: 0.8812 - f1score: 0.8849 - val_loss: 2.1823 - val_acc: 0.7086 - val_prec: 0.1747 - val_recall: 0.1594 - val_prec_1: 0.1753 - val_recall_1: 0.1206 - val_prec_2: 0.1884 - val_recall_2: 0.1809 - val_prec_3: 0.0800 - val_recall_3: 0.0279 - val_prec_4: 0.1892 - val_recall_4: 0.1362 - val_prec_5: 0.1684 - val_recall_5: 0.1504 - val_f1_m: 0.7077 - val_recall_m: 0.6972 - val_precision_m: 0.7192 - val_precision: 0.7192 - val_recall_6: 0.6972 - val_f1score: 0.7077\n",
      "Epoch 142/300\n",
      "epoch:  141\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2400 - acc: 0.8891 - prec: 0.9825 - recall: 0.9870 - prec_1: 0.9588 - recall_1: 0.9679 - prec_2: 0.9715 - recall_2: 0.9697 - prec_3: 0.7298 - recall_3: 0.7236 - prec_4: 0.7180 - recall_4: 0.7236 - prec_5: 0.9796 - recall_5: 0.9797 - f1_m: 0.8890 - recall_m: 0.8848 - precision_m: 0.8932 - precision: 0.8932 - recall_6: 0.8848 - f1score: 0.8890 - val_loss: 3.0129 - val_acc: 0.5555 - val_prec: 0.1720 - val_recall: 0.1742 - val_prec_1: 0.1737 - val_recall_1: 0.1295 - val_prec_2: 0.1919 - val_recall_2: 0.1298 - val_prec_3: 0.1279 - val_recall_3: 0.0490 - val_prec_4: 0.0160 - val_recall_4: 2.4988e-04 - val_prec_5: 0.1635 - val_recall_5: 0.1074 - val_f1_m: 0.5525 - val_recall_m: 0.5475 - val_precision_m: 0.5581 - val_precision: 0.5581 - val_recall_6: 0.5475 - val_f1score: 0.5525\n",
      "Epoch 143/300\n",
      "epoch:  142\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2429 - acc: 0.8861 - prec: 0.9811 - recall: 0.9872 - prec_1: 0.9546 - recall_1: 0.9691 - prec_2: 0.9724 - recall_2: 0.9657 - prec_3: 0.7162 - recall_3: 0.7247 - prec_4: 0.7138 - recall_4: 0.7082 - prec_5: 0.9739 - recall_5: 0.9772 - f1_m: 0.8858 - recall_m: 0.8827 - precision_m: 0.8889 - precision: 0.8889 - recall_6: 0.8827 - f1score: 0.8858 - val_loss: 1.3920 - val_acc: 0.7434 - val_prec: 0.1759 - val_recall: 0.1626 - val_prec_1: 0.1749 - val_recall_1: 0.1302 - val_prec_2: 0.1833 - val_recall_2: 0.1798 - val_prec_3: 0.1759 - val_recall_3: 0.1040 - val_prec_4: 0.1439 - val_recall_4: 0.0600 - val_prec_5: 0.1684 - val_recall_5: 0.1582 - val_f1_m: 0.7434 - val_recall_m: 0.7364 - val_precision_m: 0.7509 - val_precision: 0.7509 - val_recall_6: 0.7364 - val_f1score: 0.7434\n",
      "Epoch 144/300\n",
      "epoch:  143\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2456 - acc: 0.8843 - prec: 0.9801 - recall: 0.9882 - prec_1: 0.9538 - recall_1: 0.9663 - prec_2: 0.9766 - recall_2: 0.9716 - prec_3: 0.7088 - recall_3: 0.7169 - prec_4: 0.7214 - recall_4: 0.7060 - prec_5: 0.9714 - recall_5: 0.9789 - f1_m: 0.8833 - recall_m: 0.8792 - precision_m: 0.8875 - precision: 0.8875 - recall_6: 0.8792 - f1score: 0.8833 - val_loss: 1.4735 - val_acc: 0.6592 - val_prec: 0.1759 - val_recall: 0.1695 - val_prec_1: 0.1730 - val_recall_1: 0.1679 - val_prec_2: 0.1866 - val_recall_2: 0.1376 - val_prec_3: 0.1709 - val_recall_3: 0.1354 - val_prec_4: 0.0960 - val_recall_4: 0.0132 - val_prec_5: 0.1524 - val_recall_5: 0.0773 - val_f1_m: 0.6559 - val_recall_m: 0.6524 - val_precision_m: 0.6596 - val_precision: 0.6596 - val_recall_6: 0.6524 - val_f1score: 0.6559\n",
      "Epoch 145/300\n",
      "epoch:  144\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2430 - acc: 0.8885 - prec: 0.9804 - recall: 0.9892 - prec_1: 0.9597 - recall_1: 0.9701 - prec_2: 0.9731 - recall_2: 0.9723 - prec_3: 0.7188 - recall_3: 0.7333 - prec_4: 0.7307 - recall_4: 0.7108 - prec_5: 0.9807 - recall_5: 0.9800 - f1_m: 0.8889 - recall_m: 0.8861 - precision_m: 0.8917 - precision: 0.8917 - recall_6: 0.8861 - f1score: 0.8889 - val_loss: 2.5529 - val_acc: 0.6822 - val_prec: 0.1706 - val_recall: 0.1449 - val_prec_1: 0.1759 - val_recall_1: 0.1262 - val_prec_2: 0.1899 - val_recall_2: 0.1806 - val_prec_3: 0.1119 - val_recall_3: 0.0395 - val_prec_4: 0.1809 - val_recall_4: 0.1679 - val_prec_5: 0.1524 - val_recall_5: 0.0885 - val_f1_m: 0.6820 - val_recall_m: 0.6729 - val_precision_m: 0.6919 - val_precision: 0.6919 - val_recall_6: 0.6729 - val_f1score: 0.6820\n",
      "Epoch 146/300\n",
      "epoch:  145\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2421 - acc: 0.8855 - prec: 0.9816 - recall: 0.9911 - prec_1: 0.9596 - recall_1: 0.9659 - prec_2: 0.9728 - recall_2: 0.9716 - prec_3: 0.7158 - recall_3: 0.7061 - prec_4: 0.7136 - recall_4: 0.7223 - prec_5: 0.9744 - recall_5: 0.9795 - f1_m: 0.8855 - recall_m: 0.8816 - precision_m: 0.8894 - precision: 0.8894 - recall_6: 0.8816 - f1score: 0.8855 - val_loss: 0.7922 - val_acc: 0.7394 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1744 - val_recall_1: 0.1632 - val_prec_2: 0.1919 - val_recall_2: 0.1592 - val_prec_3: 0.1119 - val_recall_3: 0.0162 - val_prec_4: 0.1599 - val_recall_4: 0.1079 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.7366 - val_recall_m: 0.7309 - val_precision_m: 0.7430 - val_precision: 0.7430 - val_recall_6: 0.7309 - val_f1score: 0.7366\n",
      "Epoch 147/300\n",
      "epoch:  146\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2476 - acc: 0.8825 - prec: 0.9798 - recall: 0.9858 - prec_1: 0.9483 - recall_1: 0.9681 - prec_2: 0.9758 - recall_2: 0.9672 - prec_3: 0.7134 - recall_3: 0.7153 - prec_4: 0.7134 - recall_4: 0.7066 - prec_5: 0.9733 - recall_5: 0.9752 - f1_m: 0.8811 - recall_m: 0.8766 - precision_m: 0.8856 - precision: 0.8856 - recall_6: 0.8766 - f1score: 0.8811 - val_loss: 3.9643 - val_acc: 0.5357 - val_prec: 0.1747 - val_recall: 0.1704 - val_prec_1: 0.1730 - val_recall_1: 0.1659 - val_prec_2: 0.1804 - val_recall_2: 0.1613 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0730 - val_f1_m: 0.5353 - val_recall_m: 0.5335 - val_precision_m: 0.5372 - val_precision: 0.5372 - val_recall_6: 0.5335 - val_f1score: 0.5353\n",
      "Epoch 148/300\n",
      "epoch:  147\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2463 - acc: 0.8845 - prec: 0.9823 - recall: 0.9883 - prec_1: 0.9498 - recall_1: 0.9563 - prec_2: 0.9638 - recall_2: 0.9689 - prec_3: 0.7213 - recall_3: 0.7368 - prec_4: 0.7251 - recall_4: 0.7063 - prec_5: 0.9713 - recall_5: 0.9738 - f1_m: 0.8835 - recall_m: 0.8795 - precision_m: 0.8875 - precision: 0.8875 - recall_6: 0.8795 - f1score: 0.8835 - val_loss: 0.7734 - val_acc: 0.7349 - val_prec: 0.1759 - val_recall: 0.1700 - val_prec_1: 0.1744 - val_recall_1: 0.1211 - val_prec_2: 0.1887 - val_recall_2: 0.1852 - val_prec_3: 0.0480 - val_recall_3: 0.0047 - val_prec_4: 0.1649 - val_recall_4: 0.1449 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.7334 - val_recall_m: 0.7251 - val_precision_m: 0.7429 - val_precision: 0.7429 - val_recall_6: 0.7251 - val_f1score: 0.7334\n",
      "Epoch 149/300\n",
      "epoch:  148\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2513 - acc: 0.8803 - prec: 0.9749 - recall: 0.9880 - prec_1: 0.9476 - recall_1: 0.9548 - prec_2: 0.9679 - recall_2: 0.9670 - prec_3: 0.7145 - recall_3: 0.7250 - prec_4: 0.7195 - recall_4: 0.7054 - prec_5: 0.9702 - recall_5: 0.9749 - f1_m: 0.8805 - recall_m: 0.8771 - precision_m: 0.8839 - precision: 0.8839 - recall_6: 0.8771 - f1score: 0.8805 - val_loss: 0.6351 - val_acc: 0.7614 - val_prec: 0.1759 - val_recall: 0.1582 - val_prec_1: 0.1759 - val_recall_1: 0.0900 - val_prec_2: 0.1890 - val_recall_2: 0.1876 - val_prec_3: 0.1656 - val_recall_3: 0.1082 - val_prec_4: 0.1599 - val_recall_4: 0.0985 - val_prec_5: 0.1684 - val_recall_5: 0.1626 - val_f1_m: 0.7609 - val_recall_m: 0.7566 - val_precision_m: 0.7653 - val_precision: 0.7653 - val_recall_6: 0.7566 - val_f1score: 0.7609\n",
      "Epoch 150/300\n",
      "epoch:  149\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2423 - acc: 0.8840 - prec: 0.9894 - recall: 0.9918 - prec_1: 0.9543 - recall_1: 0.9616 - prec_2: 0.9723 - recall_2: 0.9691 - prec_3: 0.7127 - recall_3: 0.7166 - prec_4: 0.7162 - recall_4: 0.7154 - prec_5: 0.9679 - recall_5: 0.9746 - f1_m: 0.8827 - recall_m: 0.8796 - precision_m: 0.8859 - precision: 0.8859 - recall_6: 0.8796 - f1score: 0.8827 - val_loss: 2.3105 - val_acc: 0.6709 - val_prec: 0.1753 - val_recall: 0.1729 - val_prec_1: 0.1731 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1683 - val_prec_3: 0.1279 - val_recall_3: 0.0485 - val_prec_4: 0.0960 - val_recall_4: 0.0137 - val_prec_5: 0.1684 - val_recall_5: 0.1478 - val_f1_m: 0.6627 - val_recall_m: 0.6582 - val_precision_m: 0.6678 - val_precision: 0.6678 - val_recall_6: 0.6582 - val_f1score: 0.6627\n",
      "Epoch 151/300\n",
      "epoch:  150\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 658us/step - loss: 0.2392 - acc: 0.8900 - prec: 0.9812 - recall: 0.9882 - prec_1: 0.9536 - recall_1: 0.9700 - prec_2: 0.9765 - recall_2: 0.9716 - prec_3: 0.7232 - recall_3: 0.7309 - prec_4: 0.7262 - recall_4: 0.7193 - prec_5: 0.9817 - recall_5: 0.9797 - f1_m: 0.8892 - recall_m: 0.8857 - precision_m: 0.8927 - precision: 0.8927 - recall_6: 0.8857 - f1score: 0.8892 - val_loss: 1.4399 - val_acc: 0.7001 - val_prec: 0.1747 - val_recall: 0.1715 - val_prec_1: 0.1759 - val_recall_1: 0.1434 - val_prec_2: 0.1919 - val_recall_2: 0.1744 - val_prec_3: 0.1439 - val_recall_3: 0.0719 - val_prec_4: 0.1119 - val_recall_4: 0.0425 - val_prec_5: 0.1684 - val_recall_5: 0.1526 - val_f1_m: 0.6993 - val_recall_m: 0.6934 - val_precision_m: 0.7055 - val_precision: 0.7055 - val_recall_6: 0.6934 - val_f1score: 0.6993\n",
      "Epoch 152/300\n",
      "epoch:  151\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2304 - acc: 0.8928 - prec: 0.9848 - recall: 0.9899 - prec_1: 0.9651 - recall_1: 0.9607 - prec_2: 0.9693 - recall_2: 0.9839 - prec_3: 0.7271 - recall_3: 0.7504 - prec_4: 0.7376 - recall_4: 0.7197 - prec_5: 0.9813 - recall_5: 0.9812 - f1_m: 0.8932 - recall_m: 0.8898 - precision_m: 0.8966 - precision: 0.8966 - recall_6: 0.8898 - f1score: 0.8932 - val_loss: 0.3736 - val_acc: 0.8373 - val_prec: 0.1759 - val_recall: 0.1701 - val_prec_1: 0.1756 - val_recall_1: 0.1479 - val_prec_2: 0.1901 - val_recall_2: 0.1855 - val_prec_3: 0.1759 - val_recall_3: 0.0756 - val_prec_4: 0.1814 - val_recall_4: 0.1717 - val_prec_5: 0.1684 - val_recall_5: 0.1580 - val_f1_m: 0.8377 - val_recall_m: 0.8346 - val_precision_m: 0.8409 - val_precision: 0.8409 - val_recall_6: 0.8346 - val_f1score: 0.8377\n",
      "Epoch 153/300\n",
      "epoch:  152\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2218 - acc: 0.8968 - prec: 0.9860 - recall: 0.9925 - prec_1: 0.9595 - recall_1: 0.9771 - prec_2: 0.9894 - recall_2: 0.9765 - prec_3: 0.7334 - recall_3: 0.7413 - prec_4: 0.7417 - recall_4: 0.7346 - prec_5: 0.9802 - recall_5: 0.9859 - f1_m: 0.8966 - recall_m: 0.8935 - precision_m: 0.8999 - precision: 0.8999 - recall_6: 0.8935 - f1score: 0.8966 - val_loss: 0.4019 - val_acc: 0.8311 - val_prec: 0.1759 - val_recall: 0.1695 - val_prec_1: 0.1756 - val_recall_1: 0.1566 - val_prec_2: 0.1901 - val_recall_2: 0.1817 - val_prec_3: 0.1759 - val_recall_3: 0.1602 - val_prec_4: 0.1919 - val_recall_4: 0.0680 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8303 - val_recall_m: 0.8268 - val_precision_m: 0.8338 - val_precision: 0.8338 - val_recall_6: 0.8268 - val_f1score: 0.8303\n",
      "Epoch 154/300\n",
      "epoch:  153\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2168 - acc: 0.9001 - prec: 0.9865 - recall: 0.9933 - prec_1: 0.9777 - recall_1: 0.9824 - prec_2: 0.9847 - recall_2: 0.9868 - prec_3: 0.7307 - recall_3: 0.7457 - prec_4: 0.7384 - recall_4: 0.7280 - prec_5: 0.9860 - recall_5: 0.9889 - f1_m: 0.8996 - recall_m: 0.8967 - precision_m: 0.9025 - precision: 0.9025 - recall_6: 0.8967 - f1score: 0.8996 - val_loss: 0.5493 - val_acc: 0.8138 - val_prec: 0.1759 - val_recall: 0.1717 - val_prec_1: 0.1753 - val_recall_1: 0.1612 - val_prec_2: 0.1919 - val_recall_2: 0.1771 - val_prec_3: 0.1759 - val_recall_3: 0.1744 - val_prec_4: 0.1599 - val_recall_4: 0.0337 - val_prec_5: 0.1684 - val_recall_5: 0.1565 - val_f1_m: 0.8116 - val_recall_m: 0.8076 - val_precision_m: 0.8158 - val_precision: 0.8158 - val_recall_6: 0.8076 - val_f1score: 0.8116\n",
      "Epoch 155/300\n",
      "epoch:  154\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2168 - acc: 0.8985 - prec: 0.9890 - recall: 0.9921 - prec_1: 0.9693 - recall_1: 0.9852 - prec_2: 0.9885 - recall_2: 0.9803 - prec_3: 0.7270 - recall_3: 0.7496 - prec_4: 0.7318 - recall_4: 0.7229 - prec_5: 0.9862 - recall_5: 0.9871 - f1_m: 0.8980 - recall_m: 0.8950 - precision_m: 0.9011 - precision: 0.9011 - recall_6: 0.8950 - f1score: 0.8980 - val_loss: 0.3246 - val_acc: 0.8356 - val_prec: 0.1759 - val_recall: 0.1702 - val_prec_1: 0.1753 - val_recall_1: 0.1633 - val_prec_2: 0.1919 - val_recall_2: 0.1738 - val_prec_3: 0.1599 - val_recall_3: 0.0542 - val_prec_4: 0.1822 - val_recall_4: 0.1792 - val_prec_5: 0.1684 - val_recall_5: 0.1607 - val_f1_m: 0.8363 - val_recall_m: 0.8331 - val_precision_m: 0.8396 - val_precision: 0.8396 - val_recall_6: 0.8331 - val_f1score: 0.8363\n",
      "Epoch 156/300\n",
      "epoch:  155\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2134 - acc: 0.9034 - prec: 0.9864 - recall: 0.9920 - prec_1: 0.9813 - recall_1: 0.9804 - prec_2: 0.9849 - recall_2: 0.9824 - prec_3: 0.7400 - recall_3: 0.7483 - prec_4: 0.7419 - recall_4: 0.7454 - prec_5: 0.9841 - recall_5: 0.9916 - f1_m: 0.9024 - recall_m: 0.8993 - precision_m: 0.9055 - precision: 0.9055 - recall_6: 0.8993 - f1score: 0.9024 - val_loss: 0.3541 - val_acc: 0.8481 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1751 - val_recall_1: 0.1605 - val_prec_2: 0.1919 - val_recall_2: 0.1753 - val_prec_3: 0.1759 - val_recall_3: 0.1548 - val_prec_4: 0.1713 - val_recall_4: 0.0855 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8479 - val_recall_m: 0.8446 - val_precision_m: 0.8514 - val_precision: 0.8514 - val_recall_6: 0.8446 - val_f1score: 0.8479\n",
      "Epoch 157/300\n",
      "epoch:  156\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2127 - acc: 0.9015 - prec: 0.9815 - recall: 0.9930 - prec_1: 0.9839 - recall_1: 0.9861 - prec_2: 0.9876 - recall_2: 0.9830 - prec_3: 0.7310 - recall_3: 0.7452 - prec_4: 0.7337 - recall_4: 0.7247 - prec_5: 0.9883 - recall_5: 0.9889 - f1_m: 0.9007 - recall_m: 0.8978 - precision_m: 0.9036 - precision: 0.9036 - recall_6: 0.8978 - f1score: 0.9007 - val_loss: 0.3625 - val_acc: 0.8511 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1753 - val_recall_1: 0.1610 - val_prec_2: 0.1919 - val_recall_2: 0.1784 - val_prec_3: 0.1759 - val_recall_3: 0.1443 - val_prec_4: 0.1752 - val_recall_4: 0.0977 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.8489 - val_recall_m: 0.8441 - val_precision_m: 0.8540 - val_precision: 0.8540 - val_recall_6: 0.8441 - val_f1score: 0.8489\n",
      "Epoch 158/300\n",
      "epoch:  157\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2177 - acc: 0.8998 - prec: 0.9832 - recall: 0.9922 - prec_1: 0.9737 - recall_1: 0.9803 - prec_2: 0.9801 - recall_2: 0.9850 - prec_3: 0.7306 - recall_3: 0.7470 - prec_4: 0.7396 - recall_4: 0.7293 - prec_5: 0.9862 - recall_5: 0.9839 - f1_m: 0.9001 - recall_m: 0.8962 - precision_m: 0.9041 - precision: 0.9041 - recall_6: 0.8962 - f1score: 0.9001 - val_loss: 0.3037 - val_acc: 0.8578 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1561 - val_prec_2: 0.1901 - val_recall_2: 0.1830 - val_prec_3: 0.1759 - val_recall_3: 0.1298 - val_prec_4: 0.1919 - val_recall_4: 0.1342 - val_prec_5: 0.1684 - val_recall_5: 0.1597 - val_f1_m: 0.8573 - val_recall_m: 0.8546 - val_precision_m: 0.8601 - val_precision: 0.8601 - val_recall_6: 0.8546 - val_f1score: 0.8573\n",
      "Epoch 159/300\n",
      "epoch:  158\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2166 - acc: 0.9001 - prec: 0.9840 - recall: 0.9925 - prec_1: 0.9666 - recall_1: 0.9816 - prec_2: 0.9897 - recall_2: 0.9817 - prec_3: 0.7256 - recall_3: 0.7305 - prec_4: 0.7308 - recall_4: 0.7341 - prec_5: 0.9904 - recall_5: 0.9889 - f1_m: 0.9002 - recall_m: 0.8968 - precision_m: 0.9037 - precision: 0.9037 - recall_6: 0.8968 - f1score: 0.9002 - val_loss: 0.4198 - val_acc: 0.8353 - val_prec: 0.1759 - val_recall: 0.1717 - val_prec_1: 0.1751 - val_recall_1: 0.1624 - val_prec_2: 0.1919 - val_recall_2: 0.1730 - val_prec_3: 0.1759 - val_recall_3: 0.1385 - val_prec_4: 0.1732 - val_recall_4: 0.0915 - val_prec_5: 0.1684 - val_recall_5: 0.1572 - val_f1_m: 0.8364 - val_recall_m: 0.8233 - val_precision_m: 0.8538 - val_precision: 0.8538 - val_recall_6: 0.8233 - val_f1score: 0.8364\n",
      "Epoch 160/300\n",
      "epoch:  159\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2131 - acc: 0.9002 - prec: 0.9919 - recall: 0.9932 - prec_1: 0.9763 - recall_1: 0.9834 - prec_2: 0.9858 - recall_2: 0.9837 - prec_3: 0.7279 - recall_3: 0.7452 - prec_4: 0.7387 - recall_4: 0.7218 - prec_5: 0.9869 - recall_5: 0.9921 - f1_m: 0.8998 - recall_m: 0.8967 - precision_m: 0.9029 - precision: 0.9029 - recall_6: 0.8967 - f1score: 0.8998 - val_loss: 0.2964 - val_acc: 0.8626 - val_prec: 0.1759 - val_recall: 0.1694 - val_prec_1: 0.1753 - val_recall_1: 0.1596 - val_prec_2: 0.1919 - val_recall_2: 0.1755 - val_prec_3: 0.1759 - val_recall_3: 0.1168 - val_prec_4: 0.1911 - val_recall_4: 0.1522 - val_prec_5: 0.1684 - val_recall_5: 0.1618 - val_f1_m: 0.8637 - val_recall_m: 0.8608 - val_precision_m: 0.8667 - val_precision: 0.8667 - val_recall_6: 0.8608 - val_f1score: 0.8637\n",
      "Epoch 161/300\n",
      "epoch:  160\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2213 - acc: 0.8991 - prec: 0.9852 - recall: 0.9905 - prec_1: 0.9777 - recall_1: 0.9824 - prec_2: 0.9826 - recall_2: 0.9806 - prec_3: 0.7264 - recall_3: 0.7377 - prec_4: 0.7320 - recall_4: 0.7278 - prec_5: 0.9849 - recall_5: 0.9911 - f1_m: 0.8985 - recall_m: 0.8955 - precision_m: 0.9017 - precision: 0.9017 - recall_6: 0.8955 - f1score: 0.8985 - val_loss: 1.4519 - val_acc: 0.8231 - val_prec: 0.1759 - val_recall: 0.1681 - val_prec_1: 0.1753 - val_recall_1: 0.1576 - val_prec_2: 0.1899 - val_recall_2: 0.1794 - val_prec_3: 0.1439 - val_recall_3: 0.0487 - val_prec_4: 0.1809 - val_recall_4: 0.1804 - val_prec_5: 0.1684 - val_recall_5: 0.1582 - val_f1_m: 0.8235 - val_recall_m: 0.8198 - val_precision_m: 0.8272 - val_precision: 0.8272 - val_recall_6: 0.8198 - val_f1score: 0.8235\n",
      "Epoch 162/300\n",
      "epoch:  161\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2161 - acc: 0.8996 - prec: 0.9891 - recall: 0.9934 - prec_1: 0.9757 - recall_1: 0.9840 - prec_2: 0.9897 - recall_2: 0.9856 - prec_3: 0.7257 - recall_3: 0.7381 - prec_4: 0.7315 - recall_4: 0.7319 - prec_5: 0.9885 - recall_5: 0.9902 - f1_m: 0.8995 - recall_m: 0.8963 - precision_m: 0.9027 - precision: 0.9027 - recall_6: 0.8963 - f1score: 0.8995 - val_loss: 0.3395 - val_acc: 0.8453 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1558 - val_prec_2: 0.1901 - val_recall_2: 0.1825 - val_prec_3: 0.1599 - val_recall_3: 0.0616 - val_prec_4: 0.1824 - val_recall_4: 0.1834 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.8464 - val_recall_m: 0.8433 - val_precision_m: 0.8496 - val_precision: 0.8496 - val_recall_6: 0.8433 - val_f1score: 0.8464\n",
      "Epoch 163/300\n",
      "epoch:  162\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2163 - acc: 0.9030 - prec: 0.9889 - recall: 0.9934 - prec_1: 0.9780 - recall_1: 0.9874 - prec_2: 0.9915 - recall_2: 0.9846 - prec_3: 0.7361 - recall_3: 0.7418 - prec_4: 0.7404 - recall_4: 0.7466 - prec_5: 0.9910 - recall_5: 0.9902 - f1_m: 0.9022 - recall_m: 0.8991 - precision_m: 0.9054 - precision: 0.9054 - recall_6: 0.8991 - f1score: 0.9022 - val_loss: 0.2704 - val_acc: 0.8871 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1632 - val_prec_2: 0.1919 - val_recall_2: 0.1802 - val_prec_3: 0.1759 - val_recall_3: 0.1482 - val_prec_4: 0.1919 - val_recall_4: 0.1414 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8867 - val_recall_m: 0.8838 - val_precision_m: 0.8896 - val_precision: 0.8896 - val_recall_6: 0.8838 - val_f1score: 0.8867\n",
      "Epoch 164/300\n",
      "epoch:  163\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2153 - acc: 0.8971 - prec: 0.9852 - recall: 0.9895 - prec_1: 0.9735 - recall_1: 0.9841 - prec_2: 0.9883 - recall_2: 0.9832 - prec_3: 0.7248 - recall_3: 0.7397 - prec_4: 0.7261 - recall_4: 0.7145 - prec_5: 0.9842 - recall_5: 0.9889 - f1_m: 0.8968 - recall_m: 0.8938 - precision_m: 0.8998 - precision: 0.8998 - recall_6: 0.8938 - f1score: 0.8968 - val_loss: 0.4692 - val_acc: 0.8418 - val_prec: 0.1759 - val_recall: 0.1700 - val_prec_1: 0.1751 - val_recall_1: 0.1641 - val_prec_2: 0.1919 - val_recall_2: 0.1725 - val_prec_3: 0.1759 - val_recall_3: 0.0570 - val_prec_4: 0.1811 - val_recall_4: 0.1829 - val_prec_5: 0.1684 - val_recall_5: 0.1600 - val_f1_m: 0.8432 - val_recall_m: 0.8401 - val_precision_m: 0.8464 - val_precision: 0.8464 - val_recall_6: 0.8401 - val_f1score: 0.8432\n",
      "Epoch 165/300\n",
      "epoch:  164\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2156 - acc: 0.9010 - prec: 0.9782 - recall: 0.9938 - prec_1: 0.9745 - recall_1: 0.9840 - prec_2: 0.9853 - recall_2: 0.9835 - prec_3: 0.7334 - recall_3: 0.7358 - prec_4: 0.7335 - recall_4: 0.7374 - prec_5: 0.9914 - recall_5: 0.9850 - f1_m: 0.9012 - recall_m: 0.8983 - precision_m: 0.9041 - precision: 0.9041 - recall_6: 0.8983 - f1score: 0.9012 - val_loss: 0.6954 - val_acc: 0.8063 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1751 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1789 - val_prec_3: 0.1740 - val_recall_3: 0.1624 - val_prec_4: 0.1599 - val_recall_4: 0.0379 - val_prec_5: 0.1684 - val_recall_5: 0.1549 - val_f1_m: 0.8048 - val_recall_m: 0.8013 - val_precision_m: 0.8085 - val_precision: 0.8085 - val_recall_6: 0.8013 - val_f1score: 0.8048\n",
      "Epoch 166/300\n",
      "epoch:  165\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2150 - acc: 0.9001 - prec: 0.9854 - recall: 0.9912 - prec_1: 0.9754 - recall_1: 0.9847 - prec_2: 0.9851 - recall_2: 0.9803 - prec_3: 0.7360 - recall_3: 0.7308 - prec_4: 0.7337 - recall_4: 0.7460 - prec_5: 0.9890 - recall_5: 0.9898 - f1_m: 0.9002 - recall_m: 0.8971 - precision_m: 0.9034 - precision: 0.9034 - recall_6: 0.8971 - f1score: 0.9002 - val_loss: 0.2948 - val_acc: 0.8623 - val_prec: 0.1759 - val_recall: 0.1697 - val_prec_1: 0.1756 - val_recall_1: 0.1624 - val_prec_2: 0.1919 - val_recall_2: 0.1787 - val_prec_3: 0.1759 - val_recall_3: 0.0907 - val_prec_4: 0.1812 - val_recall_4: 0.1727 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.8620 - val_recall_m: 0.8588 - val_precision_m: 0.8652 - val_precision: 0.8652 - val_recall_6: 0.8588 - val_f1score: 0.8620\n",
      "Epoch 167/300\n",
      "epoch:  166\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2102 - acc: 0.9056 - prec: 0.9864 - recall: 0.9922 - prec_1: 0.9806 - recall_1: 0.9850 - prec_2: 0.9865 - recall_2: 0.9841 - prec_3: 0.7454 - recall_3: 0.7492 - prec_4: 0.7419 - recall_4: 0.7449 - prec_5: 0.9915 - recall_5: 0.9944 - f1_m: 0.9045 - recall_m: 0.9014 - precision_m: 0.9078 - precision: 0.9078 - recall_6: 0.9014 - f1score: 0.9045 - val_loss: 0.2881 - val_acc: 0.8753 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1612 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1513 - val_prec_4: 0.1898 - val_recall_4: 0.1180 - val_prec_5: 0.1684 - val_recall_5: 0.1580 - val_f1_m: 0.8746 - val_recall_m: 0.8708 - val_precision_m: 0.8786 - val_precision: 0.8786 - val_recall_6: 0.8708 - val_f1score: 0.8746\n",
      "Epoch 168/300\n",
      "epoch:  167\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2122 - acc: 0.9034 - prec: 0.9880 - recall: 0.9894 - prec_1: 0.9734 - recall_1: 0.9858 - prec_2: 0.9876 - recall_2: 0.9821 - prec_3: 0.7517 - recall_3: 0.7464 - prec_4: 0.7423 - recall_4: 0.7516 - prec_5: 0.9869 - recall_5: 0.9891 - f1_m: 0.9035 - recall_m: 0.9014 - precision_m: 0.9058 - precision: 0.9058 - recall_6: 0.9014 - f1score: 0.9035 - val_loss: 0.4892 - val_acc: 0.8208 - val_prec: 0.1759 - val_recall: 0.1692 - val_prec_1: 0.1756 - val_recall_1: 0.1551 - val_prec_2: 0.1901 - val_recall_2: 0.1830 - val_prec_3: 0.1279 - val_recall_3: 0.0310 - val_prec_4: 0.1809 - val_recall_4: 0.1889 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8210 - val_recall_m: 0.8178 - val_precision_m: 0.8243 - val_precision: 0.8243 - val_recall_6: 0.8178 - val_f1score: 0.8210\n",
      "Epoch 169/300\n",
      "epoch:  168\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2173 - acc: 0.8963 - prec: 0.9880 - recall: 0.9910 - prec_1: 0.9755 - recall_1: 0.9760 - prec_2: 0.9803 - recall_2: 0.9830 - prec_3: 0.7372 - recall_3: 0.7123 - prec_4: 0.7206 - recall_4: 0.7502 - prec_5: 0.9822 - recall_5: 0.9848 - f1_m: 0.8961 - recall_m: 0.8932 - precision_m: 0.8989 - precision: 0.8989 - recall_6: 0.8932 - f1score: 0.8961 - val_loss: 0.3606 - val_acc: 0.8458 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1756 - val_recall_1: 0.1587 - val_prec_2: 0.1901 - val_recall_2: 0.1805 - val_prec_3: 0.1759 - val_recall_3: 0.1638 - val_prec_4: 0.1752 - val_recall_4: 0.0737 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.8435 - val_recall_m: 0.8398 - val_precision_m: 0.8474 - val_precision: 0.8474 - val_recall_6: 0.8398 - val_f1score: 0.8435\n",
      "Epoch 170/300\n",
      "epoch:  169\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2129 - acc: 0.9054 - prec: 0.9901 - recall: 0.9932 - prec_1: 0.9755 - recall_1: 0.9847 - prec_2: 0.9847 - recall_2: 0.9841 - prec_3: 0.7372 - recall_3: 0.7613 - prec_4: 0.7570 - recall_4: 0.7342 - prec_5: 0.9928 - recall_5: 0.9930 - f1_m: 0.9044 - recall_m: 0.9012 - precision_m: 0.9076 - precision: 0.9076 - recall_6: 0.9012 - f1score: 0.9044 - val_loss: 0.3626 - val_acc: 0.8531 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1751 - val_recall_1: 0.1582 - val_prec_2: 0.1919 - val_recall_2: 0.1760 - val_prec_3: 0.1759 - val_recall_3: 0.1059 - val_prec_4: 0.1812 - val_recall_4: 0.1512 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8522 - val_recall_m: 0.8483 - val_precision_m: 0.8562 - val_precision: 0.8562 - val_recall_6: 0.8483 - val_f1score: 0.8522\n",
      "Epoch 171/300\n",
      "epoch:  170\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2201 - acc: 0.9010 - prec: 0.9837 - recall: 0.9908 - prec_1: 0.9753 - recall_1: 0.9845 - prec_2: 0.9869 - recall_2: 0.9791 - prec_3: 0.7368 - recall_3: 0.7268 - prec_4: 0.7303 - recall_4: 0.7500 - prec_5: 0.9773 - recall_5: 0.9861 - f1_m: 0.9011 - recall_m: 0.8986 - precision_m: 0.9036 - precision: 0.9036 - recall_6: 0.8986 - f1score: 0.9011 - val_loss: 0.2936 - val_acc: 0.8643 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1580 - val_prec_2: 0.1919 - val_recall_2: 0.1820 - val_prec_3: 0.1759 - val_recall_3: 0.1559 - val_prec_4: 0.1919 - val_recall_4: 0.1081 - val_prec_5: 0.1684 - val_recall_5: 0.1613 - val_f1_m: 0.8641 - val_recall_m: 0.8603 - val_precision_m: 0.8681 - val_precision: 0.8681 - val_recall_6: 0.8603 - val_f1score: 0.8641\n",
      "Epoch 172/300\n",
      "epoch:  171\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2138 - acc: 0.9032 - prec: 0.9913 - recall: 0.9912 - prec_1: 0.9712 - recall_1: 0.9861 - prec_2: 0.9904 - recall_2: 0.9829 - prec_3: 0.7292 - recall_3: 0.7467 - prec_4: 0.7444 - recall_4: 0.7267 - prec_5: 0.9854 - recall_5: 0.9877 - f1_m: 0.9023 - recall_m: 0.8992 - precision_m: 0.9055 - precision: 0.9055 - recall_6: 0.8992 - f1score: 0.9023 - val_loss: 0.4983 - val_acc: 0.8508 - val_prec: 0.1759 - val_recall: 0.1695 - val_prec_1: 0.1756 - val_recall_1: 0.1593 - val_prec_2: 0.1901 - val_recall_2: 0.1817 - val_prec_3: 0.1759 - val_recall_3: 0.0660 - val_prec_4: 0.1810 - val_recall_4: 0.1849 - val_prec_5: 0.1684 - val_recall_5: 0.1597 - val_f1_m: 0.8513 - val_recall_m: 0.8483 - val_precision_m: 0.8543 - val_precision: 0.8543 - val_recall_6: 0.8483 - val_f1score: 0.8513\n",
      "Epoch 173/300\n",
      "epoch:  172\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2122 - acc: 0.9044 - prec: 0.9887 - recall: 0.9915 - prec_1: 0.9796 - recall_1: 0.9799 - prec_2: 0.9843 - recall_2: 0.9839 - prec_3: 0.7462 - recall_3: 0.7391 - prec_4: 0.7412 - recall_4: 0.7512 - prec_5: 0.9853 - recall_5: 0.9903 - f1_m: 0.9041 - recall_m: 0.9011 - precision_m: 0.9072 - precision: 0.9072 - recall_6: 0.9011 - f1score: 0.9041 - val_loss: 0.5650 - val_acc: 0.8368 - val_prec: 0.1759 - val_recall: 0.1694 - val_prec_1: 0.1751 - val_recall_1: 0.1606 - val_prec_2: 0.1896 - val_recall_2: 0.1768 - val_prec_3: 0.1599 - val_recall_3: 0.0675 - val_prec_4: 0.1809 - val_recall_4: 0.1714 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.8372 - val_recall_m: 0.8341 - val_precision_m: 0.8404 - val_precision: 0.8404 - val_recall_6: 0.8341 - val_f1score: 0.8372\n",
      "Epoch 174/300\n",
      "epoch:  173\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2156 - acc: 0.9050 - prec: 0.9884 - recall: 0.9932 - prec_1: 0.9802 - recall_1: 0.9851 - prec_2: 0.9838 - recall_2: 0.9861 - prec_3: 0.7437 - recall_3: 0.7514 - prec_4: 0.7440 - recall_4: 0.7428 - prec_5: 0.9877 - recall_5: 0.9904 - f1_m: 0.9035 - recall_m: 0.9000 - precision_m: 0.9072 - precision: 0.9072 - recall_6: 0.9000 - f1score: 0.9035 - val_loss: 0.2786 - val_acc: 0.8703 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1613 - val_prec_2: 0.1919 - val_recall_2: 0.1800 - val_prec_3: 0.1759 - val_recall_3: 0.1043 - val_prec_4: 0.1837 - val_recall_4: 0.1674 - val_prec_5: 0.1684 - val_recall_5: 0.1600 - val_f1_m: 0.8698 - val_recall_m: 0.8673 - val_precision_m: 0.8724 - val_precision: 0.8724 - val_recall_6: 0.8673 - val_f1score: 0.8698\n",
      "Epoch 175/300\n",
      "epoch:  174\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2150 - acc: 0.9034 - prec: 0.9891 - recall: 0.9934 - prec_1: 0.9739 - recall_1: 0.9832 - prec_2: 0.9897 - recall_2: 0.9788 - prec_3: 0.7471 - recall_3: 0.7424 - prec_4: 0.7379 - recall_4: 0.7531 - prec_5: 0.9856 - recall_5: 0.9883 - f1_m: 0.9031 - recall_m: 0.9002 - precision_m: 0.9061 - precision: 0.9061 - recall_6: 0.9002 - f1score: 0.9031 - val_loss: 1.1595 - val_acc: 0.7664 - val_prec: 0.1747 - val_recall: 0.1720 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1919 - val_recall_2: 0.1712 - val_prec_3: 0.1709 - val_recall_3: 0.1472 - val_prec_4: 0.0960 - val_recall_4: 0.0115 - val_prec_5: 0.1684 - val_recall_5: 0.1522 - val_f1_m: 0.7669 - val_recall_m: 0.7624 - val_precision_m: 0.7716 - val_precision: 0.7716 - val_recall_6: 0.7624 - val_f1score: 0.7669\n",
      "Epoch 176/300\n",
      "epoch:  175\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 687us/step - loss: 0.2166 - acc: 0.8987 - prec: 0.9874 - recall: 0.9934 - prec_1: 0.9739 - recall_1: 0.9807 - prec_2: 0.9809 - recall_2: 0.9852 - prec_3: 0.7264 - recall_3: 0.7377 - prec_4: 0.7325 - recall_4: 0.7296 - prec_5: 0.9869 - recall_5: 0.9856 - f1_m: 0.8985 - recall_m: 0.8955 - precision_m: 0.9016 - precision: 0.9016 - recall_6: 0.8955 - f1score: 0.8985 - val_loss: 0.3940 - val_acc: 0.8421 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1569 - val_prec_2: 0.1901 - val_recall_2: 0.1832 - val_prec_3: 0.1759 - val_recall_3: 0.1496 - val_prec_4: 0.1752 - val_recall_4: 0.0865 - val_prec_5: 0.1684 - val_recall_5: 0.1570 - val_f1_m: 0.8411 - val_recall_m: 0.8326 - val_precision_m: 0.8509 - val_precision: 0.8509 - val_recall_6: 0.8326 - val_f1score: 0.8411\n",
      "Epoch 177/300\n",
      "epoch:  176\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2156 - acc: 0.9012 - prec: 0.9868 - recall: 0.9928 - prec_1: 0.9768 - recall_1: 0.9818 - prec_2: 0.9807 - recall_2: 0.9840 - prec_3: 0.7437 - recall_3: 0.7290 - prec_4: 0.7247 - recall_4: 0.7454 - prec_5: 0.9874 - recall_5: 0.9860 - f1_m: 0.9007 - recall_m: 0.8973 - precision_m: 0.9040 - precision: 0.9040 - recall_6: 0.8973 - f1score: 0.9007 - val_loss: 0.6338 - val_acc: 0.8376 - val_prec: 0.1759 - val_recall: 0.1709 - val_prec_1: 0.1748 - val_recall_1: 0.1567 - val_prec_2: 0.1892 - val_recall_2: 0.1712 - val_prec_3: 0.1599 - val_recall_3: 0.0905 - val_prec_4: 0.1809 - val_recall_4: 0.1549 - val_prec_5: 0.1684 - val_recall_5: 0.1599 - val_f1_m: 0.8355 - val_recall_m: 0.8308 - val_precision_m: 0.8403 - val_precision: 0.8403 - val_recall_6: 0.8308 - val_f1score: 0.8355\n",
      "Epoch 178/300\n",
      "epoch:  177\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2152 - acc: 0.9055 - prec: 0.9848 - recall: 0.9916 - prec_1: 0.9790 - recall_1: 0.9914 - prec_2: 0.9858 - recall_2: 0.9844 - prec_3: 0.7380 - recall_3: 0.7577 - prec_4: 0.7490 - recall_4: 0.7353 - prec_5: 0.9895 - recall_5: 0.9880 - f1_m: 0.9046 - recall_m: 0.9011 - precision_m: 0.9082 - precision: 0.9082 - recall_6: 0.9011 - f1score: 0.9046 - val_loss: 0.8472 - val_acc: 0.8293 - val_prec: 0.1759 - val_recall: 0.1701 - val_prec_1: 0.1748 - val_recall_1: 0.1615 - val_prec_2: 0.1919 - val_recall_2: 0.1729 - val_prec_3: 0.1439 - val_recall_3: 0.0584 - val_prec_4: 0.1809 - val_recall_4: 0.1727 - val_prec_5: 0.1684 - val_recall_5: 0.1603 - val_f1_m: 0.8283 - val_recall_m: 0.8248 - val_precision_m: 0.8318 - val_precision: 0.8318 - val_recall_6: 0.8248 - val_f1score: 0.8283\n",
      "Epoch 179/300\n",
      "epoch:  178\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2133 - acc: 0.9046 - prec: 0.9831 - recall: 0.9934 - prec_1: 0.9755 - recall_1: 0.9789 - prec_2: 0.9776 - recall_2: 0.9806 - prec_3: 0.7544 - recall_3: 0.7546 - prec_4: 0.7497 - recall_4: 0.7614 - prec_5: 0.9803 - recall_5: 0.9862 - f1_m: 0.9043 - recall_m: 0.9005 - precision_m: 0.9082 - precision: 0.9082 - recall_6: 0.9005 - f1score: 0.9043 - val_loss: 0.3093 - val_acc: 0.8633 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1756 - val_recall_1: 0.1580 - val_prec_2: 0.1919 - val_recall_2: 0.1827 - val_prec_3: 0.1759 - val_recall_3: 0.1399 - val_prec_4: 0.1919 - val_recall_4: 0.1294 - val_prec_5: 0.1684 - val_recall_5: 0.1580 - val_f1_m: 0.8620 - val_recall_m: 0.8568 - val_precision_m: 0.8673 - val_precision: 0.8673 - val_recall_6: 0.8568 - val_f1score: 0.8620\n",
      "Epoch 180/300\n",
      "epoch:  179\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2124 - acc: 0.9001 - prec: 0.9868 - recall: 0.9928 - prec_1: 0.9822 - recall_1: 0.9841 - prec_2: 0.9865 - recall_2: 0.9807 - prec_3: 0.7273 - recall_3: 0.7445 - prec_4: 0.7418 - recall_4: 0.7248 - prec_5: 0.9867 - recall_5: 0.9907 - f1_m: 0.9002 - recall_m: 0.8970 - precision_m: 0.9035 - precision: 0.9035 - recall_6: 0.8970 - f1score: 0.9002 - val_loss: 0.4166 - val_acc: 0.8193 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1534 - val_prec_2: 0.1901 - val_recall_2: 0.1840 - val_prec_3: 0.1599 - val_recall_3: 0.0779 - val_prec_4: 0.1664 - val_recall_4: 0.1302 - val_prec_5: 0.1684 - val_recall_5: 0.1572 - val_f1_m: 0.8193 - val_recall_m: 0.8151 - val_precision_m: 0.8237 - val_precision: 0.8237 - val_recall_6: 0.8151 - val_f1score: 0.8193\n",
      "Epoch 181/300\n",
      "epoch:  180\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2127 - acc: 0.9024 - prec: 0.9879 - recall: 0.9934 - prec_1: 0.9780 - recall_1: 0.9801 - prec_2: 0.9864 - recall_2: 0.9857 - prec_3: 0.7321 - recall_3: 0.7367 - prec_4: 0.7296 - recall_4: 0.7346 - prec_5: 0.9825 - recall_5: 0.9885 - f1_m: 0.9022 - recall_m: 0.8991 - precision_m: 0.9053 - precision: 0.9053 - recall_6: 0.8991 - f1score: 0.9022 - val_loss: 0.3859 - val_acc: 0.8423 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1753 - val_recall_1: 0.1640 - val_prec_2: 0.1919 - val_recall_2: 0.1774 - val_prec_3: 0.1759 - val_recall_3: 0.0677 - val_prec_4: 0.1843 - val_recall_4: 0.1737 - val_prec_5: 0.1684 - val_recall_5: 0.1580 - val_f1_m: 0.8427 - val_recall_m: 0.8396 - val_precision_m: 0.8459 - val_precision: 0.8459 - val_recall_6: 0.8396 - val_f1score: 0.8427\n",
      "Epoch 182/300\n",
      "epoch:  181\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2128 - acc: 0.9032 - prec: 0.9898 - recall: 0.9923 - prec_1: 0.9709 - recall_1: 0.9860 - prec_2: 0.9867 - recall_2: 0.9833 - prec_3: 0.7366 - recall_3: 0.7457 - prec_4: 0.7412 - recall_4: 0.7354 - prec_5: 0.9887 - recall_5: 0.9903 - f1_m: 0.9032 - recall_m: 0.9000 - precision_m: 0.9065 - precision: 0.9065 - recall_6: 0.9000 - f1score: 0.9032 - val_loss: 0.4621 - val_acc: 0.8128 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1753 - val_recall_1: 0.1576 - val_prec_2: 0.1901 - val_recall_2: 0.1820 - val_prec_3: 0.1599 - val_recall_3: 0.0666 - val_prec_4: 0.1663 - val_recall_4: 0.1347 - val_prec_5: 0.1684 - val_recall_5: 0.1551 - val_f1_m: 0.8132 - val_recall_m: 0.8081 - val_precision_m: 0.8186 - val_precision: 0.8186 - val_recall_6: 0.8081 - val_f1score: 0.8132\n",
      "Epoch 183/300\n",
      "epoch:  182\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 657us/step - loss: 0.2160 - acc: 0.8990 - prec: 0.9817 - recall: 0.9902 - prec_1: 0.9730 - recall_1: 0.9835 - prec_2: 0.9830 - recall_2: 0.9835 - prec_3: 0.7264 - recall_3: 0.7352 - prec_4: 0.7318 - recall_4: 0.7270 - prec_5: 0.9900 - recall_5: 0.9909 - f1_m: 0.8992 - recall_m: 0.8962 - precision_m: 0.9023 - precision: 0.9023 - recall_6: 0.8962 - f1score: 0.8992 - val_loss: 0.6692 - val_acc: 0.8461 - val_prec: 0.1759 - val_recall: 0.1709 - val_prec_1: 0.1748 - val_recall_1: 0.1618 - val_prec_2: 0.1919 - val_recall_2: 0.1704 - val_prec_3: 0.1759 - val_recall_3: 0.0946 - val_prec_4: 0.1810 - val_recall_4: 0.1539 - val_prec_5: 0.1684 - val_recall_5: 0.1613 - val_f1_m: 0.8446 - val_recall_m: 0.8423 - val_precision_m: 0.8469 - val_precision: 0.8469 - val_recall_6: 0.8423 - val_f1score: 0.8446\n",
      "Epoch 184/300\n",
      "epoch:  183\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2166 - acc: 0.8983 - prec: 0.9838 - recall: 0.9923 - prec_1: 0.9780 - recall_1: 0.9822 - prec_2: 0.9836 - recall_2: 0.9834 - prec_3: 0.7266 - recall_3: 0.7424 - prec_4: 0.7324 - recall_4: 0.7239 - prec_5: 0.9882 - recall_5: 0.9915 - f1_m: 0.8987 - recall_m: 0.8958 - precision_m: 0.9016 - precision: 0.9016 - recall_6: 0.8958 - f1score: 0.8987 - val_loss: 0.3060 - val_acc: 0.8676 - val_prec: 0.1759 - val_recall: 0.1699 - val_prec_1: 0.1756 - val_recall_1: 0.1508 - val_prec_2: 0.1887 - val_recall_2: 0.1832 - val_prec_3: 0.1759 - val_recall_3: 0.1318 - val_prec_4: 0.1911 - val_recall_4: 0.1457 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8677 - val_recall_m: 0.8648 - val_precision_m: 0.8707 - val_precision: 0.8707 - val_recall_6: 0.8648 - val_f1score: 0.8677\n",
      "Epoch 185/300\n",
      "epoch:  184\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2088 - acc: 0.9054 - prec: 0.9818 - recall: 0.9911 - prec_1: 0.9808 - recall_1: 0.9834 - prec_2: 0.9831 - recall_2: 0.9854 - prec_3: 0.7444 - recall_3: 0.7361 - prec_4: 0.7451 - recall_4: 0.7597 - prec_5: 0.9863 - recall_5: 0.9898 - f1_m: 0.9057 - recall_m: 0.9027 - precision_m: 0.9086 - precision: 0.9086 - recall_6: 0.9027 - f1score: 0.9057 - val_loss: 0.3136 - val_acc: 0.8556 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1586 - val_prec_2: 0.1901 - val_recall_2: 0.1830 - val_prec_3: 0.1759 - val_recall_3: 0.0711 - val_prec_4: 0.1812 - val_recall_4: 0.1807 - val_prec_5: 0.1684 - val_recall_5: 0.1590 - val_f1_m: 0.8554 - val_recall_m: 0.8516 - val_precision_m: 0.8593 - val_precision: 0.8593 - val_recall_6: 0.8516 - val_f1score: 0.8554\n",
      "Epoch 186/300\n",
      "epoch:  185\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2153 - acc: 0.9009 - prec: 0.9815 - recall: 0.9931 - prec_1: 0.9668 - recall_1: 0.9831 - prec_2: 0.9888 - recall_2: 0.9756 - prec_3: 0.7460 - recall_3: 0.7331 - prec_4: 0.7406 - recall_4: 0.7561 - prec_5: 0.9857 - recall_5: 0.9879 - f1_m: 0.9013 - recall_m: 0.8980 - precision_m: 0.9047 - precision: 0.9047 - recall_6: 0.8980 - f1score: 0.9013 - val_loss: 0.3714 - val_acc: 0.8483 - val_prec: 0.1759 - val_recall: 0.1709 - val_prec_1: 0.1750 - val_recall_1: 0.1567 - val_prec_2: 0.1896 - val_recall_2: 0.1760 - val_prec_3: 0.1759 - val_recall_3: 0.0943 - val_prec_4: 0.1810 - val_recall_4: 0.1572 - val_prec_5: 0.1684 - val_recall_5: 0.1613 - val_f1_m: 0.8481 - val_recall_m: 0.8431 - val_precision_m: 0.8534 - val_precision: 0.8534 - val_recall_6: 0.8431 - val_f1score: 0.8481\n",
      "Epoch 187/300\n",
      "epoch:  186\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2161 - acc: 0.8988 - prec: 0.9853 - recall: 0.9921 - prec_1: 0.9726 - recall_1: 0.9825 - prec_2: 0.9836 - recall_2: 0.9845 - prec_3: 0.7271 - recall_3: 0.7322 - prec_4: 0.7306 - recall_4: 0.7338 - prec_5: 0.9866 - recall_5: 0.9860 - f1_m: 0.8992 - recall_m: 0.8953 - precision_m: 0.9031 - precision: 0.9031 - recall_6: 0.8953 - f1score: 0.8992 - val_loss: 0.3324 - val_acc: 0.8623 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1756 - val_recall_1: 0.1624 - val_prec_2: 0.1919 - val_recall_2: 0.1820 - val_prec_3: 0.1759 - val_recall_3: 0.1143 - val_prec_4: 0.1677 - val_recall_4: 0.1339 - val_prec_5: 0.1684 - val_recall_5: 0.1580 - val_f1_m: 0.8568 - val_recall_m: 0.8446 - val_precision_m: 0.8714 - val_precision: 0.8714 - val_recall_6: 0.8446 - val_f1score: 0.8568\n",
      "Epoch 188/300\n",
      "epoch:  187\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2119 - acc: 0.9057 - prec: 0.9929 - recall: 0.9922 - prec_1: 0.9767 - recall_1: 0.9822 - prec_2: 0.9833 - recall_2: 0.9837 - prec_3: 0.7457 - recall_3: 0.7556 - prec_4: 0.7508 - recall_4: 0.7459 - prec_5: 0.9860 - recall_5: 0.9921 - f1_m: 0.9047 - recall_m: 0.9014 - precision_m: 0.9081 - precision: 0.9081 - recall_6: 0.9014 - f1score: 0.9047 - val_loss: 0.6161 - val_acc: 0.7889 - val_prec: 0.1759 - val_recall: 0.1702 - val_prec_1: 0.1756 - val_recall_1: 0.1647 - val_prec_2: 0.1919 - val_recall_2: 0.1792 - val_prec_3: 0.0960 - val_recall_3: 0.0072 - val_prec_4: 0.1809 - val_recall_4: 0.1652 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.7878 - val_recall_m: 0.7846 - val_precision_m: 0.7911 - val_precision: 0.7911 - val_recall_6: 0.7846 - val_f1score: 0.7878\n",
      "Epoch 189/300\n",
      "epoch:  188\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2106 - acc: 0.9045 - prec: 0.9871 - recall: 0.9912 - prec_1: 0.9737 - recall_1: 0.9855 - prec_2: 0.9859 - recall_2: 0.9816 - prec_3: 0.7406 - recall_3: 0.7560 - prec_4: 0.7509 - recall_4: 0.7439 - prec_5: 0.9855 - recall_5: 0.9892 - f1_m: 0.9053 - recall_m: 0.9024 - precision_m: 0.9083 - precision: 0.9083 - recall_6: 0.9024 - f1score: 0.9053 - val_loss: 0.3405 - val_acc: 0.8516 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1756 - val_recall_1: 0.1563 - val_prec_2: 0.1901 - val_recall_2: 0.1832 - val_prec_3: 0.1759 - val_recall_3: 0.0686 - val_prec_4: 0.1830 - val_recall_4: 0.1819 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8511 - val_recall_m: 0.8463 - val_precision_m: 0.8560 - val_precision: 0.8560 - val_recall_6: 0.8463 - val_f1score: 0.8511\n",
      "Epoch 190/300\n",
      "epoch:  189\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2119 - acc: 0.9035 - prec: 0.9813 - recall: 0.9908 - prec_1: 0.9752 - recall_1: 0.9819 - prec_2: 0.9813 - recall_2: 0.9809 - prec_3: 0.7353 - recall_3: 0.7642 - prec_4: 0.7547 - recall_4: 0.7293 - prec_5: 0.9895 - recall_5: 0.9896 - f1_m: 0.9033 - recall_m: 0.9007 - precision_m: 0.9059 - precision: 0.9059 - recall_6: 0.9007 - f1score: 0.9033 - val_loss: 0.2984 - val_acc: 0.8596 - val_prec: 0.1759 - val_recall: 0.1701 - val_prec_1: 0.1756 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1789 - val_prec_3: 0.1759 - val_recall_3: 0.0950 - val_prec_4: 0.1829 - val_recall_4: 0.1677 - val_prec_5: 0.1684 - val_recall_5: 0.1606 - val_f1_m: 0.8597 - val_recall_m: 0.8571 - val_precision_m: 0.8625 - val_precision: 0.8625 - val_recall_6: 0.8571 - val_f1score: 0.8597\n",
      "Epoch 191/300\n",
      "epoch:  190\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2126 - acc: 0.9010 - prec: 0.9862 - recall: 0.9894 - prec_1: 0.9758 - recall_1: 0.9851 - prec_2: 0.9904 - recall_2: 0.9769 - prec_3: 0.7322 - recall_3: 0.7315 - prec_4: 0.7268 - recall_4: 0.7369 - prec_5: 0.9825 - recall_5: 0.9938 - f1_m: 0.9011 - recall_m: 0.8981 - precision_m: 0.9041 - precision: 0.9041 - recall_6: 0.8981 - f1score: 0.9011 - val_loss: 0.3036 - val_acc: 0.8593 - val_prec: 0.1759 - val_recall: 0.1694 - val_prec_1: 0.1756 - val_recall_1: 0.1531 - val_prec_2: 0.1901 - val_recall_2: 0.1837 - val_prec_3: 0.1759 - val_recall_3: 0.1101 - val_prec_4: 0.1837 - val_recall_4: 0.1582 - val_prec_5: 0.1684 - val_recall_5: 0.1585 - val_f1_m: 0.8592 - val_recall_m: 0.8548 - val_precision_m: 0.8637 - val_precision: 0.8637 - val_recall_6: 0.8548 - val_f1score: 0.8592\n",
      "Epoch 192/300\n",
      "epoch:  191\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2147 - acc: 0.9021 - prec: 0.9887 - recall: 0.9946 - prec_1: 0.9834 - recall_1: 0.9843 - prec_2: 0.9863 - recall_2: 0.9847 - prec_3: 0.7211 - recall_3: 0.7490 - prec_4: 0.7463 - recall_4: 0.7242 - prec_5: 0.9872 - recall_5: 0.9917 - f1_m: 0.9018 - recall_m: 0.8993 - precision_m: 0.9042 - precision: 0.9042 - recall_6: 0.8993 - f1score: 0.9018 - val_loss: 0.3091 - val_acc: 0.8538 - val_prec: 0.1759 - val_recall: 0.1697 - val_prec_1: 0.1753 - val_recall_1: 0.1630 - val_prec_2: 0.1919 - val_recall_2: 0.1769 - val_prec_3: 0.1759 - val_recall_3: 0.0896 - val_prec_4: 0.1859 - val_recall_4: 0.1647 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.8528 - val_recall_m: 0.8501 - val_precision_m: 0.8557 - val_precision: 0.8557 - val_recall_6: 0.8501 - val_f1score: 0.8528\n",
      "Epoch 193/300\n",
      "epoch:  192\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.2137 - acc: 0.9020 - prec: 0.9864 - recall: 0.9925 - prec_1: 0.9773 - recall_1: 0.9832 - prec_2: 0.9858 - recall_2: 0.9840 - prec_3: 0.7376 - recall_3: 0.7436 - prec_4: 0.7396 - recall_4: 0.7397 - prec_5: 0.9877 - recall_5: 0.9912 - f1_m: 0.9021 - recall_m: 0.8992 - precision_m: 0.9050 - precision: 0.9050 - recall_6: 0.8992 - f1score: 0.9021 - val_loss: 0.2853 - val_acc: 0.8733 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1753 - val_recall_1: 0.1610 - val_prec_2: 0.1919 - val_recall_2: 0.1791 - val_prec_3: 0.1759 - val_recall_3: 0.1221 - val_prec_4: 0.1822 - val_recall_4: 0.1423 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.8731 - val_recall_m: 0.8701 - val_precision_m: 0.8762 - val_precision: 0.8762 - val_recall_6: 0.8701 - val_f1score: 0.8731\n",
      "Epoch 194/300\n",
      "epoch:  193\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2131 - acc: 0.9077 - prec: 0.9881 - recall: 0.9906 - prec_1: 0.9800 - recall_1: 0.9841 - prec_2: 0.9790 - recall_2: 0.9848 - prec_3: 0.7546 - recall_3: 0.7495 - prec_4: 0.7542 - recall_4: 0.7669 - prec_5: 0.9894 - recall_5: 0.9893 - f1_m: 0.9077 - recall_m: 0.9046 - precision_m: 0.9108 - precision: 0.9108 - recall_6: 0.9046 - f1score: 0.9077 - val_loss: 0.2941 - val_acc: 0.8763 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1753 - val_recall_1: 0.1610 - val_prec_2: 0.1919 - val_recall_2: 0.1789 - val_prec_3: 0.1759 - val_recall_3: 0.1568 - val_prec_4: 0.1911 - val_recall_4: 0.1227 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8761 - val_recall_m: 0.8733 - val_precision_m: 0.8790 - val_precision: 0.8790 - val_recall_6: 0.8733 - val_f1score: 0.8761\n",
      "Epoch 195/300\n",
      "epoch:  194\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 658us/step - loss: 0.2141 - acc: 0.9051 - prec: 0.9854 - recall: 0.9930 - prec_1: 0.9777 - recall_1: 0.9819 - prec_2: 0.9813 - recall_2: 0.9820 - prec_3: 0.7425 - recall_3: 0.7468 - prec_4: 0.7459 - recall_4: 0.7531 - prec_5: 0.9886 - recall_5: 0.9901 - f1_m: 0.9055 - recall_m: 0.9029 - precision_m: 0.9082 - precision: 0.9082 - recall_6: 0.9029 - f1score: 0.9055 - val_loss: 0.3548 - val_acc: 0.8486 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1627 - val_prec_2: 0.1919 - val_recall_2: 0.1805 - val_prec_3: 0.1599 - val_recall_3: 0.0790 - val_prec_4: 0.1809 - val_recall_4: 0.1679 - val_prec_5: 0.1684 - val_recall_5: 0.1590 - val_f1_m: 0.8462 - val_recall_m: 0.8426 - val_precision_m: 0.8499 - val_precision: 0.8499 - val_recall_6: 0.8426 - val_f1score: 0.8462\n",
      "Epoch 196/300\n",
      "epoch:  195\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2115 - acc: 0.9069 - prec: 0.9869 - recall: 0.9937 - prec_1: 0.9850 - recall_1: 0.9901 - prec_2: 0.9925 - recall_2: 0.9881 - prec_3: 0.7385 - recall_3: 0.7457 - prec_4: 0.7393 - recall_4: 0.7416 - prec_5: 0.9917 - recall_5: 0.9926 - f1_m: 0.9067 - recall_m: 0.9032 - precision_m: 0.9103 - precision: 0.9103 - recall_6: 0.9032 - f1score: 0.9067 - val_loss: 0.2858 - val_acc: 0.8693 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1751 - val_recall_1: 0.1607 - val_prec_2: 0.1919 - val_recall_2: 0.1765 - val_prec_3: 0.1759 - val_recall_3: 0.1238 - val_prec_4: 0.1826 - val_recall_4: 0.1370 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8686 - val_recall_m: 0.8653 - val_precision_m: 0.8720 - val_precision: 0.8720 - val_recall_6: 0.8653 - val_f1score: 0.8686\n",
      "Epoch 197/300\n",
      "epoch:  196\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2136 - acc: 0.9029 - prec: 0.9862 - recall: 0.9931 - prec_1: 0.9765 - recall_1: 0.9875 - prec_2: 0.9914 - recall_2: 0.9808 - prec_3: 0.7406 - recall_3: 0.7316 - prec_4: 0.7339 - recall_4: 0.7472 - prec_5: 0.9909 - recall_5: 0.9912 - f1_m: 0.9014 - recall_m: 0.8983 - precision_m: 0.9045 - precision: 0.9045 - recall_6: 0.8983 - f1score: 0.9014 - val_loss: 0.2788 - val_acc: 0.8731 - val_prec: 0.1759 - val_recall: 0.1701 - val_prec_1: 0.1756 - val_recall_1: 0.1562 - val_prec_2: 0.1901 - val_recall_2: 0.1822 - val_prec_3: 0.1759 - val_recall_3: 0.1215 - val_prec_4: 0.1882 - val_recall_4: 0.1567 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8730 - val_recall_m: 0.8696 - val_precision_m: 0.8766 - val_precision: 0.8766 - val_recall_6: 0.8696 - val_f1score: 0.8730\n",
      "Epoch 198/300\n",
      "epoch:  197\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2068 - acc: 0.9096 - prec: 0.9846 - recall: 0.9935 - prec_1: 0.9796 - recall_1: 0.9862 - prec_2: 0.9862 - recall_2: 0.9832 - prec_3: 0.7612 - recall_3: 0.7599 - prec_4: 0.7637 - recall_4: 0.7668 - prec_5: 0.9902 - recall_5: 0.9893 - f1_m: 0.9096 - recall_m: 0.9067 - precision_m: 0.9126 - precision: 0.9126 - recall_6: 0.9067 - f1score: 0.9096 - val_loss: 0.3296 - val_acc: 0.8556 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1587 - val_prec_2: 0.1919 - val_recall_2: 0.1827 - val_prec_3: 0.1759 - val_recall_3: 0.0959 - val_prec_4: 0.1650 - val_recall_4: 0.1422 - val_prec_5: 0.1684 - val_recall_5: 0.1590 - val_f1_m: 0.8536 - val_recall_m: 0.8501 - val_precision_m: 0.8572 - val_precision: 0.8572 - val_recall_6: 0.8501 - val_f1score: 0.8536\n",
      "Epoch 199/300\n",
      "epoch:  198\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2088 - acc: 0.9051 - prec: 0.9877 - recall: 0.9902 - prec_1: 0.9808 - recall_1: 0.9818 - prec_2: 0.9818 - recall_2: 0.9849 - prec_3: 0.7486 - recall_3: 0.7512 - prec_4: 0.7498 - recall_4: 0.7541 - prec_5: 0.9837 - recall_5: 0.9896 - f1_m: 0.9047 - recall_m: 0.9017 - precision_m: 0.9078 - precision: 0.9078 - recall_6: 0.9017 - f1score: 0.9047 - val_loss: 0.3482 - val_acc: 0.8331 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1751 - val_recall_1: 0.1614 - val_prec_2: 0.1919 - val_recall_2: 0.1684 - val_prec_3: 0.1759 - val_recall_3: 0.1672 - val_prec_4: 0.1919 - val_recall_4: 0.0721 - val_prec_5: 0.1684 - val_recall_5: 0.1613 - val_f1_m: 0.8318 - val_recall_m: 0.8283 - val_precision_m: 0.8354 - val_precision: 0.8354 - val_recall_6: 0.8283 - val_f1score: 0.8318\n",
      "Epoch 200/300\n",
      "epoch:  199\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2085 - acc: 0.9066 - prec: 0.9874 - recall: 0.9911 - prec_1: 0.9814 - recall_1: 0.9853 - prec_2: 0.9917 - recall_2: 0.9863 - prec_3: 0.7446 - recall_3: 0.7565 - prec_4: 0.7490 - recall_4: 0.7492 - prec_5: 0.9883 - recall_5: 0.9940 - f1_m: 0.9055 - recall_m: 0.9022 - precision_m: 0.9089 - precision: 0.9089 - recall_6: 0.9022 - f1score: 0.9055 - val_loss: 0.2899 - val_acc: 0.8746 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1547 - val_prec_2: 0.1919 - val_recall_2: 0.1837 - val_prec_3: 0.1759 - val_recall_3: 0.1032 - val_prec_4: 0.1841 - val_recall_4: 0.1759 - val_prec_5: 0.1684 - val_recall_5: 0.1600 - val_f1_m: 0.8726 - val_recall_m: 0.8688 - val_precision_m: 0.8765 - val_precision: 0.8765 - val_recall_6: 0.8688 - val_f1score: 0.8726\n",
      "Epoch 201/300\n",
      "epoch:  200\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2105 - acc: 0.9049 - prec: 0.9908 - recall: 0.9937 - prec_1: 0.9732 - recall_1: 0.9879 - prec_2: 0.9845 - recall_2: 0.9807 - prec_3: 0.7407 - recall_3: 0.7503 - prec_4: 0.7464 - recall_4: 0.7424 - prec_5: 0.9889 - recall_5: 0.9898 - f1_m: 0.9043 - recall_m: 0.9014 - precision_m: 0.9072 - precision: 0.9072 - recall_6: 0.9014 - f1score: 0.9043 - val_loss: 0.3748 - val_acc: 0.8426 - val_prec: 0.1759 - val_recall: 0.1686 - val_prec_1: 0.1756 - val_recall_1: 0.1524 - val_prec_2: 0.1901 - val_recall_2: 0.1840 - val_prec_3: 0.1759 - val_recall_3: 0.0855 - val_prec_4: 0.1892 - val_recall_4: 0.1629 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8423 - val_recall_m: 0.8386 - val_precision_m: 0.8461 - val_precision: 0.8461 - val_recall_6: 0.8386 - val_f1score: 0.8423\n",
      "Epoch 202/300\n",
      "epoch:  201\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2121 - acc: 0.9032 - prec: 0.9849 - recall: 0.9920 - prec_1: 0.9764 - recall_1: 0.9867 - prec_2: 0.9866 - recall_2: 0.9855 - prec_3: 0.7334 - recall_3: 0.7454 - prec_4: 0.7358 - recall_4: 0.7313 - prec_5: 0.9915 - recall_5: 0.9918 - f1_m: 0.9031 - recall_m: 0.9007 - precision_m: 0.9056 - precision: 0.9056 - recall_6: 0.9007 - f1score: 0.9031 - val_loss: 0.2855 - val_acc: 0.8761 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1609 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1366 - val_prec_4: 0.1898 - val_recall_4: 0.1376 - val_prec_5: 0.1684 - val_recall_5: 0.1590 - val_f1_m: 0.8758 - val_recall_m: 0.8726 - val_precision_m: 0.8790 - val_precision: 0.8790 - val_recall_6: 0.8726 - val_f1score: 0.8758\n",
      "Epoch 203/300\n",
      "epoch:  202\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2062 - acc: 0.9057 - prec: 0.9884 - recall: 0.9914 - prec_1: 0.9778 - recall_1: 0.9873 - prec_2: 0.9880 - recall_2: 0.9852 - prec_3: 0.7461 - recall_3: 0.7423 - prec_4: 0.7451 - recall_4: 0.7549 - prec_5: 0.9911 - recall_5: 0.9900 - f1_m: 0.9060 - recall_m: 0.9035 - precision_m: 0.9085 - precision: 0.9085 - recall_6: 0.9035 - f1score: 0.9060 - val_loss: 0.2808 - val_acc: 0.8801 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1753 - val_recall_1: 0.1596 - val_prec_2: 0.1919 - val_recall_2: 0.1766 - val_prec_3: 0.1759 - val_recall_3: 0.1472 - val_prec_4: 0.1919 - val_recall_4: 0.1397 - val_prec_5: 0.1684 - val_recall_5: 0.1610 - val_f1_m: 0.8793 - val_recall_m: 0.8768 - val_precision_m: 0.8819 - val_precision: 0.8819 - val_recall_6: 0.8768 - val_f1score: 0.8793\n",
      "Epoch 204/300\n",
      "epoch:  203\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2100 - acc: 0.9029 - prec: 0.9881 - recall: 0.9923 - prec_1: 0.9757 - recall_1: 0.9865 - prec_2: 0.9874 - recall_2: 0.9821 - prec_3: 0.7324 - recall_3: 0.7443 - prec_4: 0.7435 - recall_4: 0.7317 - prec_5: 0.9851 - recall_5: 0.9900 - f1_m: 0.9027 - recall_m: 0.8996 - precision_m: 0.9058 - precision: 0.9058 - recall_6: 0.8996 - f1score: 0.9027 - val_loss: 0.3835 - val_acc: 0.8486 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1751 - val_recall_1: 0.1612 - val_prec_2: 0.1919 - val_recall_2: 0.1763 - val_prec_3: 0.1759 - val_recall_3: 0.1407 - val_prec_4: 0.1672 - val_recall_4: 0.0985 - val_prec_5: 0.1684 - val_recall_5: 0.1571 - val_f1_m: 0.8479 - val_recall_m: 0.8446 - val_precision_m: 0.8514 - val_precision: 0.8514 - val_recall_6: 0.8446 - val_f1score: 0.8479\n",
      "Epoch 205/300\n",
      "epoch:  204\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2083 - acc: 0.9079 - prec: 0.9847 - recall: 0.9922 - prec_1: 0.9783 - recall_1: 0.9802 - prec_2: 0.9857 - recall_2: 0.9837 - prec_3: 0.7633 - recall_3: 0.7503 - prec_4: 0.7533 - recall_4: 0.7677 - prec_5: 0.9790 - recall_5: 0.9918 - f1_m: 0.9078 - recall_m: 0.9047 - precision_m: 0.9109 - precision: 0.9109 - recall_6: 0.9047 - f1score: 0.9078 - val_loss: 0.3128 - val_acc: 0.8541 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1555 - val_prec_2: 0.1901 - val_recall_2: 0.1837 - val_prec_3: 0.1759 - val_recall_3: 0.0816 - val_prec_4: 0.1816 - val_recall_4: 0.1744 - val_prec_5: 0.1684 - val_recall_5: 0.1597 - val_f1_m: 0.8537 - val_recall_m: 0.8508 - val_precision_m: 0.8566 - val_precision: 0.8566 - val_recall_6: 0.8508 - val_f1score: 0.8537\n",
      "Epoch 206/300\n",
      "epoch:  205\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2097 - acc: 0.9035 - prec: 0.9828 - recall: 0.9888 - prec_1: 0.9731 - recall_1: 0.9899 - prec_2: 0.9901 - recall_2: 0.9824 - prec_3: 0.7434 - recall_3: 0.7370 - prec_4: 0.7363 - recall_4: 0.7497 - prec_5: 0.9914 - recall_5: 0.9879 - f1_m: 0.9030 - recall_m: 0.8998 - precision_m: 0.9062 - precision: 0.9062 - recall_6: 0.8998 - f1score: 0.9030 - val_loss: 0.2860 - val_acc: 0.8681 - val_prec: 0.1759 - val_recall: 0.1706 - val_prec_1: 0.1756 - val_recall_1: 0.1541 - val_prec_2: 0.1901 - val_recall_2: 0.1832 - val_prec_3: 0.1759 - val_recall_3: 0.1037 - val_prec_4: 0.1829 - val_recall_4: 0.1684 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8679 - val_recall_m: 0.8646 - val_precision_m: 0.8712 - val_precision: 0.8712 - val_recall_6: 0.8646 - val_f1score: 0.8679\n",
      "Epoch 207/300\n",
      "epoch:  206\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2126 - acc: 0.9055 - prec: 0.9891 - recall: 0.9921 - prec_1: 0.9782 - recall_1: 0.9860 - prec_2: 0.9855 - recall_2: 0.9852 - prec_3: 0.7435 - recall_3: 0.7456 - prec_4: 0.7432 - recall_4: 0.7532 - prec_5: 0.9859 - recall_5: 0.9900 - f1_m: 0.9047 - recall_m: 0.9017 - precision_m: 0.9077 - precision: 0.9077 - recall_6: 0.9017 - f1score: 0.9047 - val_loss: 0.2982 - val_acc: 0.8686 - val_prec: 0.1759 - val_recall: 0.1686 - val_prec_1: 0.1756 - val_recall_1: 0.1588 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1083 - val_prec_4: 0.1853 - val_recall_4: 0.1596 - val_prec_5: 0.1684 - val_recall_5: 0.1618 - val_f1_m: 0.8682 - val_recall_m: 0.8651 - val_precision_m: 0.8715 - val_precision: 0.8715 - val_recall_6: 0.8651 - val_f1score: 0.8682\n",
      "Epoch 208/300\n",
      "epoch:  207\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2112 - acc: 0.9034 - prec: 0.9889 - recall: 0.9904 - prec_1: 0.9772 - recall_1: 0.9845 - prec_2: 0.9918 - recall_2: 0.9841 - prec_3: 0.7387 - recall_3: 0.7462 - prec_4: 0.7412 - recall_4: 0.7438 - prec_5: 0.9837 - recall_5: 0.9898 - f1_m: 0.9035 - recall_m: 0.9002 - precision_m: 0.9068 - precision: 0.9068 - recall_6: 0.9002 - f1score: 0.9035 - val_loss: 0.4295 - val_acc: 0.8368 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1753 - val_recall_1: 0.1602 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1713 - val_recall_3: 0.1262 - val_prec_4: 0.1653 - val_recall_4: 0.0978 - val_prec_5: 0.1684 - val_recall_5: 0.1552 - val_f1_m: 0.8371 - val_recall_m: 0.8323 - val_precision_m: 0.8421 - val_precision: 0.8421 - val_recall_6: 0.8323 - val_f1score: 0.8371\n",
      "Epoch 209/300\n",
      "epoch:  208\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2040 - acc: 0.9070 - prec: 0.9886 - recall: 0.9924 - prec_1: 0.9793 - recall_1: 0.9881 - prec_2: 0.9886 - recall_2: 0.9822 - prec_3: 0.7465 - recall_3: 0.7351 - prec_4: 0.7398 - recall_4: 0.7638 - prec_5: 0.9887 - recall_5: 0.9921 - f1_m: 0.9065 - recall_m: 0.9039 - precision_m: 0.9093 - precision: 0.9093 - recall_6: 0.9039 - f1score: 0.9065 - val_loss: 0.3053 - val_acc: 0.8571 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1753 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1786 - val_prec_3: 0.1759 - val_recall_3: 0.1311 - val_prec_4: 0.1674 - val_recall_4: 0.1147 - val_prec_5: 0.1684 - val_recall_5: 0.1600 - val_f1_m: 0.8566 - val_recall_m: 0.8538 - val_precision_m: 0.8595 - val_precision: 0.8595 - val_recall_6: 0.8538 - val_f1score: 0.8566\n",
      "Epoch 210/300\n",
      "epoch:  209\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2109 - acc: 0.9044 - prec: 0.9893 - recall: 0.9896 - prec_1: 0.9749 - recall_1: 0.9818 - prec_2: 0.9898 - recall_2: 0.9792 - prec_3: 0.7410 - recall_3: 0.7495 - prec_4: 0.7409 - recall_4: 0.7373 - prec_5: 0.9831 - recall_5: 0.9918 - f1_m: 0.9042 - recall_m: 0.9010 - precision_m: 0.9074 - precision: 0.9074 - recall_6: 0.9010 - f1score: 0.9042 - val_loss: 0.9129 - val_acc: 0.7886 - val_prec: 0.1759 - val_recall: 0.1702 - val_prec_1: 0.1756 - val_recall_1: 0.1556 - val_prec_2: 0.1901 - val_recall_2: 0.1840 - val_prec_3: 0.1119 - val_recall_3: 0.0095 - val_prec_4: 0.1809 - val_recall_4: 0.1723 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.7877 - val_recall_m: 0.7836 - val_precision_m: 0.7919 - val_precision: 0.7919 - val_recall_6: 0.7836 - val_f1score: 0.7877\n",
      "Epoch 211/300\n",
      "epoch:  210\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2075 - acc: 0.9090 - prec: 0.9893 - recall: 0.9910 - prec_1: 0.9798 - recall_1: 0.9883 - prec_2: 0.9864 - recall_2: 0.9857 - prec_3: 0.7436 - recall_3: 0.7608 - prec_4: 0.7536 - recall_4: 0.7435 - prec_5: 0.9915 - recall_5: 0.9923 - f1_m: 0.9088 - recall_m: 0.9061 - precision_m: 0.9115 - precision: 0.9115 - recall_6: 0.9061 - f1score: 0.9088 - val_loss: 0.4172 - val_acc: 0.8386 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1756 - val_recall_1: 0.1591 - val_prec_2: 0.1901 - val_recall_2: 0.1817 - val_prec_3: 0.1759 - val_recall_3: 0.1030 - val_prec_4: 0.1656 - val_recall_4: 0.1259 - val_prec_5: 0.1684 - val_recall_5: 0.1561 - val_f1_m: 0.8377 - val_recall_m: 0.8343 - val_precision_m: 0.8412 - val_precision: 0.8412 - val_recall_6: 0.8343 - val_f1score: 0.8377\n",
      "Epoch 212/300\n",
      "epoch:  211\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2092 - acc: 0.9025 - prec: 0.9872 - recall: 0.9924 - prec_1: 0.9801 - recall_1: 0.9838 - prec_2: 0.9870 - recall_2: 0.9872 - prec_3: 0.7324 - recall_3: 0.7445 - prec_4: 0.7391 - recall_4: 0.7344 - prec_5: 0.9892 - recall_5: 0.9884 - f1_m: 0.9027 - recall_m: 0.9000 - precision_m: 0.9055 - precision: 0.9055 - recall_6: 0.9000 - f1score: 0.9027 - val_loss: 0.4441 - val_acc: 0.8578 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1756 - val_recall_1: 0.1599 - val_prec_2: 0.1901 - val_recall_2: 0.1817 - val_prec_3: 0.1599 - val_recall_3: 0.0828 - val_prec_4: 0.1839 - val_recall_4: 0.1774 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8573 - val_recall_m: 0.8531 - val_precision_m: 0.8617 - val_precision: 0.8617 - val_recall_6: 0.8531 - val_f1score: 0.8573\n",
      "Epoch 213/300\n",
      "epoch:  212\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2146 - acc: 0.9029 - prec: 0.9859 - recall: 0.9916 - prec_1: 0.9817 - recall_1: 0.9859 - prec_2: 0.9845 - recall_2: 0.9859 - prec_3: 0.7501 - recall_3: 0.7300 - prec_4: 0.7315 - recall_4: 0.7522 - prec_5: 0.9831 - recall_5: 0.9885 - f1_m: 0.9027 - recall_m: 0.8990 - precision_m: 0.9065 - precision: 0.9065 - recall_6: 0.8990 - f1score: 0.9027 - val_loss: 0.3508 - val_acc: 0.8221 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1748 - val_recall_1: 0.1659 - val_prec_2: 0.1919 - val_recall_2: 0.1702 - val_prec_3: 0.1439 - val_recall_3: 0.0638 - val_prec_4: 0.1812 - val_recall_4: 0.1462 - val_prec_5: 0.1684 - val_recall_5: 0.1584 - val_f1_m: 0.8221 - val_recall_m: 0.8193 - val_precision_m: 0.8249 - val_precision: 0.8249 - val_recall_6: 0.8193 - val_f1score: 0.8221\n",
      "Epoch 214/300\n",
      "epoch:  213\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2125 - acc: 0.9042 - prec: 0.9902 - recall: 0.9914 - prec_1: 0.9780 - recall_1: 0.9825 - prec_2: 0.9837 - recall_2: 0.9816 - prec_3: 0.7436 - recall_3: 0.7624 - prec_4: 0.7482 - recall_4: 0.7347 - prec_5: 0.9862 - recall_5: 0.9917 - f1_m: 0.9048 - recall_m: 0.9022 - precision_m: 0.9075 - precision: 0.9075 - recall_6: 0.9022 - f1score: 0.9048 - val_loss: 0.3192 - val_acc: 0.8666 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1756 - val_recall_1: 0.1602 - val_prec_2: 0.1919 - val_recall_2: 0.1825 - val_prec_3: 0.1759 - val_recall_3: 0.1411 - val_prec_4: 0.1752 - val_recall_4: 0.1154 - val_prec_5: 0.1684 - val_recall_5: 0.1580 - val_f1_m: 0.8653 - val_recall_m: 0.8611 - val_precision_m: 0.8697 - val_precision: 0.8697 - val_recall_6: 0.8611 - val_f1score: 0.8653\n",
      "Epoch 215/300\n",
      "epoch:  214\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2062 - acc: 0.9052 - prec: 0.9881 - recall: 0.9938 - prec_1: 0.9812 - recall_1: 0.9872 - prec_2: 0.9849 - recall_2: 0.9830 - prec_3: 0.7377 - recall_3: 0.7511 - prec_4: 0.7480 - recall_4: 0.7373 - prec_5: 0.9895 - recall_5: 0.9893 - f1_m: 0.9052 - recall_m: 0.9024 - precision_m: 0.9080 - precision: 0.9080 - recall_6: 0.9024 - f1score: 0.9052 - val_loss: 0.7731 - val_acc: 0.8391 - val_prec: 0.1759 - val_recall: 0.1706 - val_prec_1: 0.1751 - val_recall_1: 0.1578 - val_prec_2: 0.1892 - val_recall_2: 0.1719 - val_prec_3: 0.1599 - val_recall_3: 0.0867 - val_prec_4: 0.1809 - val_recall_4: 0.1569 - val_prec_5: 0.1684 - val_recall_5: 0.1618 - val_f1_m: 0.8383 - val_recall_m: 0.8351 - val_precision_m: 0.8416 - val_precision: 0.8416 - val_recall_6: 0.8351 - val_f1score: 0.8383\n",
      "Epoch 216/300\n",
      "epoch:  215\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2121 - acc: 0.9017 - prec: 0.9858 - recall: 0.9928 - prec_1: 0.9797 - recall_1: 0.9820 - prec_2: 0.9865 - recall_2: 0.9798 - prec_3: 0.7320 - recall_3: 0.7544 - prec_4: 0.7434 - recall_4: 0.7278 - prec_5: 0.9797 - recall_5: 0.9901 - f1_m: 0.9026 - recall_m: 0.8995 - precision_m: 0.9057 - precision: 0.9057 - recall_6: 0.8995 - f1score: 0.9026 - val_loss: 0.3363 - val_acc: 0.8478 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1751 - val_recall_1: 0.1618 - val_prec_2: 0.1919 - val_recall_2: 0.1694 - val_prec_3: 0.1759 - val_recall_3: 0.1537 - val_prec_4: 0.1919 - val_recall_4: 0.0925 - val_prec_5: 0.1684 - val_recall_5: 0.1613 - val_f1_m: 0.8472 - val_recall_m: 0.8453 - val_precision_m: 0.8490 - val_precision: 0.8490 - val_recall_6: 0.8453 - val_f1score: 0.8472\n",
      "Epoch 217/300\n",
      "epoch:  216\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2063 - acc: 0.9104 - prec: 0.9918 - recall: 0.9923 - prec_1: 0.9801 - recall_1: 0.9869 - prec_2: 0.9859 - recall_2: 0.9836 - prec_3: 0.7572 - recall_3: 0.7497 - prec_4: 0.7539 - recall_4: 0.7717 - prec_5: 0.9915 - recall_5: 0.9925 - f1_m: 0.9094 - recall_m: 0.9071 - precision_m: 0.9117 - precision: 0.9117 - recall_6: 0.9071 - f1score: 0.9094 - val_loss: 0.2860 - val_acc: 0.8761 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1564 - val_prec_2: 0.1901 - val_recall_2: 0.1822 - val_prec_3: 0.1759 - val_recall_3: 0.1084 - val_prec_4: 0.1824 - val_recall_4: 0.1668 - val_prec_5: 0.1684 - val_recall_5: 0.1597 - val_f1_m: 0.8761 - val_recall_m: 0.8726 - val_precision_m: 0.8797 - val_precision: 0.8797 - val_recall_6: 0.8726 - val_f1score: 0.8761\n",
      "Epoch 218/300\n",
      "epoch:  217\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2077 - acc: 0.9052 - prec: 0.9898 - recall: 0.9942 - prec_1: 0.9823 - recall_1: 0.9905 - prec_2: 0.9905 - recall_2: 0.9846 - prec_3: 0.7440 - recall_3: 0.7384 - prec_4: 0.7370 - recall_4: 0.7440 - prec_5: 0.9877 - recall_5: 0.9928 - f1_m: 0.9046 - recall_m: 0.9017 - precision_m: 0.9075 - precision: 0.9075 - recall_6: 0.9017 - f1score: 0.9046 - val_loss: 0.2823 - val_acc: 0.8713 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1751 - val_recall_1: 0.1621 - val_prec_2: 0.1919 - val_recall_2: 0.1730 - val_prec_3: 0.1759 - val_recall_3: 0.1337 - val_prec_4: 0.1919 - val_recall_4: 0.1439 - val_prec_5: 0.1684 - val_recall_5: 0.1610 - val_f1_m: 0.8712 - val_recall_m: 0.8688 - val_precision_m: 0.8737 - val_precision: 0.8737 - val_recall_6: 0.8688 - val_f1score: 0.8712\n",
      "Epoch 219/300\n",
      "epoch:  218\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2125 - acc: 0.9047 - prec: 0.9898 - recall: 0.9918 - prec_1: 0.9696 - recall_1: 0.9836 - prec_2: 0.9840 - recall_2: 0.9818 - prec_3: 0.7449 - recall_3: 0.7495 - prec_4: 0.7553 - recall_4: 0.7535 - prec_5: 0.9873 - recall_5: 0.9899 - f1_m: 0.9051 - recall_m: 0.9015 - precision_m: 0.9088 - precision: 0.9088 - recall_6: 0.9015 - f1score: 0.9051 - val_loss: 0.3037 - val_acc: 0.8661 - val_prec: 0.1759 - val_recall: 0.1715 - val_prec_1: 0.1756 - val_recall_1: 0.1594 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.0994 - val_prec_4: 0.1853 - val_recall_4: 0.1694 - val_prec_5: 0.1684 - val_recall_5: 0.1600 - val_f1_m: 0.8655 - val_recall_m: 0.8613 - val_precision_m: 0.8699 - val_precision: 0.8699 - val_recall_6: 0.8613 - val_f1score: 0.8655\n",
      "Epoch 220/300\n",
      "epoch:  219\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2071 - acc: 0.9079 - prec: 0.9896 - recall: 0.9906 - prec_1: 0.9771 - recall_1: 0.9853 - prec_2: 0.9865 - recall_2: 0.9842 - prec_3: 0.7514 - recall_3: 0.7589 - prec_4: 0.7601 - recall_4: 0.7546 - prec_5: 0.9844 - recall_5: 0.9901 - f1_m: 0.9071 - recall_m: 0.9039 - precision_m: 0.9105 - precision: 0.9105 - recall_6: 0.9039 - f1score: 0.9071 - val_loss: 0.4709 - val_acc: 0.8451 - val_prec: 0.1759 - val_recall: 0.1717 - val_prec_1: 0.1751 - val_recall_1: 0.1589 - val_prec_2: 0.1919 - val_recall_2: 0.1758 - val_prec_3: 0.1759 - val_recall_3: 0.1624 - val_prec_4: 0.1727 - val_recall_4: 0.0770 - val_prec_5: 0.1684 - val_recall_5: 0.1581 - val_f1_m: 0.8441 - val_recall_m: 0.8408 - val_precision_m: 0.8475 - val_precision: 0.8475 - val_recall_6: 0.8408 - val_f1score: 0.8441\n",
      "Epoch 221/300\n",
      "epoch:  220\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2111 - acc: 0.9030 - prec: 0.9861 - recall: 0.9911 - prec_1: 0.9795 - recall_1: 0.9844 - prec_2: 0.9856 - recall_2: 0.9830 - prec_3: 0.7392 - recall_3: 0.7406 - prec_4: 0.7460 - recall_4: 0.7482 - prec_5: 0.9838 - recall_5: 0.9932 - f1_m: 0.9030 - recall_m: 0.9000 - precision_m: 0.9062 - precision: 0.9062 - recall_6: 0.9000 - f1score: 0.9030 - val_loss: 0.4465 - val_acc: 0.8338 - val_prec: 0.1759 - val_recall: 0.1692 - val_prec_1: 0.1756 - val_recall_1: 0.1584 - val_prec_2: 0.1919 - val_recall_2: 0.1797 - val_prec_3: 0.1759 - val_recall_3: 0.1532 - val_prec_4: 0.1919 - val_recall_4: 0.0882 - val_prec_5: 0.1684 - val_recall_5: 0.1610 - val_f1_m: 0.8321 - val_recall_m: 0.8286 - val_precision_m: 0.8358 - val_precision: 0.8358 - val_recall_6: 0.8286 - val_f1score: 0.8321\n",
      "Epoch 222/300\n",
      "epoch:  221\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2077 - acc: 0.9069 - prec: 0.9894 - recall: 0.9943 - prec_1: 0.9790 - recall_1: 0.9856 - prec_2: 0.9888 - recall_2: 0.9852 - prec_3: 0.7494 - recall_3: 0.7401 - prec_4: 0.7449 - recall_4: 0.7619 - prec_5: 0.9879 - recall_5: 0.9883 - f1_m: 0.9065 - recall_m: 0.9032 - precision_m: 0.9099 - precision: 0.9099 - recall_6: 0.9032 - f1score: 0.9065 - val_loss: 0.3030 - val_acc: 0.8681 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1566 - val_prec_2: 0.1901 - val_recall_2: 0.1820 - val_prec_3: 0.1759 - val_recall_3: 0.0901 - val_prec_4: 0.1823 - val_recall_4: 0.1779 - val_prec_5: 0.1684 - val_recall_5: 0.1598 - val_f1_m: 0.8680 - val_recall_m: 0.8646 - val_precision_m: 0.8715 - val_precision: 0.8715 - val_recall_6: 0.8646 - val_f1score: 0.8680\n",
      "Epoch 223/300\n",
      "epoch:  222\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2106 - acc: 0.9052 - prec: 0.9873 - recall: 0.9907 - prec_1: 0.9795 - recall_1: 0.9864 - prec_2: 0.9841 - recall_2: 0.9847 - prec_3: 0.7412 - recall_3: 0.7481 - prec_4: 0.7495 - recall_4: 0.7505 - prec_5: 0.9857 - recall_5: 0.9908 - f1_m: 0.9046 - recall_m: 0.9015 - precision_m: 0.9079 - precision: 0.9079 - recall_6: 0.9015 - f1score: 0.9046 - val_loss: 0.3215 - val_acc: 0.8538 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1609 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.0785 - val_prec_4: 0.1811 - val_recall_4: 0.1607 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8543 - val_recall_m: 0.8516 - val_precision_m: 0.8570 - val_precision: 0.8570 - val_recall_6: 0.8516 - val_f1score: 0.8543\n",
      "Epoch 224/300\n",
      "epoch:  223\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2086 - acc: 0.9077 - prec: 0.9884 - recall: 0.9935 - prec_1: 0.9749 - recall_1: 0.9843 - prec_2: 0.9882 - recall_2: 0.9848 - prec_3: 0.7422 - recall_3: 0.7357 - prec_4: 0.7396 - recall_4: 0.7545 - prec_5: 0.9865 - recall_5: 0.9894 - f1_m: 0.9074 - recall_m: 0.9050 - precision_m: 0.9099 - precision: 0.9099 - recall_6: 0.9050 - f1score: 0.9074 - val_loss: 0.3983 - val_acc: 0.8436 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1756 - val_recall_1: 0.1563 - val_prec_2: 0.1887 - val_recall_2: 0.1817 - val_prec_3: 0.1759 - val_recall_3: 0.1070 - val_prec_4: 0.1650 - val_recall_4: 0.1269 - val_prec_5: 0.1684 - val_recall_5: 0.1582 - val_f1_m: 0.8434 - val_recall_m: 0.8401 - val_precision_m: 0.8468 - val_precision: 0.8468 - val_recall_6: 0.8401 - val_f1score: 0.8434\n",
      "Epoch 225/300\n",
      "epoch:  224\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2113 - acc: 0.9061 - prec: 0.9884 - recall: 0.9919 - prec_1: 0.9768 - recall_1: 0.9886 - prec_2: 0.9899 - recall_2: 0.9838 - prec_3: 0.7455 - recall_3: 0.7502 - prec_4: 0.7450 - recall_4: 0.7472 - prec_5: 0.9878 - recall_5: 0.9879 - f1_m: 0.9064 - recall_m: 0.9039 - precision_m: 0.9090 - precision: 0.9090 - recall_6: 0.9039 - f1score: 0.9064 - val_loss: 0.2673 - val_acc: 0.8878 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1571 - val_prec_2: 0.1919 - val_recall_2: 0.1817 - val_prec_3: 0.1759 - val_recall_3: 0.1325 - val_prec_4: 0.1851 - val_recall_4: 0.1587 - val_prec_5: 0.1684 - val_recall_5: 0.1607 - val_f1_m: 0.8873 - val_recall_m: 0.8848 - val_precision_m: 0.8900 - val_precision: 0.8900 - val_recall_6: 0.8848 - val_f1score: 0.8873\n",
      "Epoch 226/300\n",
      "epoch:  225\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2090 - acc: 0.9042 - prec: 0.9851 - recall: 0.9910 - prec_1: 0.9801 - recall_1: 0.9817 - prec_2: 0.9840 - recall_2: 0.9877 - prec_3: 0.7326 - recall_3: 0.7543 - prec_4: 0.7441 - recall_4: 0.7318 - prec_5: 0.9903 - recall_5: 0.9898 - f1_m: 0.9041 - recall_m: 0.9012 - precision_m: 0.9070 - precision: 0.9070 - recall_6: 0.9012 - f1score: 0.9041 - val_loss: 0.3707 - val_acc: 0.8566 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1751 - val_recall_1: 0.1609 - val_prec_2: 0.1919 - val_recall_2: 0.1699 - val_prec_3: 0.1599 - val_recall_3: 0.0920 - val_prec_4: 0.1809 - val_recall_4: 0.1679 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8569 - val_recall_m: 0.8546 - val_precision_m: 0.8593 - val_precision: 0.8593 - val_recall_6: 0.8546 - val_f1score: 0.8569\n",
      "Epoch 227/300\n",
      "epoch:  226\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2101 - acc: 0.9047 - prec: 0.9901 - recall: 0.9933 - prec_1: 0.9829 - recall_1: 0.9851 - prec_2: 0.9857 - recall_2: 0.9877 - prec_3: 0.7316 - recall_3: 0.7513 - prec_4: 0.7474 - recall_4: 0.7380 - prec_5: 0.9882 - recall_5: 0.9922 - f1_m: 0.9037 - recall_m: 0.9005 - precision_m: 0.9070 - precision: 0.9070 - recall_6: 0.9005 - f1score: 0.9037 - val_loss: 0.2649 - val_acc: 0.8838 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1753 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1791 - val_prec_3: 0.1759 - val_recall_3: 0.1345 - val_prec_4: 0.1866 - val_recall_4: 0.1529 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8841 - val_recall_m: 0.8813 - val_precision_m: 0.8870 - val_precision: 0.8870 - val_recall_6: 0.8813 - val_f1score: 0.8841\n",
      "Epoch 228/300\n",
      "epoch:  227\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2098 - acc: 0.9034 - prec: 0.9881 - recall: 0.9906 - prec_1: 0.9825 - recall_1: 0.9891 - prec_2: 0.9890 - recall_2: 0.9840 - prec_3: 0.7371 - recall_3: 0.7457 - prec_4: 0.7350 - recall_4: 0.7414 - prec_5: 0.9831 - recall_5: 0.9900 - f1_m: 0.9031 - recall_m: 0.8996 - precision_m: 0.9066 - precision: 0.9066 - recall_6: 0.8996 - f1score: 0.9031 - val_loss: 0.2663 - val_acc: 0.8856 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1602 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1413 - val_prec_4: 0.1898 - val_recall_4: 0.1413 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8841 - val_recall_m: 0.8816 - val_precision_m: 0.8866 - val_precision: 0.8866 - val_recall_6: 0.8816 - val_f1score: 0.8841\n",
      "Epoch 229/300\n",
      "epoch:  228\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2079 - acc: 0.9051 - prec: 0.9870 - recall: 0.9897 - prec_1: 0.9786 - recall_1: 0.9846 - prec_2: 0.9867 - recall_2: 0.9820 - prec_3: 0.7455 - recall_3: 0.7499 - prec_4: 0.7459 - recall_4: 0.7567 - prec_5: 0.9906 - recall_5: 0.9906 - f1_m: 0.9050 - recall_m: 0.9016 - precision_m: 0.9085 - precision: 0.9085 - recall_6: 0.9016 - f1score: 0.9050 - val_loss: 0.2647 - val_acc: 0.8883 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1464 - val_prec_4: 0.1905 - val_recall_4: 0.1414 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8874 - val_recall_m: 0.8851 - val_precision_m: 0.8897 - val_precision: 0.8897 - val_recall_6: 0.8851 - val_f1score: 0.8874\n",
      "Epoch 230/300\n",
      "epoch:  229\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2077 - acc: 0.9044 - prec: 0.9864 - recall: 0.9939 - prec_1: 0.9823 - recall_1: 0.9878 - prec_2: 0.9917 - recall_2: 0.9875 - prec_3: 0.7478 - recall_3: 0.7355 - prec_4: 0.7335 - recall_4: 0.7525 - prec_5: 0.9899 - recall_5: 0.9911 - f1_m: 0.9037 - recall_m: 0.9009 - precision_m: 0.9065 - precision: 0.9065 - recall_6: 0.9009 - f1score: 0.9037 - val_loss: 0.2652 - val_acc: 0.8881 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1502 - val_prec_4: 0.1919 - val_recall_4: 0.1396 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8883 - val_recall_m: 0.8856 - val_precision_m: 0.8910 - val_precision: 0.8910 - val_recall_6: 0.8856 - val_f1score: 0.8883\n",
      "Epoch 231/300\n",
      "epoch:  230\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2044 - acc: 0.9082 - prec: 0.9841 - recall: 0.9943 - prec_1: 0.9847 - recall_1: 0.9812 - prec_2: 0.9835 - recall_2: 0.9837 - prec_3: 0.7592 - recall_3: 0.7567 - prec_4: 0.7502 - recall_4: 0.7600 - prec_5: 0.9775 - recall_5: 0.9895 - f1_m: 0.9083 - recall_m: 0.9051 - precision_m: 0.9117 - precision: 0.9117 - recall_6: 0.9051 - f1score: 0.9083 - val_loss: 0.2632 - val_acc: 0.8891 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1481 - val_prec_4: 0.1911 - val_recall_4: 0.1447 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8891 - val_recall_m: 0.8863 - val_precision_m: 0.8920 - val_precision: 0.8920 - val_recall_6: 0.8863 - val_f1score: 0.8891\n",
      "Epoch 232/300\n",
      "epoch:  231\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2072 - acc: 0.9044 - prec: 0.9871 - recall: 0.9900 - prec_1: 0.9756 - recall_1: 0.9850 - prec_2: 0.9837 - recall_2: 0.9848 - prec_3: 0.7466 - recall_3: 0.7473 - prec_4: 0.7452 - recall_4: 0.7529 - prec_5: 0.9848 - recall_5: 0.9893 - f1_m: 0.9044 - recall_m: 0.9010 - precision_m: 0.9079 - precision: 0.9079 - recall_6: 0.9010 - f1score: 0.9044 - val_loss: 0.2631 - val_acc: 0.8918 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1458 - val_prec_4: 0.1911 - val_recall_4: 0.1497 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8916 - val_recall_m: 0.8893 - val_precision_m: 0.8940 - val_precision: 0.8940 - val_recall_6: 0.8893 - val_f1score: 0.8916\n",
      "Epoch 233/300\n",
      "epoch:  232\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2068 - acc: 0.9099 - prec: 0.9897 - recall: 0.9926 - prec_1: 0.9789 - recall_1: 0.9895 - prec_2: 0.9899 - recall_2: 0.9861 - prec_3: 0.7518 - recall_3: 0.7577 - prec_4: 0.7566 - recall_4: 0.7599 - prec_5: 0.9902 - recall_5: 0.9906 - f1_m: 0.9096 - recall_m: 0.9069 - precision_m: 0.9124 - precision: 0.9124 - recall_6: 0.9069 - f1score: 0.9096 - val_loss: 0.2641 - val_acc: 0.8861 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1416 - val_prec_4: 0.1898 - val_recall_4: 0.1434 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8857 - val_recall_m: 0.8836 - val_precision_m: 0.8879 - val_precision: 0.8879 - val_recall_6: 0.8836 - val_f1score: 0.8857\n",
      "Epoch 234/300\n",
      "epoch:  233\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2075 - acc: 0.9056 - prec: 0.9921 - recall: 0.9925 - prec_1: 0.9767 - recall_1: 0.9837 - prec_2: 0.9839 - recall_2: 0.9869 - prec_3: 0.7418 - recall_3: 0.7577 - prec_4: 0.7467 - recall_4: 0.7412 - prec_5: 0.9895 - recall_5: 0.9905 - f1_m: 0.9042 - recall_m: 0.9014 - precision_m: 0.9071 - precision: 0.9071 - recall_6: 0.9014 - f1score: 0.9042 - val_loss: 0.2632 - val_acc: 0.8913 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1479 - val_prec_4: 0.1919 - val_recall_4: 0.1474 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8911 - val_recall_m: 0.8888 - val_precision_m: 0.8935 - val_precision: 0.8935 - val_recall_6: 0.8888 - val_f1score: 0.8911\n",
      "Epoch 235/300\n",
      "epoch:  234\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2024 - acc: 0.9137 - prec: 0.9891 - recall: 0.9896 - prec_1: 0.9811 - recall_1: 0.9890 - prec_2: 0.9917 - recall_2: 0.9828 - prec_3: 0.7665 - recall_3: 0.7696 - prec_4: 0.7655 - recall_4: 0.7664 - prec_5: 0.9872 - recall_5: 0.9926 - f1_m: 0.9136 - recall_m: 0.9102 - precision_m: 0.9170 - precision: 0.9170 - recall_6: 0.9102 - f1score: 0.9136 - val_loss: 0.2631 - val_acc: 0.8941 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1425 - val_prec_4: 0.1911 - val_recall_4: 0.1557 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8942 - val_recall_m: 0.8918 - val_precision_m: 0.8967 - val_precision: 0.8967 - val_recall_6: 0.8918 - val_f1score: 0.8942\n",
      "Epoch 236/300\n",
      "epoch:  235\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2073 - acc: 0.9057 - prec: 0.9875 - recall: 0.9928 - prec_1: 0.9848 - recall_1: 0.9899 - prec_2: 0.9900 - recall_2: 0.9883 - prec_3: 0.7419 - recall_3: 0.7533 - prec_4: 0.7402 - recall_4: 0.7453 - prec_5: 0.9911 - recall_5: 0.9922 - f1_m: 0.9048 - recall_m: 0.9017 - precision_m: 0.9079 - precision: 0.9079 - recall_6: 0.9017 - f1score: 0.9048 - val_loss: 0.2616 - val_acc: 0.8921 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1383 - val_prec_4: 0.1887 - val_recall_4: 0.1564 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8911 - val_recall_m: 0.8888 - val_precision_m: 0.8935 - val_precision: 0.8935 - val_recall_6: 0.8888 - val_f1score: 0.8911\n",
      "Epoch 237/300\n",
      "epoch:  236\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2036 - acc: 0.9090 - prec: 0.9886 - recall: 0.9933 - prec_1: 0.9787 - recall_1: 0.9872 - prec_2: 0.9887 - recall_2: 0.9834 - prec_3: 0.7536 - recall_3: 0.7555 - prec_4: 0.7536 - recall_4: 0.7550 - prec_5: 0.9935 - recall_5: 0.9915 - f1_m: 0.9084 - recall_m: 0.9060 - precision_m: 0.9109 - precision: 0.9109 - recall_6: 0.9060 - f1score: 0.9084 - val_loss: 0.2615 - val_acc: 0.8918 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1445 - val_prec_4: 0.1905 - val_recall_4: 0.1514 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8912 - val_recall_m: 0.8888 - val_precision_m: 0.8937 - val_precision: 0.8937 - val_recall_6: 0.8888 - val_f1score: 0.8912\n",
      "Epoch 238/300\n",
      "epoch:  237\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2046 - acc: 0.9075 - prec: 0.9910 - recall: 0.9922 - prec_1: 0.9709 - recall_1: 0.9846 - prec_2: 0.9800 - recall_2: 0.9809 - prec_3: 0.7518 - recall_3: 0.7580 - prec_4: 0.7551 - recall_4: 0.7639 - prec_5: 0.9893 - recall_5: 0.9867 - f1_m: 0.9075 - recall_m: 0.9040 - precision_m: 0.9111 - precision: 0.9111 - recall_6: 0.9040 - f1score: 0.9075 - val_loss: 0.2620 - val_acc: 0.8923 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1455 - val_prec_4: 0.1905 - val_recall_4: 0.1507 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8925 - val_recall_m: 0.8903 - val_precision_m: 0.8947 - val_precision: 0.8947 - val_recall_6: 0.8903 - val_f1score: 0.8925\n",
      "Epoch 239/300\n",
      "epoch:  238\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 675us/step - loss: 0.2075 - acc: 0.9040 - prec: 0.9872 - recall: 0.9903 - prec_1: 0.9734 - recall_1: 0.9827 - prec_2: 0.9834 - recall_2: 0.9833 - prec_3: 0.7489 - recall_3: 0.7395 - prec_4: 0.7451 - recall_4: 0.7604 - prec_5: 0.9864 - recall_5: 0.9891 - f1_m: 0.9041 - recall_m: 0.9007 - precision_m: 0.9076 - precision: 0.9076 - recall_6: 0.9007 - f1score: 0.9041 - val_loss: 0.2639 - val_acc: 0.8863 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1589 - val_prec_2: 0.1901 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1216 - val_prec_4: 0.1878 - val_recall_4: 0.1684 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8863 - val_recall_m: 0.8838 - val_precision_m: 0.8889 - val_precision: 0.8889 - val_recall_6: 0.8838 - val_f1score: 0.8863\n",
      "Epoch 240/300\n",
      "epoch:  239\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2067 - acc: 0.9092 - prec: 0.9898 - recall: 0.9927 - prec_1: 0.9734 - recall_1: 0.9866 - prec_2: 0.9804 - recall_2: 0.9829 - prec_3: 0.7643 - recall_3: 0.7572 - prec_4: 0.7533 - recall_4: 0.7674 - prec_5: 0.9905 - recall_5: 0.9875 - f1_m: 0.9091 - recall_m: 0.9057 - precision_m: 0.9125 - precision: 0.9125 - recall_6: 0.9057 - f1score: 0.9091 - val_loss: 0.2629 - val_acc: 0.8881 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1602 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1273 - val_prec_4: 0.1878 - val_recall_4: 0.1634 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8881 - val_recall_m: 0.8856 - val_precision_m: 0.8907 - val_precision: 0.8907 - val_recall_6: 0.8856 - val_f1score: 0.8881\n",
      "Epoch 241/300\n",
      "epoch:  240\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2102 - acc: 0.9077 - prec: 0.9893 - recall: 0.9900 - prec_1: 0.9789 - recall_1: 0.9898 - prec_2: 0.9827 - recall_2: 0.9815 - prec_3: 0.7437 - recall_3: 0.7555 - prec_4: 0.7526 - recall_4: 0.7509 - prec_5: 0.9904 - recall_5: 0.9916 - f1_m: 0.9075 - recall_m: 0.9051 - precision_m: 0.9100 - precision: 0.9100 - recall_6: 0.9051 - f1score: 0.9075 - val_loss: 0.2620 - val_acc: 0.8923 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1444 - val_prec_4: 0.1905 - val_recall_4: 0.1514 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8919 - val_recall_m: 0.8891 - val_precision_m: 0.8947 - val_precision: 0.8947 - val_recall_6: 0.8891 - val_f1score: 0.8919\n",
      "Epoch 242/300\n",
      "epoch:  241\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2080 - acc: 0.9079 - prec: 0.9860 - recall: 0.9930 - prec_1: 0.9824 - recall_1: 0.9901 - prec_2: 0.9899 - recall_2: 0.9873 - prec_3: 0.7527 - recall_3: 0.7550 - prec_4: 0.7425 - recall_4: 0.7516 - prec_5: 0.9937 - recall_5: 0.9924 - f1_m: 0.9069 - recall_m: 0.9036 - precision_m: 0.9103 - precision: 0.9103 - recall_6: 0.9036 - f1score: 0.9069 - val_loss: 0.2624 - val_acc: 0.8883 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1602 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1503 - val_prec_4: 0.1911 - val_recall_4: 0.1412 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8884 - val_recall_m: 0.8856 - val_precision_m: 0.8913 - val_precision: 0.8913 - val_recall_6: 0.8856 - val_f1score: 0.8884\n",
      "Epoch 243/300\n",
      "epoch:  242\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2098 - acc: 0.9027 - prec: 0.9875 - recall: 0.9925 - prec_1: 0.9798 - recall_1: 0.9910 - prec_2: 0.9887 - recall_2: 0.9845 - prec_3: 0.7301 - recall_3: 0.7287 - prec_4: 0.7343 - recall_4: 0.7403 - prec_5: 0.9933 - recall_5: 0.9920 - f1_m: 0.9023 - recall_m: 0.8992 - precision_m: 0.9054 - precision: 0.9054 - recall_6: 0.8992 - f1score: 0.9023 - val_loss: 0.2611 - val_acc: 0.8881 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1371 - val_prec_4: 0.1878 - val_recall_4: 0.1537 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8882 - val_recall_m: 0.8858 - val_precision_m: 0.8907 - val_precision: 0.8907 - val_recall_6: 0.8858 - val_f1score: 0.8882\n",
      "Epoch 244/300\n",
      "epoch:  243\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 657us/step - loss: 0.2037 - acc: 0.9082 - prec: 0.9873 - recall: 0.9912 - prec_1: 0.9780 - recall_1: 0.9883 - prec_2: 0.9862 - recall_2: 0.9876 - prec_3: 0.7468 - recall_3: 0.7533 - prec_4: 0.7538 - recall_4: 0.7476 - prec_5: 0.9902 - recall_5: 0.9891 - f1_m: 0.9081 - recall_m: 0.9056 - precision_m: 0.9106 - precision: 0.9106 - recall_6: 0.9056 - f1score: 0.9081 - val_loss: 0.2607 - val_acc: 0.8906 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1589 - val_prec_2: 0.1901 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1365 - val_prec_4: 0.1878 - val_recall_4: 0.1577 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8904 - val_recall_m: 0.8876 - val_precision_m: 0.8933 - val_precision: 0.8933 - val_recall_6: 0.8876 - val_f1score: 0.8904\n",
      "Epoch 245/300\n",
      "epoch:  244\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2056 - acc: 0.9069 - prec: 0.9908 - recall: 0.9907 - prec_1: 0.9810 - recall_1: 0.9858 - prec_2: 0.9860 - recall_2: 0.9850 - prec_3: 0.7438 - recall_3: 0.7521 - prec_4: 0.7442 - recall_4: 0.7518 - prec_5: 0.9882 - recall_5: 0.9926 - f1_m: 0.9062 - recall_m: 0.9030 - precision_m: 0.9095 - precision: 0.9095 - recall_6: 0.9030 - f1score: 0.9062 - val_loss: 0.2606 - val_acc: 0.8918 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1445 - val_prec_4: 0.1905 - val_recall_4: 0.1509 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8920 - val_recall_m: 0.8896 - val_precision_m: 0.8946 - val_precision: 0.8946 - val_recall_6: 0.8896 - val_f1score: 0.8920\n",
      "Epoch 246/300\n",
      "epoch:  245\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2056 - acc: 0.9074 - prec: 0.9856 - recall: 0.9936 - prec_1: 0.9903 - recall_1: 0.9807 - prec_2: 0.9836 - recall_2: 0.9901 - prec_3: 0.7554 - recall_3: 0.7360 - prec_4: 0.7412 - recall_4: 0.7684 - prec_5: 0.9885 - recall_5: 0.9963 - f1_m: 0.9064 - recall_m: 0.9034 - precision_m: 0.9096 - precision: 0.9096 - recall_6: 0.9034 - f1score: 0.9064 - val_loss: 0.2623 - val_acc: 0.8871 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1604 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1348 - val_prec_4: 0.1873 - val_recall_4: 0.1549 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8866 - val_recall_m: 0.8843 - val_precision_m: 0.8889 - val_precision: 0.8889 - val_recall_6: 0.8843 - val_f1score: 0.8866\n",
      "Epoch 247/300\n",
      "epoch:  246\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2084 - acc: 0.9064 - prec: 0.9857 - recall: 0.9923 - prec_1: 0.9806 - recall_1: 0.9901 - prec_2: 0.9918 - recall_2: 0.9843 - prec_3: 0.7429 - recall_3: 0.7629 - prec_4: 0.7497 - recall_4: 0.7438 - prec_5: 0.9923 - recall_5: 0.9928 - f1_m: 0.9058 - recall_m: 0.9031 - precision_m: 0.9085 - precision: 0.9085 - recall_6: 0.9031 - f1score: 0.9058 - val_loss: 0.2633 - val_acc: 0.8856 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1461 - val_prec_4: 0.1898 - val_recall_4: 0.1406 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8854 - val_recall_m: 0.8831 - val_precision_m: 0.8879 - val_precision: 0.8879 - val_recall_6: 0.8831 - val_f1score: 0.8854\n",
      "Epoch 248/300\n",
      "epoch:  247\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2082 - acc: 0.9055 - prec: 0.9918 - recall: 0.9943 - prec_1: 0.9840 - recall_1: 0.9871 - prec_2: 0.9848 - recall_2: 0.9880 - prec_3: 0.7394 - recall_3: 0.7490 - prec_4: 0.7389 - recall_4: 0.7409 - prec_5: 0.9904 - recall_5: 0.9894 - f1_m: 0.9049 - recall_m: 0.9020 - precision_m: 0.9079 - precision: 0.9079 - recall_6: 0.9020 - f1score: 0.9049 - val_loss: 0.2615 - val_acc: 0.8901 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1433 - val_prec_4: 0.1898 - val_recall_4: 0.1502 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8895 - val_recall_m: 0.8873 - val_precision_m: 0.8918 - val_precision: 0.8918 - val_recall_6: 0.8873 - val_f1score: 0.8895\n",
      "Epoch 249/300\n",
      "epoch:  248\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2108 - acc: 0.9060 - prec: 0.9906 - recall: 0.9935 - prec_1: 0.9818 - recall_1: 0.9867 - prec_2: 0.9900 - recall_2: 0.9871 - prec_3: 0.7475 - recall_3: 0.7479 - prec_4: 0.7393 - recall_4: 0.7492 - prec_5: 0.9881 - recall_5: 0.9907 - f1_m: 0.9059 - recall_m: 0.9032 - precision_m: 0.9086 - precision: 0.9086 - recall_6: 0.9032 - f1score: 0.9059 - val_loss: 0.2618 - val_acc: 0.8883 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1417 - val_prec_4: 0.1882 - val_recall_4: 0.1494 - val_prec_5: 0.1684 - val_recall_5: 0.1610 - val_f1_m: 0.8877 - val_recall_m: 0.8851 - val_precision_m: 0.8903 - val_precision: 0.8903 - val_recall_6: 0.8851 - val_f1score: 0.8877\n",
      "Epoch 250/300\n",
      "epoch:  249\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2066 - acc: 0.9067 - prec: 0.9867 - recall: 0.9935 - prec_1: 0.9782 - recall_1: 0.9903 - prec_2: 0.9949 - recall_2: 0.9830 - prec_3: 0.7446 - recall_3: 0.7477 - prec_4: 0.7451 - recall_4: 0.7489 - prec_5: 0.9885 - recall_5: 0.9933 - f1_m: 0.9066 - recall_m: 0.9042 - precision_m: 0.9090 - precision: 0.9090 - recall_6: 0.9042 - f1score: 0.9066 - val_loss: 0.2627 - val_acc: 0.8878 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1310 - val_prec_4: 0.1866 - val_recall_4: 0.1592 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8879 - val_recall_m: 0.8856 - val_precision_m: 0.8902 - val_precision: 0.8902 - val_recall_6: 0.8856 - val_f1score: 0.8879\n",
      "Epoch 251/300\n",
      "epoch:  250\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2061 - acc: 0.9076 - prec: 0.9867 - recall: 0.9921 - prec_1: 0.9830 - recall_1: 0.9881 - prec_2: 0.9885 - recall_2: 0.9862 - prec_3: 0.7434 - recall_3: 0.7398 - prec_4: 0.7447 - recall_4: 0.7587 - prec_5: 0.9916 - recall_5: 0.9922 - f1_m: 0.9067 - recall_m: 0.9041 - precision_m: 0.9093 - precision: 0.9093 - recall_6: 0.9041 - f1score: 0.9067 - val_loss: 0.2653 - val_acc: 0.8891 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1187 - val_prec_4: 0.1859 - val_recall_4: 0.1729 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8894 - val_recall_m: 0.8868 - val_precision_m: 0.8921 - val_precision: 0.8921 - val_recall_6: 0.8868 - val_f1score: 0.8894\n",
      "Epoch 252/300\n",
      "epoch:  251\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2066 - acc: 0.9067 - prec: 0.9862 - recall: 0.9935 - prec_1: 0.9783 - recall_1: 0.9930 - prec_2: 0.9935 - recall_2: 0.9834 - prec_3: 0.7436 - recall_3: 0.7454 - prec_4: 0.7445 - recall_4: 0.7531 - prec_5: 0.9900 - recall_5: 0.9918 - f1_m: 0.9058 - recall_m: 0.9027 - precision_m: 0.9090 - precision: 0.9090 - recall_6: 0.9027 - f1score: 0.9058 - val_loss: 0.2657 - val_acc: 0.8863 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1172 - val_prec_4: 0.1846 - val_recall_4: 0.1714 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8865 - val_recall_m: 0.8838 - val_precision_m: 0.8893 - val_precision: 0.8893 - val_recall_6: 0.8838 - val_f1score: 0.8865\n",
      "Epoch 253/300\n",
      "epoch:  252\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2066 - acc: 0.9085 - prec: 0.9888 - recall: 0.9945 - prec_1: 0.9789 - recall_1: 0.9846 - prec_2: 0.9845 - recall_2: 0.9824 - prec_3: 0.7516 - recall_3: 0.7648 - prec_4: 0.7554 - recall_4: 0.7570 - prec_5: 0.9901 - recall_5: 0.9930 - f1_m: 0.9077 - recall_m: 0.9044 - precision_m: 0.9112 - precision: 0.9112 - recall_6: 0.9044 - f1score: 0.9077 - val_loss: 0.2626 - val_acc: 0.8888 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1457 - val_prec_4: 0.1892 - val_recall_4: 0.1462 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8884 - val_recall_m: 0.8861 - val_precision_m: 0.8909 - val_precision: 0.8909 - val_recall_6: 0.8861 - val_f1score: 0.8884\n",
      "Epoch 254/300\n",
      "epoch:  253\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2104 - acc: 0.9062 - prec: 0.9848 - recall: 0.9941 - prec_1: 0.9748 - recall_1: 0.9855 - prec_2: 0.9857 - recall_2: 0.9836 - prec_3: 0.7526 - recall_3: 0.7494 - prec_4: 0.7478 - recall_4: 0.7607 - prec_5: 0.9821 - recall_5: 0.9912 - f1_m: 0.9062 - recall_m: 0.9037 - precision_m: 0.9088 - precision: 0.9088 - recall_6: 0.9037 - f1score: 0.9062 - val_loss: 0.2628 - val_acc: 0.8918 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1444 - val_prec_4: 0.1905 - val_recall_4: 0.1517 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8920 - val_recall_m: 0.8891 - val_precision_m: 0.8951 - val_precision: 0.8951 - val_recall_6: 0.8891 - val_f1score: 0.8920\n",
      "Epoch 255/300\n",
      "epoch:  254\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2016 - acc: 0.9112 - prec: 0.9863 - recall: 0.9929 - prec_1: 0.9821 - recall_1: 0.9879 - prec_2: 0.9866 - recall_2: 0.9858 - prec_3: 0.7593 - recall_3: 0.7610 - prec_4: 0.7638 - recall_4: 0.7723 - prec_5: 0.9902 - recall_5: 0.9919 - f1_m: 0.9106 - recall_m: 0.9081 - precision_m: 0.9132 - precision: 0.9132 - recall_6: 0.9081 - f1score: 0.9106 - val_loss: 0.2637 - val_acc: 0.8871 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1478 - val_prec_4: 0.1911 - val_recall_4: 0.1429 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8867 - val_recall_m: 0.8841 - val_precision_m: 0.8894 - val_precision: 0.8894 - val_recall_6: 0.8841 - val_f1score: 0.8867\n",
      "Epoch 256/300\n",
      "epoch:  255\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2067 - acc: 0.9076 - prec: 0.9906 - recall: 0.9937 - prec_1: 0.9801 - recall_1: 0.9857 - prec_2: 0.9846 - recall_2: 0.9840 - prec_3: 0.7514 - recall_3: 0.7611 - prec_4: 0.7514 - recall_4: 0.7588 - prec_5: 0.9836 - recall_5: 0.9887 - f1_m: 0.9068 - recall_m: 0.9042 - precision_m: 0.9095 - precision: 0.9095 - recall_6: 0.9042 - f1score: 0.9068 - val_loss: 0.2654 - val_acc: 0.8926 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1590 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1467 - val_prec_4: 0.1911 - val_recall_4: 0.1502 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8924 - val_recall_m: 0.8901 - val_precision_m: 0.8948 - val_precision: 0.8948 - val_recall_6: 0.8901 - val_f1score: 0.8924\n",
      "Epoch 257/300\n",
      "epoch:  256\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2081 - acc: 0.9091 - prec: 0.9870 - recall: 0.9917 - prec_1: 0.9824 - recall_1: 0.9927 - prec_2: 0.9931 - recall_2: 0.9867 - prec_3: 0.7476 - recall_3: 0.7515 - prec_4: 0.7517 - recall_4: 0.7546 - prec_5: 0.9952 - recall_5: 0.9937 - f1_m: 0.9083 - recall_m: 0.9051 - precision_m: 0.9117 - precision: 0.9117 - recall_6: 0.9051 - f1score: 0.9083 - val_loss: 0.2611 - val_acc: 0.8886 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1605 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1432 - val_prec_4: 0.1882 - val_recall_4: 0.1479 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8882 - val_recall_m: 0.8853 - val_precision_m: 0.8911 - val_precision: 0.8911 - val_recall_6: 0.8853 - val_f1score: 0.8882\n",
      "Epoch 258/300\n",
      "epoch:  257\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2052 - acc: 0.9061 - prec: 0.9890 - recall: 0.9935 - prec_1: 0.9774 - recall_1: 0.9884 - prec_2: 0.9899 - recall_2: 0.9835 - prec_3: 0.7494 - recall_3: 0.7388 - prec_4: 0.7408 - recall_4: 0.7589 - prec_5: 0.9911 - recall_5: 0.9952 - f1_m: 0.9063 - recall_m: 0.9039 - precision_m: 0.9088 - precision: 0.9088 - recall_6: 0.9039 - f1score: 0.9063 - val_loss: 0.2608 - val_acc: 0.8911 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1369 - val_prec_4: 0.1882 - val_recall_4: 0.1579 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8910 - val_recall_m: 0.8883 - val_precision_m: 0.8938 - val_precision: 0.8938 - val_recall_6: 0.8883 - val_f1score: 0.8910\n",
      "Epoch 259/300\n",
      "epoch:  258\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2068 - acc: 0.9045 - prec: 0.9863 - recall: 0.9931 - prec_1: 0.9869 - recall_1: 0.9901 - prec_2: 0.9916 - recall_2: 0.9899 - prec_3: 0.7371 - recall_3: 0.7422 - prec_4: 0.7375 - recall_4: 0.7347 - prec_5: 0.9934 - recall_5: 0.9925 - f1_m: 0.9040 - recall_m: 0.9011 - precision_m: 0.9069 - precision: 0.9069 - recall_6: 0.9011 - f1score: 0.9040 - val_loss: 0.2625 - val_acc: 0.8891 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1323 - val_prec_4: 0.1866 - val_recall_4: 0.1592 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8886 - val_recall_m: 0.8863 - val_precision_m: 0.8910 - val_precision: 0.8910 - val_recall_6: 0.8863 - val_f1score: 0.8886\n",
      "Epoch 260/300\n",
      "epoch:  259\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2049 - acc: 0.9095 - prec: 0.9920 - recall: 0.9921 - prec_1: 0.9769 - recall_1: 0.9883 - prec_2: 0.9906 - recall_2: 0.9869 - prec_3: 0.7535 - recall_3: 0.7619 - prec_4: 0.7583 - recall_4: 0.7594 - prec_5: 0.9883 - recall_5: 0.9897 - f1_m: 0.9088 - recall_m: 0.9060 - precision_m: 0.9117 - precision: 0.9117 - recall_6: 0.9060 - f1score: 0.9088 - val_loss: 0.2609 - val_acc: 0.8886 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1424 - val_prec_4: 0.1887 - val_recall_4: 0.1494 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8888 - val_recall_m: 0.8866 - val_precision_m: 0.8911 - val_precision: 0.8911 - val_recall_6: 0.8866 - val_f1score: 0.8888\n",
      "Epoch 261/300\n",
      "epoch:  260\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2065 - acc: 0.9069 - prec: 0.9902 - recall: 0.9937 - prec_1: 0.9787 - recall_1: 0.9832 - prec_2: 0.9881 - recall_2: 0.9818 - prec_3: 0.7490 - recall_3: 0.7580 - prec_4: 0.7562 - recall_4: 0.7505 - prec_5: 0.9837 - recall_5: 0.9944 - f1_m: 0.9063 - recall_m: 0.9032 - precision_m: 0.9095 - precision: 0.9095 - recall_6: 0.9032 - f1score: 0.9063 - val_loss: 0.2619 - val_acc: 0.8883 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1469 - val_prec_4: 0.1905 - val_recall_4: 0.1447 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8882 - val_recall_m: 0.8858 - val_precision_m: 0.8906 - val_precision: 0.8906 - val_recall_6: 0.8858 - val_f1score: 0.8882\n",
      "Epoch 262/300\n",
      "epoch:  261\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2037 - acc: 0.9095 - prec: 0.9870 - recall: 0.9934 - prec_1: 0.9869 - recall_1: 0.9822 - prec_2: 0.9884 - recall_2: 0.9878 - prec_3: 0.7599 - recall_3: 0.7497 - prec_4: 0.7521 - recall_4: 0.7704 - prec_5: 0.9887 - recall_5: 0.9927 - f1_m: 0.9094 - recall_m: 0.9066 - precision_m: 0.9123 - precision: 0.9123 - recall_6: 0.9066 - f1score: 0.9094 - val_loss: 0.2601 - val_acc: 0.8893 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1397 - val_prec_4: 0.1882 - val_recall_4: 0.1524 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8892 - val_recall_m: 0.8868 - val_precision_m: 0.8917 - val_precision: 0.8917 - val_recall_6: 0.8868 - val_f1score: 0.8892\n",
      "Epoch 263/300\n",
      "epoch:  262\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2076 - acc: 0.9051 - prec: 0.9887 - recall: 0.9928 - prec_1: 0.9759 - recall_1: 0.9840 - prec_2: 0.9852 - recall_2: 0.9889 - prec_3: 0.7463 - recall_3: 0.7410 - prec_4: 0.7336 - recall_4: 0.7445 - prec_5: 0.9880 - recall_5: 0.9875 - f1_m: 0.9046 - recall_m: 0.9019 - precision_m: 0.9074 - precision: 0.9074 - recall_6: 0.9019 - f1score: 0.9046 - val_loss: 0.2617 - val_acc: 0.8903 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1314 - val_prec_4: 0.1873 - val_recall_4: 0.1612 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8904 - val_recall_m: 0.8876 - val_precision_m: 0.8933 - val_precision: 0.8933 - val_recall_6: 0.8876 - val_f1score: 0.8904\n",
      "Epoch 264/300\n",
      "epoch:  263\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2062 - acc: 0.9086 - prec: 0.9862 - recall: 0.9919 - prec_1: 0.9776 - recall_1: 0.9851 - prec_2: 0.9844 - recall_2: 0.9852 - prec_3: 0.7571 - recall_3: 0.7591 - prec_4: 0.7509 - recall_4: 0.7600 - prec_5: 0.9898 - recall_5: 0.9916 - f1_m: 0.9086 - recall_m: 0.9057 - precision_m: 0.9116 - precision: 0.9116 - recall_6: 0.9057 - f1score: 0.9086 - val_loss: 0.2607 - val_acc: 0.8908 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1386 - val_prec_4: 0.1892 - val_recall_4: 0.1557 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8907 - val_recall_m: 0.8883 - val_precision_m: 0.8932 - val_precision: 0.8932 - val_recall_6: 0.8883 - val_f1score: 0.8907\n",
      "Epoch 265/300\n",
      "epoch:  264\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2084 - acc: 0.9089 - prec: 0.9894 - recall: 0.9926 - prec_1: 0.9808 - recall_1: 0.9862 - prec_2: 0.9845 - recall_2: 0.9838 - prec_3: 0.7503 - recall_3: 0.7520 - prec_4: 0.7442 - recall_4: 0.7561 - prec_5: 0.9884 - recall_5: 0.9910 - f1_m: 0.9085 - recall_m: 0.9062 - precision_m: 0.9108 - precision: 0.9108 - recall_6: 0.9062 - f1score: 0.9085 - val_loss: 0.2663 - val_acc: 0.8868 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1602 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1234 - val_prec_4: 0.1856 - val_recall_4: 0.1591 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8866 - val_recall_m: 0.8846 - val_precision_m: 0.8887 - val_precision: 0.8887 - val_recall_6: 0.8846 - val_f1score: 0.8866\n",
      "Epoch 266/300\n",
      "epoch:  265\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2082 - acc: 0.9045 - prec: 0.9869 - recall: 0.9933 - prec_1: 0.9838 - recall_1: 0.9854 - prec_2: 0.9905 - recall_2: 0.9873 - prec_3: 0.7396 - recall_3: 0.7468 - prec_4: 0.7364 - recall_4: 0.7354 - prec_5: 0.9891 - recall_5: 0.9930 - f1_m: 0.9039 - recall_m: 0.9010 - precision_m: 0.9069 - precision: 0.9069 - recall_6: 0.9010 - f1score: 0.9039 - val_loss: 0.2609 - val_acc: 0.8873 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1414 - val_prec_4: 0.1878 - val_recall_4: 0.1492 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8877 - val_recall_m: 0.8848 - val_precision_m: 0.8906 - val_precision: 0.8906 - val_recall_6: 0.8848 - val_f1score: 0.8877\n",
      "Epoch 267/300\n",
      "epoch:  266\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2077 - acc: 0.9092 - prec: 0.9907 - recall: 0.9926 - prec_1: 0.9800 - recall_1: 0.9898 - prec_2: 0.9901 - recall_2: 0.9845 - prec_3: 0.7535 - recall_3: 0.7569 - prec_4: 0.7535 - recall_4: 0.7597 - prec_5: 0.9890 - recall_5: 0.9915 - f1_m: 0.9085 - recall_m: 0.9059 - precision_m: 0.9112 - precision: 0.9112 - recall_6: 0.9059 - f1score: 0.9085 - val_loss: 0.2618 - val_acc: 0.8888 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1354 - val_prec_4: 0.1873 - val_recall_4: 0.1564 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8881 - val_recall_m: 0.8856 - val_precision_m: 0.8907 - val_precision: 0.8907 - val_recall_6: 0.8856 - val_f1score: 0.8881\n",
      "Epoch 268/300\n",
      "epoch:  267\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2099 - acc: 0.9040 - prec: 0.9885 - recall: 0.9900 - prec_1: 0.9868 - recall_1: 0.9850 - prec_2: 0.9859 - recall_2: 0.9903 - prec_3: 0.7328 - recall_3: 0.7502 - prec_4: 0.7475 - recall_4: 0.7383 - prec_5: 0.9878 - recall_5: 0.9909 - f1_m: 0.9045 - recall_m: 0.9019 - precision_m: 0.9072 - precision: 0.9072 - recall_6: 0.9019 - f1score: 0.9045 - val_loss: 0.2636 - val_acc: 0.8853 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1590 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1393 - val_prec_4: 0.1873 - val_recall_4: 0.1451 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8853 - val_recall_m: 0.8823 - val_precision_m: 0.8883 - val_precision: 0.8883 - val_recall_6: 0.8823 - val_f1score: 0.8853\n",
      "Epoch 269/300\n",
      "epoch:  268\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2051 - acc: 0.9065 - prec: 0.9889 - recall: 0.9921 - prec_1: 0.9810 - recall_1: 0.9894 - prec_2: 0.9921 - recall_2: 0.9845 - prec_3: 0.7447 - recall_3: 0.7489 - prec_4: 0.7463 - recall_4: 0.7457 - prec_5: 0.9919 - recall_5: 0.9938 - f1_m: 0.9060 - recall_m: 0.9031 - precision_m: 0.9090 - precision: 0.9090 - recall_6: 0.9031 - f1score: 0.9060 - val_loss: 0.2618 - val_acc: 0.8913 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1459 - val_prec_4: 0.1905 - val_recall_4: 0.1489 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8916 - val_recall_m: 0.8891 - val_precision_m: 0.8942 - val_precision: 0.8942 - val_recall_6: 0.8891 - val_f1score: 0.8916\n",
      "Epoch 270/300\n",
      "epoch:  269\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2084 - acc: 0.9064 - prec: 0.9841 - recall: 0.9897 - prec_1: 0.9790 - recall_1: 0.9845 - prec_2: 0.9900 - recall_2: 0.9860 - prec_3: 0.7444 - recall_3: 0.7507 - prec_4: 0.7493 - recall_4: 0.7489 - prec_5: 0.9834 - recall_5: 0.9893 - f1_m: 0.9058 - recall_m: 0.9024 - precision_m: 0.9092 - precision: 0.9092 - recall_6: 0.9024 - f1score: 0.9058 - val_loss: 0.2658 - val_acc: 0.8883 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1426 - val_prec_4: 0.1919 - val_recall_4: 0.1502 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8879 - val_recall_m: 0.8856 - val_precision_m: 0.8903 - val_precision: 0.8903 - val_recall_6: 0.8856 - val_f1score: 0.8879\n",
      "Epoch 271/300\n",
      "epoch:  270\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2059 - acc: 0.9070 - prec: 0.9911 - recall: 0.9954 - prec_1: 0.9826 - recall_1: 0.9940 - prec_2: 0.9885 - recall_2: 0.9880 - prec_3: 0.7451 - recall_3: 0.7341 - prec_4: 0.7430 - recall_4: 0.7587 - prec_5: 0.9946 - recall_5: 0.9882 - f1_m: 0.9070 - recall_m: 0.9040 - precision_m: 0.9100 - precision: 0.9100 - recall_6: 0.9040 - f1score: 0.9070 - val_loss: 0.2614 - val_acc: 0.8913 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1410 - val_prec_4: 0.1898 - val_recall_4: 0.1547 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8908 - val_recall_m: 0.8883 - val_precision_m: 0.8933 - val_precision: 0.8933 - val_recall_6: 0.8883 - val_f1score: 0.8908\n",
      "Epoch 272/300\n",
      "epoch:  271\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2018 - acc: 0.9094 - prec: 0.9886 - recall: 0.9932 - prec_1: 0.9855 - recall_1: 0.9837 - prec_2: 0.9884 - recall_2: 0.9887 - prec_3: 0.7569 - recall_3: 0.7525 - prec_4: 0.7475 - recall_4: 0.7652 - prec_5: 0.9891 - recall_5: 0.9941 - f1_m: 0.9081 - recall_m: 0.9057 - precision_m: 0.9105 - precision: 0.9105 - recall_6: 0.9057 - f1score: 0.9081 - val_loss: 0.2622 - val_acc: 0.8838 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1901 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1395 - val_prec_4: 0.1873 - val_recall_4: 0.1451 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8843 - val_recall_m: 0.8821 - val_precision_m: 0.8866 - val_precision: 0.8866 - val_recall_6: 0.8821 - val_f1score: 0.8843\n",
      "Epoch 273/300\n",
      "epoch:  272\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2052 - acc: 0.9092 - prec: 0.9882 - recall: 0.9924 - prec_1: 0.9798 - recall_1: 0.9898 - prec_2: 0.9913 - recall_2: 0.9868 - prec_3: 0.7459 - recall_3: 0.7569 - prec_4: 0.7532 - recall_4: 0.7537 - prec_5: 0.9918 - recall_5: 0.9934 - f1_m: 0.9089 - recall_m: 0.9057 - precision_m: 0.9121 - precision: 0.9121 - recall_6: 0.9057 - f1score: 0.9089 - val_loss: 0.2613 - val_acc: 0.8871 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1365 - val_prec_4: 0.1873 - val_recall_4: 0.1532 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8869 - val_recall_m: 0.8846 - val_precision_m: 0.8894 - val_precision: 0.8894 - val_recall_6: 0.8846 - val_f1score: 0.8869\n",
      "Epoch 274/300\n",
      "epoch:  273\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.2032 - acc: 0.9090 - prec: 0.9888 - recall: 0.9934 - prec_1: 0.9838 - recall_1: 0.9857 - prec_2: 0.9875 - recall_2: 0.9854 - prec_3: 0.7558 - recall_3: 0.7563 - prec_4: 0.7527 - recall_4: 0.7600 - prec_5: 0.9896 - recall_5: 0.9915 - f1_m: 0.9089 - recall_m: 0.9064 - precision_m: 0.9115 - precision: 0.9115 - recall_6: 0.9064 - f1score: 0.9089 - val_loss: 0.2605 - val_acc: 0.8891 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1278 - val_prec_4: 0.1869 - val_recall_4: 0.1639 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8891 - val_recall_m: 0.8871 - val_precision_m: 0.8913 - val_precision: 0.8913 - val_recall_6: 0.8871 - val_f1score: 0.8891\n",
      "Epoch 275/300\n",
      "epoch:  274\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2088 - acc: 0.9089 - prec: 0.9903 - recall: 0.9933 - prec_1: 0.9836 - recall_1: 0.9847 - prec_2: 0.9878 - recall_2: 0.9876 - prec_3: 0.7500 - recall_3: 0.7504 - prec_4: 0.7510 - recall_4: 0.7529 - prec_5: 0.9894 - recall_5: 0.9950 - f1_m: 0.9080 - recall_m: 0.9044 - precision_m: 0.9118 - precision: 0.9118 - recall_6: 0.9044 - f1score: 0.9080 - val_loss: 0.2617 - val_acc: 0.8901 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1330 - val_prec_4: 0.1866 - val_recall_4: 0.1597 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8900 - val_recall_m: 0.8873 - val_precision_m: 0.8927 - val_precision: 0.8927 - val_recall_6: 0.8873 - val_f1score: 0.8900\n",
      "Epoch 276/300\n",
      "epoch:  275\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2009 - acc: 0.9075 - prec: 0.9902 - recall: 0.9947 - prec_1: 0.9814 - recall_1: 0.9911 - prec_2: 0.9916 - recall_2: 0.9843 - prec_3: 0.7412 - recall_3: 0.7493 - prec_4: 0.7419 - recall_4: 0.7441 - prec_5: 0.9931 - recall_5: 0.9941 - f1_m: 0.9065 - recall_m: 0.9036 - precision_m: 0.9094 - precision: 0.9094 - recall_6: 0.9036 - f1score: 0.9065 - val_loss: 0.2615 - val_acc: 0.8926 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1590 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1429 - val_prec_4: 0.1905 - val_recall_4: 0.1539 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8923 - val_recall_m: 0.8901 - val_precision_m: 0.8947 - val_precision: 0.8947 - val_recall_6: 0.8901 - val_f1score: 0.8923\n",
      "Epoch 277/300\n",
      "epoch:  276\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2048 - acc: 0.9091 - prec: 0.9883 - recall: 0.9945 - prec_1: 0.9742 - recall_1: 0.9860 - prec_2: 0.9907 - recall_2: 0.9795 - prec_3: 0.7486 - recall_3: 0.7614 - prec_4: 0.7619 - recall_4: 0.7598 - prec_5: 0.9882 - recall_5: 0.9913 - f1_m: 0.9094 - recall_m: 0.9069 - precision_m: 0.9121 - precision: 0.9121 - recall_6: 0.9069 - f1score: 0.9094 - val_loss: 0.2627 - val_acc: 0.8918 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1590 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1447 - val_prec_4: 0.1911 - val_recall_4: 0.1514 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8911 - val_recall_m: 0.8883 - val_precision_m: 0.8939 - val_precision: 0.8939 - val_recall_6: 0.8883 - val_f1score: 0.8911\n",
      "Epoch 278/300\n",
      "epoch:  277\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2047 - acc: 0.9097 - prec: 0.9918 - recall: 0.9934 - prec_1: 0.9801 - recall_1: 0.9888 - prec_2: 0.9879 - recall_2: 0.9863 - prec_3: 0.7574 - recall_3: 0.7602 - prec_4: 0.7552 - recall_4: 0.7632 - prec_5: 0.9904 - recall_5: 0.9902 - f1_m: 0.9101 - recall_m: 0.9074 - precision_m: 0.9129 - precision: 0.9129 - recall_6: 0.9074 - f1score: 0.9101 - val_loss: 0.2611 - val_acc: 0.8926 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1432 - val_prec_4: 0.1905 - val_recall_4: 0.1534 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8923 - val_recall_m: 0.8896 - val_precision_m: 0.8951 - val_precision: 0.8951 - val_recall_6: 0.8896 - val_f1score: 0.8923\n",
      "Epoch 279/300\n",
      "epoch:  278\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2055 - acc: 0.9090 - prec: 0.9907 - recall: 0.9917 - prec_1: 0.9830 - recall_1: 0.9853 - prec_2: 0.9876 - recall_2: 0.9859 - prec_3: 0.7568 - recall_3: 0.7602 - prec_4: 0.7506 - recall_4: 0.7619 - prec_5: 0.9874 - recall_5: 0.9916 - f1_m: 0.9084 - recall_m: 0.9056 - precision_m: 0.9112 - precision: 0.9112 - recall_6: 0.9056 - f1score: 0.9084 - val_loss: 0.2600 - val_acc: 0.8906 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1369 - val_prec_4: 0.1882 - val_recall_4: 0.1569 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8905 - val_recall_m: 0.8886 - val_precision_m: 0.8925 - val_precision: 0.8925 - val_recall_6: 0.8886 - val_f1score: 0.8905\n",
      "Epoch 280/300\n",
      "epoch:  279\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2037 - acc: 0.9090 - prec: 0.9871 - recall: 0.9928 - prec_1: 0.9817 - recall_1: 0.9908 - prec_2: 0.9875 - recall_2: 0.9876 - prec_3: 0.7535 - recall_3: 0.7542 - prec_4: 0.7553 - recall_4: 0.7602 - prec_5: 0.9959 - recall_5: 0.9891 - f1_m: 0.9083 - recall_m: 0.9050 - precision_m: 0.9116 - precision: 0.9116 - recall_6: 0.9050 - f1score: 0.9083 - val_loss: 0.2602 - val_acc: 0.8888 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1602 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1430 - val_prec_4: 0.1892 - val_recall_4: 0.1489 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8897 - val_recall_m: 0.8871 - val_precision_m: 0.8925 - val_precision: 0.8925 - val_recall_6: 0.8871 - val_f1score: 0.8897\n",
      "Epoch 281/300\n",
      "epoch:  280\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2041 - acc: 0.9047 - prec: 0.9881 - recall: 0.9923 - prec_1: 0.9810 - recall_1: 0.9849 - prec_2: 0.9909 - recall_2: 0.9890 - prec_3: 0.7345 - recall_3: 0.7391 - prec_4: 0.7428 - recall_4: 0.7425 - prec_5: 0.9863 - recall_5: 0.9909 - f1_m: 0.9039 - recall_m: 0.9014 - precision_m: 0.9066 - precision: 0.9066 - recall_6: 0.9014 - f1score: 0.9039 - val_loss: 0.2631 - val_acc: 0.8836 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1463 - val_prec_4: 0.1898 - val_recall_4: 0.1361 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8839 - val_recall_m: 0.8816 - val_precision_m: 0.8863 - val_precision: 0.8863 - val_recall_6: 0.8816 - val_f1score: 0.8839\n",
      "Epoch 282/300\n",
      "epoch:  281\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2048 - acc: 0.9064 - prec: 0.9855 - recall: 0.9934 - prec_1: 0.9823 - recall_1: 0.9868 - prec_2: 0.9876 - recall_2: 0.9860 - prec_3: 0.7402 - recall_3: 0.7529 - prec_4: 0.7435 - recall_4: 0.7358 - prec_5: 0.9898 - recall_5: 0.9909 - f1_m: 0.9059 - recall_m: 0.9029 - precision_m: 0.9090 - precision: 0.9090 - recall_6: 0.9029 - f1score: 0.9059 - val_loss: 0.2605 - val_acc: 0.8908 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1589 - val_prec_2: 0.1901 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1344 - val_prec_4: 0.1873 - val_recall_4: 0.1599 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8906 - val_recall_m: 0.8886 - val_precision_m: 0.8927 - val_precision: 0.8927 - val_recall_6: 0.8886 - val_f1score: 0.8906\n",
      "Epoch 283/300\n",
      "epoch:  282\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2053 - acc: 0.9064 - prec: 0.9880 - recall: 0.9939 - prec_1: 0.9875 - recall_1: 0.9906 - prec_2: 0.9885 - recall_2: 0.9887 - prec_3: 0.7421 - recall_3: 0.7382 - prec_4: 0.7362 - recall_4: 0.7478 - prec_5: 0.9943 - recall_5: 0.9945 - f1_m: 0.9062 - recall_m: 0.9037 - precision_m: 0.9087 - precision: 0.9087 - recall_6: 0.9037 - f1score: 0.9062 - val_loss: 0.2605 - val_acc: 0.8891 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1587 - val_prec_2: 0.1901 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1423 - val_prec_4: 0.1898 - val_recall_4: 0.1509 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8893 - val_recall_m: 0.8866 - val_precision_m: 0.8921 - val_precision: 0.8921 - val_recall_6: 0.8866 - val_f1score: 0.8893\n",
      "Epoch 284/300\n",
      "epoch:  283\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2105 - acc: 0.9050 - prec: 0.9850 - recall: 0.9939 - prec_1: 0.9833 - recall_1: 0.9846 - prec_2: 0.9853 - recall_2: 0.9866 - prec_3: 0.7398 - recall_3: 0.7428 - prec_4: 0.7459 - recall_4: 0.7502 - prec_5: 0.9829 - recall_5: 0.9906 - f1_m: 0.9032 - recall_m: 0.8998 - precision_m: 0.9066 - precision: 0.9066 - recall_6: 0.8998 - f1score: 0.9032 - val_loss: 0.2610 - val_acc: 0.8888 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1584 - val_prec_2: 0.1901 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1382 - val_prec_4: 0.1873 - val_recall_4: 0.1547 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8888 - val_recall_m: 0.8868 - val_precision_m: 0.8909 - val_precision: 0.8909 - val_recall_6: 0.8868 - val_f1score: 0.8888\n",
      "Epoch 285/300\n",
      "epoch:  284\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 658us/step - loss: 0.2066 - acc: 0.9052 - prec: 0.9894 - recall: 0.9933 - prec_1: 0.9844 - recall_1: 0.9857 - prec_2: 0.9851 - recall_2: 0.9890 - prec_3: 0.7401 - recall_3: 0.7461 - prec_4: 0.7417 - recall_4: 0.7437 - prec_5: 0.9903 - recall_5: 0.9904 - f1_m: 0.9049 - recall_m: 0.9019 - precision_m: 0.9080 - precision: 0.9080 - recall_6: 0.9019 - f1score: 0.9049 - val_loss: 0.2625 - val_acc: 0.8846 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1587 - val_prec_2: 0.1901 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1385 - val_prec_4: 0.1873 - val_recall_4: 0.1456 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8848 - val_recall_m: 0.8818 - val_precision_m: 0.8878 - val_precision: 0.8878 - val_recall_6: 0.8818 - val_f1score: 0.8848\n",
      "Epoch 286/300\n",
      "epoch:  285\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2051 - acc: 0.9109 - prec: 0.9902 - recall: 0.9931 - prec_1: 0.9768 - recall_1: 0.9897 - prec_2: 0.9905 - recall_2: 0.9852 - prec_3: 0.7626 - recall_3: 0.7570 - prec_4: 0.7566 - recall_4: 0.7654 - prec_5: 0.9918 - recall_5: 0.9880 - f1_m: 0.9107 - recall_m: 0.9080 - precision_m: 0.9135 - precision: 0.9135 - recall_6: 0.9080 - f1score: 0.9107 - val_loss: 0.2611 - val_acc: 0.8911 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1443 - val_prec_4: 0.1898 - val_recall_4: 0.1499 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8910 - val_recall_m: 0.8883 - val_precision_m: 0.8937 - val_precision: 0.8937 - val_recall_6: 0.8883 - val_f1score: 0.8910\n",
      "Epoch 287/300\n",
      "epoch:  286\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2091 - acc: 0.9081 - prec: 0.9878 - recall: 0.9938 - prec_1: 0.9796 - recall_1: 0.9922 - prec_2: 0.9849 - recall_2: 0.9864 - prec_3: 0.7547 - recall_3: 0.7424 - prec_4: 0.7424 - recall_4: 0.7658 - prec_5: 0.9921 - recall_5: 0.9910 - f1_m: 0.9073 - recall_m: 0.9046 - precision_m: 0.9101 - precision: 0.9101 - recall_6: 0.9046 - f1score: 0.9073 - val_loss: 0.2604 - val_acc: 0.8906 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1412 - val_prec_4: 0.1892 - val_recall_4: 0.1529 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8907 - val_recall_m: 0.8883 - val_precision_m: 0.8932 - val_precision: 0.8932 - val_recall_6: 0.8883 - val_f1score: 0.8907\n",
      "Epoch 288/300\n",
      "epoch:  287\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2058 - acc: 0.9089 - prec: 0.9841 - recall: 0.9910 - prec_1: 0.9826 - recall_1: 0.9869 - prec_2: 0.9876 - recall_2: 0.9855 - prec_3: 0.7530 - recall_3: 0.7619 - prec_4: 0.7629 - recall_4: 0.7591 - prec_5: 0.9896 - recall_5: 0.9927 - f1_m: 0.9084 - recall_m: 0.9054 - precision_m: 0.9115 - precision: 0.9115 - recall_6: 0.9054 - f1score: 0.9084 - val_loss: 0.2612 - val_acc: 0.8901 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1472 - val_prec_4: 0.1905 - val_recall_4: 0.1457 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8899 - val_recall_m: 0.8873 - val_precision_m: 0.8926 - val_precision: 0.8926 - val_recall_6: 0.8873 - val_f1score: 0.8899\n",
      "Epoch 289/300\n",
      "epoch:  288\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2033 - acc: 0.9089 - prec: 0.9860 - recall: 0.9907 - prec_1: 0.9898 - recall_1: 0.9848 - prec_2: 0.9872 - recall_2: 0.9871 - prec_3: 0.7483 - recall_3: 0.7503 - prec_4: 0.7512 - recall_4: 0.7612 - prec_5: 0.9821 - recall_5: 0.9935 - f1_m: 0.9089 - recall_m: 0.9061 - precision_m: 0.9117 - precision: 0.9117 - recall_6: 0.9061 - f1score: 0.9089 - val_loss: 0.2614 - val_acc: 0.8923 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1605 - val_prec_2: 0.1919 - val_recall_2: 0.1807 - val_prec_3: 0.1759 - val_recall_3: 0.1462 - val_prec_4: 0.1905 - val_recall_4: 0.1494 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8919 - val_recall_m: 0.8891 - val_precision_m: 0.8949 - val_precision: 0.8949 - val_recall_6: 0.8891 - val_f1score: 0.8919\n",
      "Epoch 290/300\n",
      "epoch:  289\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2038 - acc: 0.9086 - prec: 0.9878 - recall: 0.9947 - prec_1: 0.9822 - recall_1: 0.9908 - prec_2: 0.9914 - recall_2: 0.9862 - prec_3: 0.7441 - recall_3: 0.7566 - prec_4: 0.7546 - recall_4: 0.7471 - prec_5: 0.9924 - recall_5: 0.9934 - f1_m: 0.9081 - recall_m: 0.9049 - precision_m: 0.9115 - precision: 0.9115 - recall_6: 0.9049 - f1score: 0.9081 - val_loss: 0.2599 - val_acc: 0.8901 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1602 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1369 - val_prec_4: 0.1882 - val_recall_4: 0.1564 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8907 - val_recall_m: 0.8886 - val_precision_m: 0.8929 - val_precision: 0.8929 - val_recall_6: 0.8886 - val_f1score: 0.8907\n",
      "Epoch 291/300\n",
      "epoch:  290\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2074 - acc: 0.9039 - prec: 0.9891 - recall: 0.9938 - prec_1: 0.9777 - recall_1: 0.9860 - prec_2: 0.9908 - recall_2: 0.9860 - prec_3: 0.7415 - recall_3: 0.7350 - prec_4: 0.7407 - recall_4: 0.7549 - prec_5: 0.9883 - recall_5: 0.9916 - f1_m: 0.9029 - recall_m: 0.8993 - precision_m: 0.9066 - precision: 0.9066 - recall_6: 0.8993 - f1score: 0.9029 - val_loss: 0.2614 - val_acc: 0.8886 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1807 - val_prec_3: 0.1759 - val_recall_3: 0.1469 - val_prec_4: 0.1905 - val_recall_4: 0.1457 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8887 - val_recall_m: 0.8861 - val_precision_m: 0.8914 - val_precision: 0.8914 - val_recall_6: 0.8861 - val_f1score: 0.8887\n",
      "Epoch 292/300\n",
      "epoch:  291\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2060 - acc: 0.9081 - prec: 0.9891 - recall: 0.9915 - prec_1: 0.9722 - recall_1: 0.9898 - prec_2: 0.9890 - recall_2: 0.9836 - prec_3: 0.7461 - recall_3: 0.7526 - prec_4: 0.7500 - recall_4: 0.7511 - prec_5: 0.9897 - recall_5: 0.9884 - f1_m: 0.9077 - recall_m: 0.9042 - precision_m: 0.9113 - precision: 0.9113 - recall_6: 0.9042 - f1score: 0.9077 - val_loss: 0.2617 - val_acc: 0.8891 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1593 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1459 - val_prec_4: 0.1905 - val_recall_4: 0.1467 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.8887 - val_recall_m: 0.8866 - val_precision_m: 0.8909 - val_precision: 0.8909 - val_recall_6: 0.8866 - val_f1score: 0.8887\n",
      "Epoch 293/300\n",
      "epoch:  292\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2036 - acc: 0.9080 - prec: 0.9880 - recall: 0.9935 - prec_1: 0.9817 - recall_1: 0.9838 - prec_2: 0.9850 - recall_2: 0.9887 - prec_3: 0.7514 - recall_3: 0.7500 - prec_4: 0.7513 - recall_4: 0.7567 - prec_5: 0.9895 - recall_5: 0.9925 - f1_m: 0.9078 - recall_m: 0.9054 - precision_m: 0.9103 - precision: 0.9103 - recall_6: 0.9054 - f1score: 0.9078 - val_loss: 0.2605 - val_acc: 0.8906 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1595 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1389 - val_prec_4: 0.1882 - val_recall_4: 0.1552 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8901 - val_recall_m: 0.8881 - val_precision_m: 0.8922 - val_precision: 0.8922 - val_recall_6: 0.8881 - val_f1score: 0.8901\n",
      "Epoch 294/300\n",
      "epoch:  293\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 652us/step - loss: 0.2051 - acc: 0.9075 - prec: 0.9844 - recall: 0.9919 - prec_1: 0.9835 - recall_1: 0.9876 - prec_2: 0.9916 - recall_2: 0.9892 - prec_3: 0.7512 - recall_3: 0.7591 - prec_4: 0.7507 - recall_4: 0.7537 - prec_5: 0.9854 - recall_5: 0.9924 - f1_m: 0.9073 - recall_m: 0.9051 - precision_m: 0.9096 - precision: 0.9096 - recall_6: 0.9051 - f1score: 0.9073 - val_loss: 0.2616 - val_acc: 0.8923 - val_prec: 0.1759 - val_recall: 0.1705 - val_prec_1: 0.1756 - val_recall_1: 0.1590 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1434 - val_prec_4: 0.1905 - val_recall_4: 0.1539 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8923 - val_recall_m: 0.8901 - val_precision_m: 0.8946 - val_precision: 0.8946 - val_recall_6: 0.8901 - val_f1score: 0.8923\n",
      "Epoch 295/300\n",
      "epoch:  294\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2055 - acc: 0.9084 - prec: 0.9902 - recall: 0.9942 - prec_1: 0.9808 - recall_1: 0.9908 - prec_2: 0.9944 - recall_2: 0.9837 - prec_3: 0.7483 - recall_3: 0.7598 - prec_4: 0.7510 - recall_4: 0.7428 - prec_5: 0.9876 - recall_5: 0.9926 - f1_m: 0.9083 - recall_m: 0.9052 - precision_m: 0.9115 - precision: 0.9115 - recall_6: 0.9052 - f1score: 0.9083 - val_loss: 0.2613 - val_acc: 0.8893 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1590 - val_prec_2: 0.1919 - val_recall_2: 0.1812 - val_prec_3: 0.1759 - val_recall_3: 0.1282 - val_prec_4: 0.1873 - val_recall_4: 0.1649 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8896 - val_recall_m: 0.8871 - val_precision_m: 0.8922 - val_precision: 0.8922 - val_recall_6: 0.8871 - val_f1score: 0.8896\n",
      "Epoch 296/300\n",
      "epoch:  295\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2079 - acc: 0.9084 - prec: 0.9891 - recall: 0.9920 - prec_1: 0.9804 - recall_1: 0.9889 - prec_2: 0.9898 - recall_2: 0.9865 - prec_3: 0.7379 - recall_3: 0.7622 - prec_4: 0.7613 - recall_4: 0.7459 - prec_5: 0.9933 - recall_5: 0.9907 - f1_m: 0.9080 - recall_m: 0.9046 - precision_m: 0.9114 - precision: 0.9114 - recall_6: 0.9046 - f1score: 0.9080 - val_loss: 0.2615 - val_acc: 0.8916 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1592 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1452 - val_prec_4: 0.1905 - val_recall_4: 0.1504 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8912 - val_recall_m: 0.8883 - val_precision_m: 0.8942 - val_precision: 0.8942 - val_recall_6: 0.8883 - val_f1score: 0.8912\n",
      "Epoch 297/300\n",
      "epoch:  296\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2097 - acc: 0.9031 - prec: 0.9893 - recall: 0.9936 - prec_1: 0.9759 - recall_1: 0.9879 - prec_2: 0.9917 - recall_2: 0.9799 - prec_3: 0.7388 - recall_3: 0.7422 - prec_4: 0.7357 - recall_4: 0.7428 - prec_5: 0.9866 - recall_5: 0.9906 - f1_m: 0.9031 - recall_m: 0.9001 - precision_m: 0.9061 - precision: 0.9061 - recall_6: 0.9001 - f1score: 0.9031 - val_loss: 0.2604 - val_acc: 0.8906 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1590 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1345 - val_prec_4: 0.1878 - val_recall_4: 0.1597 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8910 - val_recall_m: 0.8888 - val_precision_m: 0.8932 - val_precision: 0.8932 - val_recall_6: 0.8888 - val_f1score: 0.8910\n",
      "Epoch 298/300\n",
      "epoch:  297\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2048 - acc: 0.9086 - prec: 0.9870 - recall: 0.9950 - prec_1: 0.9826 - recall_1: 0.9884 - prec_2: 0.9907 - recall_2: 0.9833 - prec_3: 0.7503 - recall_3: 0.7542 - prec_4: 0.7553 - recall_4: 0.7556 - prec_5: 0.9861 - recall_5: 0.9936 - f1_m: 0.9081 - recall_m: 0.9052 - precision_m: 0.9110 - precision: 0.9110 - recall_6: 0.9052 - f1score: 0.9081 - val_loss: 0.2627 - val_acc: 0.8886 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1810 - val_prec_3: 0.1759 - val_recall_3: 0.1494 - val_prec_4: 0.1919 - val_recall_4: 0.1434 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8881 - val_recall_m: 0.8851 - val_precision_m: 0.8913 - val_precision: 0.8913 - val_recall_6: 0.8851 - val_f1score: 0.8881\n",
      "Epoch 299/300\n",
      "epoch:  298\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2065 - acc: 0.9044 - prec: 0.9913 - recall: 0.9936 - prec_1: 0.9800 - recall_1: 0.9894 - prec_2: 0.9907 - recall_2: 0.9850 - prec_3: 0.7355 - recall_3: 0.7520 - prec_4: 0.7422 - recall_4: 0.7348 - prec_5: 0.9888 - recall_5: 0.9891 - f1_m: 0.9037 - recall_m: 0.9007 - precision_m: 0.9068 - precision: 0.9068 - recall_6: 0.9007 - f1score: 0.9037 - val_loss: 0.2613 - val_acc: 0.8851 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1600 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1419 - val_prec_4: 0.1878 - val_recall_4: 0.1457 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8854 - val_recall_m: 0.8828 - val_precision_m: 0.8880 - val_precision: 0.8880 - val_recall_6: 0.8828 - val_f1score: 0.8854\n",
      "Epoch 300/300\n",
      "epoch:  299\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2076 - acc: 0.9076 - prec: 0.9878 - recall: 0.9887 - prec_1: 0.9829 - recall_1: 0.9871 - prec_2: 0.9850 - recall_2: 0.9883 - prec_3: 0.7441 - recall_3: 0.7532 - prec_4: 0.7428 - recall_4: 0.7484 - prec_5: 0.9895 - recall_5: 0.9903 - f1_m: 0.9074 - recall_m: 0.9037 - precision_m: 0.9111 - precision: 0.9111 - recall_6: 0.9037 - f1score: 0.9074 - val_loss: 0.2615 - val_acc: 0.8868 - val_prec: 0.1759 - val_recall: 0.1710 - val_prec_1: 0.1756 - val_recall_1: 0.1597 - val_prec_2: 0.1919 - val_recall_2: 0.1815 - val_prec_3: 0.1759 - val_recall_3: 0.1353 - val_prec_4: 0.1873 - val_recall_4: 0.1544 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8859 - val_recall_m: 0.8833 - val_precision_m: 0.8886 - val_precision: 0.8886 - val_recall_6: 0.8833 - val_f1score: 0.8859\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        (None, 500, 1)            0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_7 (ZeroPaddin (None, 506, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1/conv (Conv1D)          (None, 500, 24)           192       \n",
      "_________________________________________________________________\n",
      "1_conv (Conv1D)              (None, 500, 48)           1200      \n",
      "_________________________________________________________________\n",
      "1_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "1_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "2_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "2_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "2_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "3_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "3_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "3_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "4_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "4_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "4_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "5_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "5_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "5_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "6_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "6_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "6_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "7_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "7_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "7_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "8_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "8_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "8_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "9_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "9_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "9_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "10_conv (Conv1D)             (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "10_bn (BatchNormalization)   (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "10_relu (Activation)         (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "avg_pool (GlobalAveragePooli (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 6)                 294       \n",
      "=================================================================\n",
      "Total params: 24,774\n",
      "Trainable params: 23,814\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7998 samples, validate on 4002 samples\n",
      "Epoch 1/300\n",
      "epoch:  0\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 15s 2ms/step - loss: 1.2762 - acc: 0.5233 - prec: 0.6841 - recall: 0.8530 - prec_1: 0.3067 - recall_1: 0.1744 - prec_2: 0.3999 - recall_2: 0.3988 - prec_3: 0.5702 - recall_3: 0.5366 - prec_4: 0.5343 - recall_4: 0.6036 - prec_5: 0.4327 - recall_5: 0.6041 - f1_m: 0.1140 - recall_m: 0.0673 - precision_m: 0.4617 - precision: 0.4617 - recall_6: 0.0673 - f1score: 0.1140 - val_loss: 1.8398 - val_acc: 0.1822 - val_prec: 0.1439 - val_recall: 0.0155 - val_prec_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - val_prec_2: 0.1673 - val_recall_2: 0.1919 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0000e+00 - val_recall_5: 0.0000e+00 - val_f1_m: 0.1372 - val_recall_m: 0.1187 - val_precision_m: 0.1776 - val_precision: 0.1776 - val_recall_6: 0.1187 - val_f1score: 0.1372\n",
      "Epoch 2/300\n",
      "epoch:  1\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.9334 - acc: 0.6588 - prec: 0.7825 - recall: 0.9440 - prec_1: 0.6000 - recall_1: 0.3574 - prec_2: 0.7818 - recall_2: 0.7511 - prec_3: 0.6182 - recall_3: 0.5985 - prec_4: 0.6063 - recall_4: 0.6103 - prec_5: 0.5698 - recall_5: 0.7238 - f1_m: 0.4243 - recall_m: 0.2944 - precision_m: 0.7759 - precision: 0.7759 - recall_6: 0.2944 - f1score: 0.4243 - val_loss: 1.9340 - val_acc: 0.2929 - val_prec: 0.1759 - val_recall: 0.1147 - val_prec_1: 0.0800 - val_recall_1: 0.0027 - val_prec_2: 0.1782 - val_recall_2: 0.1881 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0885 - val_recall_5: 0.0141 - val_f1_m: 0.1560 - val_recall_m: 0.1434 - val_precision_m: 0.2004 - val_precision: 0.2004 - val_recall_6: 0.1434 - val_f1score: 0.1560\n",
      "Epoch 3/300\n",
      "epoch:  2\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.8349 - acc: 0.6773 - prec: 0.8424 - recall: 0.9445 - prec_1: 0.5923 - recall_1: 0.4678 - prec_2: 0.7867 - recall_2: 0.7513 - prec_3: 0.6472 - recall_3: 0.5659 - prec_4: 0.6073 - recall_4: 0.6652 - prec_5: 0.6134 - recall_5: 0.7132 - f1_m: 0.5114 - recall_m: 0.3812 - precision_m: 0.7848 - precision: 0.7848 - recall_6: 0.3812 - f1score: 0.5114 - val_loss: 1.9917 - val_acc: 0.4478 - val_prec: 0.1759 - val_recall: 0.1470 - val_prec_1: 0.1700 - val_recall_1: 0.0858 - val_prec_2: 0.1770 - val_recall_2: 0.1570 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0858 - val_f1_m: 0.2544 - val_recall_m: 0.2146 - val_precision_m: 0.3643 - val_precision: 0.3643 - val_recall_6: 0.2146 - val_f1score: 0.2544\n",
      "Epoch 4/300\n",
      "epoch:  3\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.7786 - acc: 0.6832 - prec: 0.8493 - recall: 0.9394 - prec_1: 0.6081 - recall_1: 0.4893 - prec_2: 0.8233 - recall_2: 0.7459 - prec_3: 0.6072 - recall_3: 0.5700 - prec_4: 0.5910 - recall_4: 0.6205 - prec_5: 0.6253 - recall_5: 0.7688 - f1_m: 0.5651 - recall_m: 0.4485 - precision_m: 0.7704 - precision: 0.7704 - recall_6: 0.4485 - f1score: 0.5651 - val_loss: 2.0747 - val_acc: 0.4943 - val_prec: 0.1759 - val_recall: 0.1594 - val_prec_1: 0.1716 - val_recall_1: 0.1047 - val_prec_2: 0.1773 - val_recall_2: 0.1421 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1174 - val_f1_m: 0.3357 - val_recall_m: 0.2904 - val_precision_m: 0.4671 - val_precision: 0.4671 - val_recall_6: 0.2904 - val_f1score: 0.3357\n",
      "Epoch 5/300\n",
      "epoch:  4\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.7377 - acc: 0.7061 - prec: 0.8742 - recall: 0.9385 - prec_1: 0.6240 - recall_1: 0.5488 - prec_2: 0.8323 - recall_2: 0.7599 - prec_3: 0.6482 - recall_3: 0.5994 - prec_4: 0.6229 - recall_4: 0.6648 - prec_5: 0.6558 - recall_5: 0.7628 - f1_m: 0.6182 - recall_m: 0.5095 - precision_m: 0.7902 - precision: 0.7902 - recall_6: 0.5095 - f1score: 0.6182 - val_loss: 1.8285 - val_acc: 0.4543 - val_prec: 0.1759 - val_recall: 0.1466 - val_prec_1: 0.1599 - val_recall_1: 0.0699 - val_prec_2: 0.1774 - val_recall_2: 0.1594 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1050 - val_f1_m: 0.3268 - val_recall_m: 0.2791 - val_precision_m: 0.4514 - val_precision: 0.4514 - val_recall_6: 0.2791 - val_f1score: 0.3268\n",
      "Epoch 6/300\n",
      "epoch:  5\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.7047 - acc: 0.7124 - prec: 0.8839 - recall: 0.9442 - prec_1: 0.6375 - recall_1: 0.5152 - prec_2: 0.8221 - recall_2: 0.7982 - prec_3: 0.6371 - recall_3: 0.5858 - prec_4: 0.6136 - recall_4: 0.6571 - prec_5: 0.6750 - recall_5: 0.8079 - f1_m: 0.6408 - recall_m: 0.5430 - precision_m: 0.7853 - precision: 0.7853 - recall_6: 0.5430 - f1score: 0.6408 - val_loss: 2.1621 - val_acc: 0.4175 - val_prec: 0.1759 - val_recall: 0.1656 - val_prec_1: 0.1727 - val_recall_1: 0.1137 - val_prec_2: 0.1599 - val_recall_2: 0.0459 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1124 - val_f1_m: 0.2988 - val_recall_m: 0.2616 - val_precision_m: 0.4119 - val_precision: 0.4119 - val_recall_6: 0.2616 - val_f1score: 0.2988\n",
      "Epoch 7/300\n",
      "epoch:  6\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.6803 - acc: 0.7223 - prec: 0.8969 - recall: 0.9414 - prec_1: 0.6392 - recall_1: 0.6036 - prec_2: 0.8424 - recall_2: 0.7873 - prec_3: 0.6529 - recall_3: 0.5842 - prec_4: 0.6155 - recall_4: 0.6823 - prec_5: 0.7079 - recall_5: 0.7894 - f1_m: 0.6592 - recall_m: 0.5690 - precision_m: 0.7869 - precision: 0.7869 - recall_6: 0.5690 - f1score: 0.6592 - val_loss: 3.5933 - val_acc: 0.4815 - val_prec: 0.1759 - val_recall: 0.1623 - val_prec_1: 0.1741 - val_recall_1: 0.0918 - val_prec_2: 0.1764 - val_recall_2: 0.1304 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1228 - val_f1_m: 0.3826 - val_recall_m: 0.3468 - val_precision_m: 0.4615 - val_precision: 0.4615 - val_recall_6: 0.3468 - val_f1score: 0.3826\n",
      "Epoch 8/300\n",
      "epoch:  7\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.6638 - acc: 0.7262 - prec: 0.9027 - recall: 0.9385 - prec_1: 0.6524 - recall_1: 0.6155 - prec_2: 0.8272 - recall_2: 0.7942 - prec_3: 0.6300 - recall_3: 0.5754 - prec_4: 0.6030 - recall_4: 0.6614 - prec_5: 0.7332 - recall_5: 0.8103 - f1_m: 0.6731 - recall_m: 0.5898 - precision_m: 0.7869 - precision: 0.7869 - recall_6: 0.5898 - f1score: 0.6731 - val_loss: 3.0293 - val_acc: 0.4470 - val_prec: 0.1759 - val_recall: 0.1653 - val_prec_1: 0.1722 - val_recall_1: 0.1117 - val_prec_2: 0.1784 - val_recall_2: 0.1661 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0358 - val_f1_m: 0.3759 - val_recall_m: 0.3416 - val_precision_m: 0.4474 - val_precision: 0.4474 - val_recall_6: 0.3416 - val_f1score: 0.3759\n",
      "Epoch 9/300\n",
      "epoch:  8\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.6414 - acc: 0.7338 - prec: 0.9118 - recall: 0.9349 - prec_1: 0.6510 - recall_1: 0.6485 - prec_2: 0.8465 - recall_2: 0.7891 - prec_3: 0.6400 - recall_3: 0.5751 - prec_4: 0.6000 - recall_4: 0.6644 - prec_5: 0.7638 - recall_5: 0.8192 - f1_m: 0.6907 - recall_m: 0.6147 - precision_m: 0.7906 - precision: 0.7906 - recall_6: 0.6147 - f1score: 0.6907 - val_loss: 3.4389 - val_acc: 0.4458 - val_prec: 0.1759 - val_recall: 0.1641 - val_prec_1: 0.1731 - val_recall_1: 0.0901 - val_prec_2: 0.1779 - val_recall_2: 0.1607 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0605 - val_f1_m: 0.3473 - val_recall_m: 0.3091 - val_precision_m: 0.4644 - val_precision: 0.4644 - val_recall_6: 0.3091 - val_f1score: 0.3473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300\n",
      "epoch:  9\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.6285 - acc: 0.7414 - prec: 0.9046 - recall: 0.9431 - prec_1: 0.6892 - recall_1: 0.6351 - prec_2: 0.8460 - recall_2: 0.8154 - prec_3: 0.6452 - recall_3: 0.5904 - prec_4: 0.6089 - recall_4: 0.6588 - prec_5: 0.7603 - recall_5: 0.8466 - f1_m: 0.7001 - recall_m: 0.6298 - precision_m: 0.7899 - precision: 0.7899 - recall_6: 0.6298 - f1score: 0.7001 - val_loss: 3.3620 - val_acc: 0.3526 - val_prec: 0.1759 - val_recall: 0.1601 - val_prec_1: 0.1519 - val_recall_1: 0.0308 - val_prec_2: 0.1788 - val_recall_2: 0.1892 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0800 - val_recall_5: 0.0072 - val_f1_m: 0.3274 - val_recall_m: 0.3146 - val_precision_m: 0.3456 - val_precision: 0.3456 - val_recall_6: 0.3146 - val_f1score: 0.3274\n",
      "Epoch 11/300\n",
      "epoch:  10\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.6153 - acc: 0.7456 - prec: 0.9052 - recall: 0.9383 - prec_1: 0.6918 - recall_1: 0.6801 - prec_2: 0.8484 - recall_2: 0.8131 - prec_3: 0.6354 - recall_3: 0.5791 - prec_4: 0.5982 - recall_4: 0.6584 - prec_5: 0.8004 - recall_5: 0.8471 - f1_m: 0.7074 - recall_m: 0.6415 - precision_m: 0.7898 - precision: 0.7898 - recall_6: 0.6415 - f1score: 0.7074 - val_loss: 3.3746 - val_acc: 0.4658 - val_prec: 0.1759 - val_recall: 0.1611 - val_prec_1: 0.1739 - val_recall_1: 0.1109 - val_prec_2: 0.1793 - val_recall_2: 0.1724 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0576 - val_f1_m: 0.4451 - val_recall_m: 0.4190 - val_precision_m: 0.4786 - val_precision: 0.4786 - val_recall_6: 0.4190 - val_f1score: 0.4451\n",
      "Epoch 12/300\n",
      "epoch:  11\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.6020 - acc: 0.7494 - prec: 0.9140 - recall: 0.9381 - prec_1: 0.6928 - recall_1: 0.6737 - prec_2: 0.8499 - recall_2: 0.8178 - prec_3: 0.6411 - recall_3: 0.5838 - prec_4: 0.6073 - recall_4: 0.6638 - prec_5: 0.7913 - recall_5: 0.8530 - f1_m: 0.7194 - recall_m: 0.6572 - precision_m: 0.7968 - precision: 0.7968 - recall_6: 0.6572 - f1score: 0.7194 - val_loss: 3.1683 - val_acc: 0.4653 - val_prec: 0.1759 - val_recall: 0.1638 - val_prec_1: 0.1719 - val_recall_1: 0.0895 - val_prec_2: 0.1781 - val_recall_2: 0.1733 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0726 - val_f1_m: 0.4251 - val_recall_m: 0.3956 - val_precision_m: 0.4755 - val_precision: 0.4755 - val_recall_6: 0.3956 - val_f1score: 0.4251\n",
      "Epoch 13/300\n",
      "epoch:  12\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.5788 - acc: 0.7618 - prec: 0.9194 - recall: 0.9437 - prec_1: 0.7178 - recall_1: 0.7051 - prec_2: 0.8514 - recall_2: 0.8150 - prec_3: 0.6575 - recall_3: 0.5961 - prec_4: 0.6168 - recall_4: 0.6738 - prec_5: 0.8148 - recall_5: 0.8654 - f1_m: 0.7368 - recall_m: 0.6834 - precision_m: 0.8005 - precision: 0.8005 - recall_6: 0.6834 - f1score: 0.7368 - val_loss: 3.0865 - val_acc: 0.3193 - val_prec: 0.1759 - val_recall: 0.0952 - val_prec_1: 0.1621 - val_recall_1: 0.0507 - val_prec_2: 0.1784 - val_recall_2: 0.1846 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1364 - val_recall_5: 0.0170 - val_f1_m: 0.2983 - val_recall_m: 0.2794 - val_precision_m: 0.3296 - val_precision: 0.3296 - val_recall_6: 0.2794 - val_f1score: 0.2983\n",
      "Epoch 14/300\n",
      "epoch:  13\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.5807 - acc: 0.7557 - prec: 0.9225 - recall: 0.9378 - prec_1: 0.7200 - recall_1: 0.7151 - prec_2: 0.8533 - recall_2: 0.8243 - prec_3: 0.6311 - recall_3: 0.5790 - prec_4: 0.5945 - recall_4: 0.6415 - prec_5: 0.8189 - recall_5: 0.8719 - f1_m: 0.7310 - recall_m: 0.6793 - precision_m: 0.7923 - precision: 0.7923 - recall_6: 0.6793 - f1score: 0.7310 - val_loss: 2.4754 - val_acc: 0.4130 - val_prec: 0.1692 - val_recall: 0.1681 - val_prec_1: 0.1720 - val_recall_1: 0.0751 - val_prec_2: 0.1780 - val_recall_2: 0.1484 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1364 - val_recall_5: 0.0494 - val_f1_m: 0.3981 - val_recall_m: 0.3726 - val_precision_m: 0.4342 - val_precision: 0.4342 - val_recall_6: 0.3726 - val_f1score: 0.3981\n",
      "Epoch 15/300\n",
      "epoch:  14\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.5654 - acc: 0.7697 - prec: 0.9095 - recall: 0.9422 - prec_1: 0.7331 - recall_1: 0.7268 - prec_2: 0.8677 - recall_2: 0.8254 - prec_3: 0.6660 - recall_3: 0.5715 - prec_4: 0.6139 - recall_4: 0.7083 - prec_5: 0.8377 - recall_5: 0.8847 - f1_m: 0.7501 - recall_m: 0.7044 - precision_m: 0.8033 - precision: 0.8033 - recall_6: 0.7044 - f1score: 0.7501 - val_loss: 5.3173 - val_acc: 0.3871 - val_prec: 0.1759 - val_recall: 0.1648 - val_prec_1: 0.1718 - val_recall_1: 0.0555 - val_prec_2: 0.1784 - val_recall_2: 0.1828 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1044 - val_recall_5: 0.0208 - val_f1_m: 0.3658 - val_recall_m: 0.3533 - val_precision_m: 0.3824 - val_precision: 0.3824 - val_recall_6: 0.3533 - val_f1score: 0.3658\n",
      "Epoch 16/300\n",
      "epoch:  15\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.5593 - acc: 0.7586 - prec: 0.9240 - recall: 0.9436 - prec_1: 0.7134 - recall_1: 0.7248 - prec_2: 0.8583 - recall_2: 0.8203 - prec_3: 0.6315 - recall_3: 0.5642 - prec_4: 0.5912 - recall_4: 0.6537 - prec_5: 0.8333 - recall_5: 0.8769 - f1_m: 0.7438 - recall_m: 0.6998 - precision_m: 0.7948 - precision: 0.7948 - recall_6: 0.6998 - f1score: 0.7438 - val_loss: 3.7703 - val_acc: 0.4058 - val_prec: 0.1748 - val_recall: 0.1658 - val_prec_1: 0.1676 - val_recall_1: 0.0637 - val_prec_2: 0.1279 - val_recall_2: 0.0207 - val_prec_3: 0.1279 - val_recall_3: 0.0423 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1295 - val_f1_m: 0.3837 - val_recall_m: 0.3681 - val_precision_m: 0.4043 - val_precision: 0.4043 - val_recall_6: 0.3681 - val_f1score: 0.3837\n",
      "Epoch 17/300\n",
      "epoch:  16\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.5537 - acc: 0.7739 - prec: 0.9168 - recall: 0.9409 - prec_1: 0.7478 - recall_1: 0.7398 - prec_2: 0.8617 - recall_2: 0.8292 - prec_3: 0.6529 - recall_3: 0.5813 - prec_4: 0.6157 - recall_4: 0.6902 - prec_5: 0.8471 - recall_5: 0.8946 - f1_m: 0.7596 - recall_m: 0.7179 - precision_m: 0.8074 - precision: 0.8074 - recall_6: 0.7179 - f1score: 0.7596 - val_loss: 3.5246 - val_acc: 0.4008 - val_prec: 0.1759 - val_recall: 0.1131 - val_prec_1: 0.1615 - val_recall_1: 0.0852 - val_prec_2: 0.1639 - val_recall_2: 0.0635 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1529 - val_f1_m: 0.3931 - val_recall_m: 0.3798 - val_precision_m: 0.4087 - val_precision: 0.4087 - val_recall_6: 0.3798 - val_f1score: 0.3931\n",
      "Epoch 18/300\n",
      "epoch:  17\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 675us/step - loss: 0.5508 - acc: 0.7698 - prec: 0.9290 - recall: 0.9356 - prec_1: 0.7418 - recall_1: 0.7404 - prec_2: 0.8671 - recall_2: 0.8395 - prec_3: 0.6512 - recall_3: 0.5783 - prec_4: 0.6099 - recall_4: 0.6822 - prec_5: 0.8371 - recall_5: 0.8903 - f1_m: 0.7533 - recall_m: 0.7113 - precision_m: 0.8019 - precision: 0.8019 - recall_6: 0.7113 - f1score: 0.7533 - val_loss: 3.5300 - val_acc: 0.3251 - val_prec: 0.1439 - val_recall: 0.0287 - val_prec_1: 0.1652 - val_recall_1: 0.1472 - val_prec_2: 0.1660 - val_recall_2: 0.1381 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1364 - val_recall_5: 0.0325 - val_f1_m: 0.3152 - val_recall_m: 0.3053 - val_precision_m: 0.3299 - val_precision: 0.3299 - val_recall_6: 0.3053 - val_f1score: 0.3152\n",
      "Epoch 19/300\n",
      "epoch:  18\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.5375 - acc: 0.7723 - prec: 0.9201 - recall: 0.9445 - prec_1: 0.7442 - recall_1: 0.7641 - prec_2: 0.8801 - recall_2: 0.8308 - prec_3: 0.6404 - recall_3: 0.5743 - prec_4: 0.6099 - recall_4: 0.6632 - prec_5: 0.8529 - recall_5: 0.8871 - f1_m: 0.7602 - recall_m: 0.7236 - precision_m: 0.8017 - precision: 0.8017 - recall_6: 0.7236 - f1score: 0.7602 - val_loss: 5.4238 - val_acc: 0.3821 - val_prec: 0.1676 - val_recall: 0.1712 - val_prec_1: 0.1740 - val_recall_1: 0.0641 - val_prec_2: 0.1799 - val_recall_2: 0.1089 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0629 - val_f1_m: 0.3702 - val_recall_m: 0.3503 - val_precision_m: 0.3976 - val_precision: 0.3976 - val_recall_6: 0.3503 - val_f1score: 0.3702\n",
      "Epoch 20/300\n",
      "epoch:  19\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.5365 - acc: 0.7747 - prec: 0.9207 - recall: 0.9439 - prec_1: 0.7555 - recall_1: 0.7657 - prec_2: 0.8771 - recall_2: 0.8373 - prec_3: 0.6446 - recall_3: 0.5764 - prec_4: 0.6038 - recall_4: 0.6683 - prec_5: 0.8549 - recall_5: 0.9010 - f1_m: 0.7596 - recall_m: 0.7233 - precision_m: 0.8007 - precision: 0.8007 - recall_6: 0.7233 - f1score: 0.7596 - val_loss: 2.4062 - val_acc: 0.5277 - val_prec: 0.1759 - val_recall: 0.1517 - val_prec_1: 0.1737 - val_recall_1: 0.1270 - val_prec_2: 0.1839 - val_recall_2: 0.1194 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1608 - val_f1_m: 0.5160 - val_recall_m: 0.4960 - val_precision_m: 0.5400 - val_precision: 0.5400 - val_recall_6: 0.4960 - val_f1score: 0.5160\n",
      "Epoch 21/300\n",
      "epoch:  20\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.5199 - acc: 0.7849 - prec: 0.9205 - recall: 0.9441 - prec_1: 0.7656 - recall_1: 0.7733 - prec_2: 0.8824 - recall_2: 0.8401 - prec_3: 0.6746 - recall_3: 0.5863 - prec_4: 0.6203 - recall_4: 0.7040 - prec_5: 0.8652 - recall_5: 0.8957 - f1_m: 0.7732 - recall_m: 0.7381 - precision_m: 0.8128 - precision: 0.8128 - recall_6: 0.7381 - f1score: 0.7732 - val_loss: 0.5941 - val_acc: 0.7556 - val_prec: 0.1748 - val_recall: 0.1661 - val_prec_1: 0.1734 - val_recall_1: 0.1138 - val_prec_2: 0.1805 - val_recall_2: 0.1519 - val_prec_3: 0.1659 - val_recall_3: 0.1063 - val_prec_4: 0.1759 - val_recall_4: 0.1124 - val_prec_5: 0.1684 - val_recall_5: 0.1542 - val_f1_m: 0.7288 - val_recall_m: 0.6854 - val_precision_m: 0.7948 - val_precision: 0.7948 - val_recall_6: 0.6854 - val_f1score: 0.7288\n",
      "Epoch 22/300\n",
      "epoch:  21\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.5129 - acc: 0.7884 - prec: 0.9305 - recall: 0.9474 - prec_1: 0.7636 - recall_1: 0.7882 - prec_2: 0.8995 - recall_2: 0.8557 - prec_3: 0.6752 - recall_3: 0.6142 - prec_4: 0.6267 - recall_4: 0.6809 - prec_5: 0.8670 - recall_5: 0.8915 - f1_m: 0.7776 - recall_m: 0.7431 - precision_m: 0.8162 - precision: 0.8162 - recall_6: 0.7431 - f1score: 0.7776 - val_loss: 3.7014 - val_acc: 0.5415 - val_prec: 0.1743 - val_recall: 0.1656 - val_prec_1: 0.1733 - val_recall_1: 0.1273 - val_prec_2: 0.1796 - val_recall_2: 0.1349 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1457 - val_f1_m: 0.5060 - val_recall_m: 0.4623 - val_precision_m: 0.5725 - val_precision: 0.5725 - val_recall_6: 0.4623 - val_f1score: 0.5060\n",
      "Epoch 23/300\n",
      "epoch:  22\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.5105 - acc: 0.7864 - prec: 0.9316 - recall: 0.9450 - prec_1: 0.7704 - recall_1: 0.7825 - prec_2: 0.8823 - recall_2: 0.8556 - prec_3: 0.6514 - recall_3: 0.5775 - prec_4: 0.6097 - recall_4: 0.6815 - prec_5: 0.8702 - recall_5: 0.9037 - f1_m: 0.7766 - recall_m: 0.7453 - precision_m: 0.8113 - precision: 0.8113 - recall_6: 0.7453 - f1score: 0.7766 - val_loss: 5.2677 - val_acc: 0.4120 - val_prec: 0.1723 - val_recall: 0.1699 - val_prec_1: 0.1738 - val_recall_1: 0.0927 - val_prec_2: 0.1830 - val_recall_2: 0.1414 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0400 - val_f1_m: 0.4031 - val_recall_m: 0.3863 - val_precision_m: 0.4232 - val_precision: 0.4232 - val_recall_6: 0.3863 - val_f1score: 0.4031\n",
      "Epoch 24/300\n",
      "epoch:  23\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.5052 - acc: 0.7886 - prec: 0.9207 - recall: 0.9430 - prec_1: 0.7776 - recall_1: 0.7837 - prec_2: 0.8868 - recall_2: 0.8558 - prec_3: 0.6722 - recall_3: 0.5713 - prec_4: 0.6143 - recall_4: 0.7020 - prec_5: 0.8702 - recall_5: 0.9087 - f1_m: 0.7788 - recall_m: 0.7491 - precision_m: 0.8116 - precision: 0.8116 - recall_6: 0.7491 - f1score: 0.7788 - val_loss: 1.8692 - val_acc: 0.5495 - val_prec: 0.1743 - val_recall: 0.1678 - val_prec_1: 0.1731 - val_recall_1: 0.1625 - val_prec_2: 0.1805 - val_recall_2: 0.1101 - val_prec_3: 0.1599 - val_recall_3: 0.0720 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0710 - val_f1_m: 0.5408 - val_recall_m: 0.5307 - val_precision_m: 0.5522 - val_precision: 0.5522 - val_recall_6: 0.5307 - val_f1score: 0.5408\n",
      "Epoch 25/300\n",
      "epoch:  24\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.5067 - acc: 0.7854 - prec: 0.9262 - recall: 0.9472 - prec_1: 0.7724 - recall_1: 0.7904 - prec_2: 0.8842 - recall_2: 0.8588 - prec_3: 0.6508 - recall_3: 0.6282 - prec_4: 0.6215 - recall_4: 0.6509 - prec_5: 0.8707 - recall_5: 0.8961 - f1_m: 0.7762 - recall_m: 0.7462 - precision_m: 0.8094 - precision: 0.8094 - recall_6: 0.7462 - f1score: 0.7762 - val_loss: 5.4627 - val_acc: 0.3633 - val_prec: 0.1738 - val_recall: 0.1666 - val_prec_1: 0.1712 - val_recall_1: 0.0697 - val_prec_2: 0.1775 - val_recall_2: 0.1502 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1044 - val_recall_5: 0.0060 - val_f1_m: 0.3518 - val_recall_m: 0.3418 - val_precision_m: 0.3634 - val_precision: 0.3634 - val_recall_6: 0.3418 - val_f1score: 0.3518\n",
      "Epoch 26/300\n",
      "epoch:  25\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.4940 - acc: 0.7883 - prec: 0.9329 - recall: 0.9427 - prec_1: 0.7789 - recall_1: 0.7953 - prec_2: 0.8932 - recall_2: 0.8581 - prec_3: 0.6410 - recall_3: 0.5979 - prec_4: 0.6149 - recall_4: 0.6648 - prec_5: 0.8696 - recall_5: 0.9028 - f1_m: 0.7814 - recall_m: 0.7523 - precision_m: 0.8133 - precision: 0.8133 - recall_6: 0.7523 - f1score: 0.7814 - val_loss: 4.1279 - val_acc: 0.4470 - val_prec: 0.1759 - val_recall: 0.1638 - val_prec_1: 0.1738 - val_recall_1: 0.0783 - val_prec_2: 0.1789 - val_recall_2: 0.1861 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0549 - val_f1_m: 0.4374 - val_recall_m: 0.4243 - val_precision_m: 0.4539 - val_precision: 0.4539 - val_recall_6: 0.4243 - val_f1score: 0.4374\n",
      "Epoch 27/300\n",
      "epoch:  26\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.4890 - acc: 0.7922 - prec: 0.9331 - recall: 0.9492 - prec_1: 0.7964 - recall_1: 0.8013 - prec_2: 0.8980 - recall_2: 0.8644 - prec_3: 0.6578 - recall_3: 0.5803 - prec_4: 0.6122 - recall_4: 0.6880 - prec_5: 0.8788 - recall_5: 0.9095 - f1_m: 0.7857 - recall_m: 0.7582 - precision_m: 0.8159 - precision: 0.8159 - recall_6: 0.7582 - f1score: 0.7857 - val_loss: 4.0195 - val_acc: 0.4393 - val_prec: 0.1759 - val_recall: 0.1482 - val_prec_1: 0.1721 - val_recall_1: 0.1332 - val_prec_2: 0.1812 - val_recall_2: 0.0848 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0940 - val_f1_m: 0.4375 - val_recall_m: 0.4325 - val_precision_m: 0.4429 - val_precision: 0.4429 - val_recall_6: 0.4325 - val_f1score: 0.4375\n",
      "Epoch 28/300\n",
      "epoch:  27\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.4862 - acc: 0.7934 - prec: 0.9301 - recall: 0.9458 - prec_1: 0.7916 - recall_1: 0.8146 - prec_2: 0.8985 - recall_2: 0.8657 - prec_3: 0.6521 - recall_3: 0.5835 - prec_4: 0.6090 - recall_4: 0.6754 - prec_5: 0.8819 - recall_5: 0.9097 - f1_m: 0.7851 - recall_m: 0.7589 - precision_m: 0.8137 - precision: 0.8137 - recall_6: 0.7589 - f1score: 0.7851 - val_loss: 4.6133 - val_acc: 0.4828 - val_prec: 0.1753 - val_recall: 0.1658 - val_prec_1: 0.1733 - val_recall_1: 0.1461 - val_prec_2: 0.1819 - val_recall_2: 0.1307 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0717 - val_f1_m: 0.4803 - val_recall_m: 0.4708 - val_precision_m: 0.4908 - val_precision: 0.4908 - val_recall_6: 0.4708 - val_f1score: 0.4803\n",
      "Epoch 29/300\n",
      "epoch:  28\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.4829 - acc: 0.7951 - prec: 0.9299 - recall: 0.9417 - prec_1: 0.7888 - recall_1: 0.8171 - prec_2: 0.9056 - recall_2: 0.8612 - prec_3: 0.6597 - recall_3: 0.5866 - prec_4: 0.6170 - recall_4: 0.6922 - prec_5: 0.8847 - recall_5: 0.9123 - f1_m: 0.7883 - recall_m: 0.7617 - precision_m: 0.8173 - precision: 0.8173 - recall_6: 0.7617 - f1score: 0.7883 - val_loss: 3.5326 - val_acc: 0.5112 - val_prec: 0.1753 - val_recall: 0.1643 - val_prec_1: 0.1727 - val_recall_1: 0.1435 - val_prec_2: 0.1777 - val_recall_2: 0.1399 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0960 - val_f1_m: 0.5067 - val_recall_m: 0.4948 - val_precision_m: 0.5205 - val_precision: 0.5205 - val_recall_6: 0.4948 - val_f1score: 0.5067\n",
      "Epoch 30/300\n",
      "epoch:  29\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.4743 - acc: 0.8012 - prec: 0.9372 - recall: 0.9525 - prec_1: 0.7887 - recall_1: 0.8045 - prec_2: 0.9020 - recall_2: 0.8715 - prec_3: 0.6781 - recall_3: 0.6182 - prec_4: 0.6351 - recall_4: 0.6905 - prec_5: 0.8744 - recall_5: 0.9126 - f1_m: 0.7943 - recall_m: 0.7672 - precision_m: 0.8241 - precision: 0.8241 - recall_6: 0.7672 - f1score: 0.7943 - val_loss: 1.6708 - val_acc: 0.5130 - val_prec: 0.1743 - val_recall: 0.1681 - val_prec_1: 0.1738 - val_recall_1: 0.1479 - val_prec_2: 0.1679 - val_recall_2: 0.0767 - val_prec_3: 0.0640 - val_recall_3: 0.0202 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1250 - val_f1_m: 0.4890 - val_recall_m: 0.4675 - val_precision_m: 0.5154 - val_precision: 0.5154 - val_recall_6: 0.4675 - val_f1score: 0.4890\n",
      "Epoch 31/300\n",
      "epoch:  30\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.4751 - acc: 0.7988 - prec: 0.9290 - recall: 0.9481 - prec_1: 0.7957 - recall_1: 0.8221 - prec_2: 0.9038 - recall_2: 0.8635 - prec_3: 0.6650 - recall_3: 0.5955 - prec_4: 0.6237 - recall_4: 0.6921 - prec_5: 0.8852 - recall_5: 0.9157 - f1_m: 0.7948 - recall_m: 0.7706 - precision_m: 0.8212 - precision: 0.8212 - recall_6: 0.7706 - f1score: 0.7948 - val_loss: 2.6777 - val_acc: 0.4708 - val_prec: 0.1759 - val_recall: 0.1553 - val_prec_1: 0.1727 - val_recall_1: 0.0670 - val_prec_2: 0.1639 - val_recall_2: 0.1082 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1645 - val_f1_m: 0.4631 - val_recall_m: 0.4510 - val_precision_m: 0.4771 - val_precision: 0.4771 - val_recall_6: 0.4510 - val_f1score: 0.4631\n",
      "Epoch 32/300\n",
      "epoch:  31\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.4738 - acc: 0.8010 - prec: 0.9274 - recall: 0.9491 - prec_1: 0.8009 - recall_1: 0.8188 - prec_2: 0.9024 - recall_2: 0.8737 - prec_3: 0.6687 - recall_3: 0.5997 - prec_4: 0.6256 - recall_4: 0.6832 - prec_5: 0.8760 - recall_5: 0.9095 - f1_m: 0.7961 - recall_m: 0.7721 - precision_m: 0.8221 - precision: 0.8221 - recall_6: 0.7721 - f1score: 0.7961 - val_loss: 0.9538 - val_acc: 0.6154 - val_prec: 0.1759 - val_recall: 0.1064 - val_prec_1: 0.1679 - val_recall_1: 0.1108 - val_prec_2: 0.1812 - val_recall_2: 0.1224 - val_prec_3: 0.0640 - val_recall_3: 0.0122 - val_prec_4: 0.1809 - val_recall_4: 0.1562 - val_prec_5: 0.1684 - val_recall_5: 0.1454 - val_f1_m: 0.5897 - val_recall_m: 0.5520 - val_precision_m: 0.6569 - val_precision: 0.6569 - val_recall_6: 0.5520 - val_f1score: 0.5897\n",
      "Epoch 33/300\n",
      "epoch:  32\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.4607 - acc: 0.8030 - prec: 0.9363 - recall: 0.9520 - prec_1: 0.8089 - recall_1: 0.8446 - prec_2: 0.9128 - recall_2: 0.8730 - prec_3: 0.6582 - recall_3: 0.5908 - prec_4: 0.6189 - recall_4: 0.6809 - prec_5: 0.8970 - recall_5: 0.9183 - f1_m: 0.7968 - recall_m: 0.7743 - precision_m: 0.8209 - precision: 0.8209 - recall_6: 0.7743 - f1score: 0.7968 - val_loss: 4.0662 - val_acc: 0.2989 - val_prec: 0.1439 - val_recall: 0.0381 - val_prec_1: 0.1439 - val_recall_1: 0.0350 - val_prec_2: 0.1672 - val_recall_2: 0.1912 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0607 - val_f1_m: 0.2939 - val_recall_m: 0.2904 - val_precision_m: 0.2981 - val_precision: 0.2981 - val_recall_6: 0.2904 - val_f1score: 0.2939\n",
      "Epoch 34/300\n",
      "epoch:  33\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.4587 - acc: 0.8061 - prec: 0.9341 - recall: 0.9471 - prec_1: 0.8092 - recall_1: 0.8315 - prec_2: 0.9139 - recall_2: 0.8817 - prec_3: 0.6666 - recall_3: 0.6165 - prec_4: 0.6266 - recall_4: 0.6751 - prec_5: 0.8926 - recall_5: 0.9144 - f1_m: 0.8004 - recall_m: 0.7784 - precision_m: 0.8240 - precision: 0.8240 - recall_6: 0.7784 - f1score: 0.8004 - val_loss: 1.5888 - val_acc: 0.5657 - val_prec: 0.1712 - val_recall: 0.1691 - val_prec_1: 0.1745 - val_recall_1: 0.0932 - val_prec_2: 0.1855 - val_recall_2: 0.1160 - val_prec_3: 0.1651 - val_recall_3: 0.0577 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1595 - val_f1_m: 0.5332 - val_recall_m: 0.5040 - val_precision_m: 0.5726 - val_precision: 0.5726 - val_recall_6: 0.5040 - val_f1score: 0.5332\n",
      "Epoch 35/300\n",
      "epoch:  34\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.4611 - acc: 0.8022 - prec: 0.9314 - recall: 0.9519 - prec_1: 0.8116 - recall_1: 0.8149 - prec_2: 0.9025 - recall_2: 0.8835 - prec_3: 0.6627 - recall_3: 0.6071 - prec_4: 0.6317 - recall_4: 0.6865 - prec_5: 0.8740 - recall_5: 0.9152 - f1_m: 0.7983 - recall_m: 0.7744 - precision_m: 0.8243 - precision: 0.8243 - recall_6: 0.7744 - f1score: 0.7983 - val_loss: 1.3647 - val_acc: 0.4915 - val_prec: 0.1759 - val_recall: 0.1648 - val_prec_1: 0.1702 - val_recall_1: 0.0445 - val_prec_2: 0.1839 - val_recall_2: 0.1083 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1279 - val_recall_4: 0.0464 - val_prec_5: 0.1684 - val_recall_5: 0.1655 - val_f1_m: 0.4802 - val_recall_m: 0.4605 - val_precision_m: 0.5118 - val_precision: 0.5118 - val_recall_6: 0.4605 - val_f1score: 0.4802\n",
      "Epoch 36/300\n",
      "epoch:  35\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.4535 - acc: 0.8020 - prec: 0.9311 - recall: 0.9488 - prec_1: 0.8211 - recall_1: 0.8313 - prec_2: 0.9088 - recall_2: 0.8820 - prec_3: 0.6373 - recall_3: 0.5946 - prec_4: 0.6117 - recall_4: 0.6600 - prec_5: 0.8948 - recall_5: 0.9255 - f1_m: 0.7958 - recall_m: 0.7731 - precision_m: 0.8203 - precision: 0.8203 - recall_6: 0.7731 - f1score: 0.7958 - val_loss: 2.5352 - val_acc: 0.4305 - val_prec: 0.1759 - val_recall: 0.1473 - val_prec_1: 0.1745 - val_recall_1: 0.0579 - val_prec_2: 0.1787 - val_recall_2: 0.1708 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0872 - val_f1_m: 0.4160 - val_recall_m: 0.3958 - val_precision_m: 0.4418 - val_precision: 0.4418 - val_recall_6: 0.3958 - val_f1score: 0.4160\n",
      "Epoch 37/300\n",
      "epoch:  36\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.4464 - acc: 0.8091 - prec: 0.9380 - recall: 0.9547 - prec_1: 0.8318 - recall_1: 0.8469 - prec_2: 0.9205 - recall_2: 0.8807 - prec_3: 0.6443 - recall_3: 0.6152 - prec_4: 0.6285 - recall_4: 0.6600 - prec_5: 0.8982 - recall_5: 0.9330 - f1_m: 0.8039 - recall_m: 0.7813 - precision_m: 0.8282 - precision: 0.8282 - recall_6: 0.7813 - f1score: 0.8039 - val_loss: 1.6241 - val_acc: 0.5327 - val_prec: 0.1701 - val_recall: 0.1706 - val_prec_1: 0.1736 - val_recall_1: 0.0865 - val_prec_2: 0.1879 - val_recall_2: 0.1393 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1809 - val_recall_4: 0.1566 - val_prec_5: 0.1204 - val_recall_5: 0.0318 - val_f1_m: 0.5290 - val_recall_m: 0.5157 - val_precision_m: 0.5443 - val_precision: 0.5443 - val_recall_6: 0.5157 - val_f1score: 0.5290\n",
      "Epoch 38/300\n",
      "epoch:  37\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.4455 - acc: 0.8128 - prec: 0.9441 - recall: 0.9503 - prec_1: 0.8326 - recall_1: 0.8524 - prec_2: 0.9168 - recall_2: 0.8915 - prec_3: 0.6654 - recall_3: 0.5982 - prec_4: 0.6301 - recall_4: 0.6922 - prec_5: 0.9018 - recall_5: 0.9276 - f1_m: 0.8063 - recall_m: 0.7857 - precision_m: 0.8286 - precision: 0.8286 - recall_6: 0.7857 - f1score: 0.8063 - val_loss: 0.8779 - val_acc: 0.5710 - val_prec: 0.1759 - val_recall: 0.0571 - val_prec_1: 0.1734 - val_recall_1: 0.1022 - val_prec_2: 0.1794 - val_recall_2: 0.1762 - val_prec_3: 0.1119 - val_recall_3: 0.0428 - val_prec_4: 0.1759 - val_recall_4: 0.1302 - val_prec_5: 0.1684 - val_recall_5: 0.1093 - val_f1_m: 0.5424 - val_recall_m: 0.5110 - val_precision_m: 0.5906 - val_precision: 0.5906 - val_recall_6: 0.5110 - val_f1score: 0.5424\n",
      "Epoch 39/300\n",
      "epoch:  38\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.4439 - acc: 0.8045 - prec: 0.9351 - recall: 0.9519 - prec_1: 0.8189 - recall_1: 0.8286 - prec_2: 0.9104 - recall_2: 0.8793 - prec_3: 0.6522 - recall_3: 0.6098 - prec_4: 0.6256 - recall_4: 0.6629 - prec_5: 0.8950 - recall_5: 0.9306 - f1_m: 0.8009 - recall_m: 0.7808 - precision_m: 0.8224 - precision: 0.8224 - recall_6: 0.7808 - f1score: 0.8009 - val_loss: 3.6413 - val_acc: 0.2299 - val_prec: 0.1599 - val_recall: 0.0351 - val_prec_1: 0.1279 - val_recall_1: 0.0172 - val_prec_2: 0.1744 - val_recall_2: 0.1917 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0146 - val_f1_m: 0.2151 - val_recall_m: 0.2101 - val_precision_m: 0.2232 - val_precision: 0.2232 - val_recall_6: 0.2101 - val_f1score: 0.2151\n",
      "Epoch 40/300\n",
      "epoch:  39\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.4397 - acc: 0.8102 - prec: 0.9373 - recall: 0.9549 - prec_1: 0.8254 - recall_1: 0.8394 - prec_2: 0.9064 - recall_2: 0.8845 - prec_3: 0.6697 - recall_3: 0.6086 - prec_4: 0.6277 - recall_4: 0.6858 - prec_5: 0.8973 - recall_5: 0.9242 - f1_m: 0.8063 - recall_m: 0.7854 - precision_m: 0.8288 - precision: 0.8288 - recall_6: 0.7854 - f1score: 0.8063 - val_loss: 2.1119 - val_acc: 0.5707 - val_prec: 0.1738 - val_recall: 0.1696 - val_prec_1: 0.1732 - val_recall_1: 0.1540 - val_prec_2: 0.1839 - val_recall_2: 0.1323 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1279 - val_recall_4: 0.0434 - val_prec_5: 0.1684 - val_recall_5: 0.1124 - val_f1_m: 0.5482 - val_recall_m: 0.5262 - val_precision_m: 0.5776 - val_precision: 0.5776 - val_recall_6: 0.5262 - val_f1score: 0.5482\n",
      "Epoch 41/300\n",
      "epoch:  40\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 676us/step - loss: 0.4364 - acc: 0.8080 - prec: 0.9327 - recall: 0.9494 - prec_1: 0.8284 - recall_1: 0.8364 - prec_2: 0.9114 - recall_2: 0.8830 - prec_3: 0.6629 - recall_3: 0.5935 - prec_4: 0.6221 - recall_4: 0.6924 - prec_5: 0.8863 - recall_5: 0.9276 - f1_m: 0.8041 - recall_m: 0.7826 - precision_m: 0.8273 - precision: 0.8273 - recall_6: 0.7826 - f1score: 0.8041 - val_loss: 2.1032 - val_acc: 0.5277 - val_prec: 0.1759 - val_recall: 0.1545 - val_prec_1: 0.1733 - val_recall_1: 0.1217 - val_prec_2: 0.1839 - val_recall_2: 0.1194 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0960 - val_recall_4: 0.0197 - val_prec_5: 0.1684 - val_recall_5: 0.1388 - val_f1_m: 0.5225 - val_recall_m: 0.5135 - val_precision_m: 0.5326 - val_precision: 0.5326 - val_recall_6: 0.5135 - val_f1score: 0.5225\n",
      "Epoch 42/300\n",
      "epoch:  41\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.4293 - acc: 0.8073 - prec: 0.9372 - recall: 0.9462 - prec_1: 0.8249 - recall_1: 0.8429 - prec_2: 0.9172 - recall_2: 0.8966 - prec_3: 0.6520 - recall_3: 0.5970 - prec_4: 0.6165 - recall_4: 0.6687 - prec_5: 0.9017 - recall_5: 0.9202 - f1_m: 0.8046 - recall_m: 0.7857 - precision_m: 0.8247 - precision: 0.8247 - recall_6: 0.7857 - f1score: 0.8046 - val_loss: 0.6262 - val_acc: 0.6954 - val_prec: 0.1748 - val_recall: 0.1676 - val_prec_1: 0.1740 - val_recall_1: 0.1044 - val_prec_2: 0.1866 - val_recall_2: 0.1369 - val_prec_3: 0.0480 - val_recall_3: 0.0125 - val_prec_4: 0.1772 - val_recall_4: 0.1698 - val_prec_5: 0.1684 - val_recall_5: 0.1500 - val_f1_m: 0.6810 - val_recall_m: 0.6547 - val_precision_m: 0.7135 - val_precision: 0.7135 - val_recall_6: 0.6547 - val_f1score: 0.6810\n",
      "Epoch 43/300\n",
      "epoch:  42\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.4368 - acc: 0.8076 - prec: 0.9272 - recall: 0.9438 - prec_1: 0.8200 - recall_1: 0.8367 - prec_2: 0.9060 - recall_2: 0.8956 - prec_3: 0.6725 - recall_3: 0.6160 - prec_4: 0.6306 - recall_4: 0.6915 - prec_5: 0.8999 - recall_5: 0.9133 - f1_m: 0.8061 - recall_m: 0.7873 - precision_m: 0.8261 - precision: 0.8261 - recall_6: 0.7873 - f1score: 0.8061 - val_loss: 2.6761 - val_acc: 0.3811 - val_prec: 0.1599 - val_recall: 0.0563 - val_prec_1: 0.0800 - val_recall_1: 0.0050 - val_prec_2: 0.1675 - val_recall_2: 0.1914 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1769 - val_recall_4: 0.1306 - val_prec_5: 0.1204 - val_recall_5: 0.0405 - val_f1_m: 0.3789 - val_recall_m: 0.3716 - val_precision_m: 0.3878 - val_precision: 0.3878 - val_recall_6: 0.3716 - val_f1score: 0.3789\n",
      "Epoch 44/300\n",
      "epoch:  43\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.4241 - acc: 0.8140 - prec: 0.9321 - recall: 0.9543 - prec_1: 0.8312 - recall_1: 0.8454 - prec_2: 0.9144 - recall_2: 0.8890 - prec_3: 0.6739 - recall_3: 0.6046 - prec_4: 0.6336 - recall_4: 0.7043 - prec_5: 0.9063 - recall_5: 0.9331 - f1_m: 0.8109 - recall_m: 0.7912 - precision_m: 0.8320 - precision: 0.8320 - recall_6: 0.7912 - f1score: 0.8109 - val_loss: 2.1960 - val_acc: 0.5330 - val_prec: 0.1759 - val_recall: 0.1636 - val_prec_1: 0.1747 - val_recall_1: 0.0735 - val_prec_2: 0.1764 - val_recall_2: 0.1518 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1279 - val_recall_4: 0.0480 - val_prec_5: 0.1684 - val_recall_5: 0.1228 - val_f1_m: 0.5218 - val_recall_m: 0.5070 - val_precision_m: 0.5386 - val_precision: 0.5386 - val_recall_6: 0.5070 - val_f1score: 0.5218\n",
      "Epoch 45/300\n",
      "epoch:  44\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.4287 - acc: 0.8141 - prec: 0.9362 - recall: 0.9482 - prec_1: 0.8491 - recall_1: 0.8428 - prec_2: 0.9207 - recall_2: 0.8983 - prec_3: 0.6575 - recall_3: 0.6073 - prec_4: 0.6262 - recall_4: 0.6782 - prec_5: 0.8986 - recall_5: 0.9388 - f1_m: 0.8120 - recall_m: 0.7919 - precision_m: 0.8334 - precision: 0.8334 - recall_6: 0.7919 - f1score: 0.8120 - val_loss: 2.3570 - val_acc: 0.4333 - val_prec: 0.1729 - val_recall: 0.1693 - val_prec_1: 0.1735 - val_recall_1: 0.1016 - val_prec_2: 0.1866 - val_recall_2: 0.1014 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0905 - val_f1_m: 0.4295 - val_recall_m: 0.4143 - val_precision_m: 0.4479 - val_precision: 0.4479 - val_recall_6: 0.4143 - val_f1score: 0.4295\n",
      "Epoch 46/300\n",
      "epoch:  45\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.4230 - acc: 0.8126 - prec: 0.9334 - recall: 0.9494 - prec_1: 0.8302 - recall_1: 0.8466 - prec_2: 0.9154 - recall_2: 0.9000 - prec_3: 0.6520 - recall_3: 0.6164 - prec_4: 0.6306 - recall_4: 0.6692 - prec_5: 0.9152 - recall_5: 0.9256 - f1_m: 0.8077 - recall_m: 0.7898 - precision_m: 0.8267 - precision: 0.8267 - recall_6: 0.7898 - f1score: 0.8077 - val_loss: 1.1848 - val_acc: 0.5780 - val_prec: 0.1748 - val_recall: 0.1668 - val_prec_1: 0.1736 - val_recall_1: 0.1064 - val_prec_2: 0.1879 - val_recall_2: 0.1281 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1658 - val_recall_4: 0.1002 - val_prec_5: 0.1684 - val_recall_5: 0.1206 - val_f1_m: 0.5526 - val_recall_m: 0.5235 - val_precision_m: 0.5961 - val_precision: 0.5961 - val_recall_6: 0.5235 - val_f1score: 0.5526\n",
      "Epoch 47/300\n",
      "epoch:  46\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.4211 - acc: 0.8157 - prec: 0.9383 - recall: 0.9597 - prec_1: 0.8280 - recall_1: 0.8446 - prec_2: 0.9179 - recall_2: 0.8934 - prec_3: 0.6689 - recall_3: 0.6072 - prec_4: 0.6330 - recall_4: 0.6874 - prec_5: 0.9074 - recall_5: 0.9268 - f1_m: 0.8130 - recall_m: 0.7937 - precision_m: 0.8337 - precision: 0.8337 - recall_6: 0.7937 - f1score: 0.8130 - val_loss: 5.8379 - val_acc: 0.3778 - val_prec: 0.1712 - val_recall: 0.1641 - val_prec_1: 0.1727 - val_recall_1: 0.0329 - val_prec_2: 0.1784 - val_recall_2: 0.1792 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1364 - val_recall_5: 0.0350 - val_f1_m: 0.3735 - val_recall_m: 0.3666 - val_precision_m: 0.3816 - val_precision: 0.3816 - val_recall_6: 0.3666 - val_f1score: 0.3735\n",
      "Epoch 48/300\n",
      "epoch:  47\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.4211 - acc: 0.8107 - prec: 0.9395 - recall: 0.9528 - prec_1: 0.8255 - recall_1: 0.8502 - prec_2: 0.9114 - recall_2: 0.8979 - prec_3: 0.6608 - recall_3: 0.6202 - prec_4: 0.6296 - recall_4: 0.6708 - prec_5: 0.9109 - recall_5: 0.9235 - f1_m: 0.8090 - recall_m: 0.7908 - precision_m: 0.8285 - precision: 0.8285 - recall_6: 0.7908 - f1score: 0.8090 - val_loss: 2.8618 - val_acc: 0.3646 - val_prec: 0.1665 - val_recall: 0.1741 - val_prec_1: 0.1439 - val_recall_1: 0.0402 - val_prec_2: 0.1919 - val_recall_2: 0.1019 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0703 - val_f1_m: 0.3541 - val_recall_m: 0.3436 - val_precision_m: 0.3667 - val_precision: 0.3667 - val_recall_6: 0.3436 - val_f1score: 0.3541\n",
      "Epoch 49/300\n",
      "epoch:  48\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.4209 - acc: 0.8165 - prec: 0.9394 - recall: 0.9528 - prec_1: 0.8519 - recall_1: 0.8572 - prec_2: 0.9259 - recall_2: 0.9017 - prec_3: 0.6607 - recall_3: 0.5840 - prec_4: 0.6321 - recall_4: 0.7095 - prec_5: 0.9060 - recall_5: 0.9329 - f1_m: 0.8132 - recall_m: 0.7953 - precision_m: 0.8322 - precision: 0.8322 - recall_6: 0.7953 - f1score: 0.8132 - val_loss: 1.6313 - val_acc: 0.4953 - val_prec: 0.1692 - val_recall: 0.1701 - val_prec_1: 0.1729 - val_recall_1: 0.0913 - val_prec_2: 0.1759 - val_recall_2: 0.0667 - val_prec_3: 0.0640 - val_recall_3: 0.0195 - val_prec_4: 0.1649 - val_recall_4: 0.1412 - val_prec_5: 0.1204 - val_recall_5: 0.0383 - val_f1_m: 0.4918 - val_recall_m: 0.4875 - val_precision_m: 0.4971 - val_precision: 0.4971 - val_recall_6: 0.4875 - val_f1score: 0.4918\n",
      "Epoch 50/300\n",
      "epoch:  49\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.4095 - acc: 0.8175 - prec: 0.9326 - recall: 0.9541 - prec_1: 0.8503 - recall_1: 0.8557 - prec_2: 0.9281 - recall_2: 0.9000 - prec_3: 0.6606 - recall_3: 0.5925 - prec_4: 0.6252 - recall_4: 0.6906 - prec_5: 0.9074 - recall_5: 0.9371 - f1_m: 0.8151 - recall_m: 0.7982 - precision_m: 0.8331 - precision: 0.8331 - recall_6: 0.7982 - f1score: 0.8151 - val_loss: 2.3937 - val_acc: 0.3923 - val_prec: 0.1759 - val_recall: 0.0998 - val_prec_1: 0.1599 - val_recall_1: 0.0503 - val_prec_2: 0.1839 - val_recall_2: 0.0979 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1667 - val_recall_5: 0.1684 - val_f1_m: 0.3879 - val_recall_m: 0.3828 - val_precision_m: 0.3934 - val_precision: 0.3934 - val_recall_6: 0.3828 - val_f1score: 0.3879\n",
      "Epoch 51/300\n",
      "epoch:  50\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.4133 - acc: 0.8117 - prec: 0.9355 - recall: 0.9545 - prec_1: 0.8393 - recall_1: 0.8497 - prec_2: 0.9141 - recall_2: 0.8871 - prec_3: 0.6656 - recall_3: 0.6111 - prec_4: 0.6216 - recall_4: 0.6747 - prec_5: 0.9018 - recall_5: 0.9352 - f1_m: 0.8084 - recall_m: 0.7902 - precision_m: 0.8277 - precision: 0.8277 - recall_6: 0.7902 - f1score: 0.8084 - val_loss: 3.3769 - val_acc: 0.4360 - val_prec: 0.1729 - val_recall: 0.1656 - val_prec_1: 0.1706 - val_recall_1: 0.0470 - val_prec_2: 0.1808 - val_recall_2: 0.1882 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0720 - val_f1_m: 0.4332 - val_recall_m: 0.4273 - val_precision_m: 0.4398 - val_precision: 0.4398 - val_recall_6: 0.4273 - val_f1score: 0.4332\n",
      "Epoch 52/300\n",
      "epoch:  51\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 681us/step - loss: 0.4131 - acc: 0.8161 - prec: 0.9383 - recall: 0.9545 - prec_1: 0.8470 - recall_1: 0.8577 - prec_2: 0.9174 - recall_2: 0.8972 - prec_3: 0.6550 - recall_3: 0.6268 - prec_4: 0.6278 - recall_4: 0.6621 - prec_5: 0.9168 - recall_5: 0.9352 - f1_m: 0.8128 - recall_m: 0.7944 - precision_m: 0.8323 - precision: 0.8323 - recall_6: 0.7944 - f1score: 0.8128 - val_loss: 3.9900 - val_acc: 0.3271 - val_prec: 0.1759 - val_recall: 0.0790 - val_prec_1: 0.1704 - val_recall_1: 0.0531 - val_prec_2: 0.1806 - val_recall_2: 0.1893 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0400 - val_f1_m: 0.3202 - val_recall_m: 0.3113 - val_precision_m: 0.3311 - val_precision: 0.3311 - val_recall_6: 0.3113 - val_f1score: 0.3202\n",
      "Epoch 53/300\n",
      "epoch:  52\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.4156 - acc: 0.8158 - prec: 0.9365 - recall: 0.9512 - prec_1: 0.8369 - recall_1: 0.8614 - prec_2: 0.9196 - recall_2: 0.9036 - prec_3: 0.6546 - recall_3: 0.6199 - prec_4: 0.6275 - recall_4: 0.6709 - prec_5: 0.9203 - recall_5: 0.9313 - f1_m: 0.8102 - recall_m: 0.7918 - precision_m: 0.8299 - precision: 0.8299 - recall_6: 0.7918 - f1score: 0.8102 - val_loss: 6.1474 - val_acc: 0.4205 - val_prec: 0.1759 - val_recall: 0.1648 - val_prec_1: 0.1595 - val_recall_1: 0.0546 - val_prec_2: 0.1599 - val_recall_2: 0.0525 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1628 - val_f1_m: 0.4176 - val_recall_m: 0.4108 - val_precision_m: 0.4254 - val_precision: 0.4254 - val_recall_6: 0.4108 - val_f1score: 0.4176\n",
      "Epoch 54/300\n",
      "epoch:  53\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.4056 - acc: 0.8193 - prec: 0.9347 - recall: 0.9537 - prec_1: 0.8536 - recall_1: 0.8712 - prec_2: 0.9255 - recall_2: 0.8958 - prec_3: 0.6619 - recall_3: 0.6088 - prec_4: 0.6281 - recall_4: 0.6819 - prec_5: 0.9190 - recall_5: 0.9394 - f1_m: 0.8165 - recall_m: 0.7989 - precision_m: 0.8351 - precision: 0.8351 - recall_6: 0.7989 - f1score: 0.8165 - val_loss: 0.9725 - val_acc: 0.6612 - val_prec: 0.1692 - val_recall: 0.1711 - val_prec_1: 0.1735 - val_recall_1: 0.0993 - val_prec_2: 0.1879 - val_recall_2: 0.1196 - val_prec_3: 0.1709 - val_recall_3: 0.1592 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1493 - val_f1_m: 0.6541 - val_recall_m: 0.6262 - val_precision_m: 0.6913 - val_precision: 0.6913 - val_recall_6: 0.6262 - val_f1score: 0.6541\n",
      "Epoch 55/300\n",
      "epoch:  54\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.4011 - acc: 0.8236 - prec: 0.9429 - recall: 0.9560 - prec_1: 0.8475 - recall_1: 0.8728 - prec_2: 0.9235 - recall_2: 0.9011 - prec_3: 0.6630 - recall_3: 0.6424 - prec_4: 0.6420 - recall_4: 0.6705 - prec_5: 0.9199 - recall_5: 0.9324 - f1_m: 0.8204 - recall_m: 0.8035 - precision_m: 0.8384 - precision: 0.8384 - recall_6: 0.8035 - f1score: 0.8204 - val_loss: 5.7448 - val_acc: 0.3668 - val_prec: 0.1748 - val_recall: 0.1651 - val_prec_1: 0.0640 - val_recall_1: 0.0093 - val_prec_2: 0.1788 - val_recall_2: 0.1871 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1364 - val_recall_5: 0.0387 - val_f1_m: 0.3617 - val_recall_m: 0.3593 - val_precision_m: 0.3650 - val_precision: 0.3650 - val_recall_6: 0.3593 - val_f1score: 0.3617\n",
      "Epoch 56/300\n",
      "epoch:  55\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.4016 - acc: 0.8227 - prec: 0.9491 - recall: 0.9528 - prec_1: 0.8541 - recall_1: 0.8546 - prec_2: 0.9011 - recall_2: 0.9104 - prec_3: 0.6677 - recall_3: 0.6495 - prec_4: 0.6466 - recall_4: 0.6699 - prec_5: 0.9117 - recall_5: 0.9323 - f1_m: 0.8219 - recall_m: 0.8063 - precision_m: 0.8383 - precision: 0.8383 - recall_6: 0.8063 - f1score: 0.8219 - val_loss: 3.7582 - val_acc: 0.4050 - val_prec: 0.1759 - val_recall: 0.1192 - val_prec_1: 0.1599 - val_recall_1: 0.0544 - val_prec_2: 0.1839 - val_recall_2: 0.0882 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1669 - val_f1_m: 0.4010 - val_recall_m: 0.3966 - val_precision_m: 0.4057 - val_precision: 0.4057 - val_recall_6: 0.3966 - val_f1score: 0.4010\n",
      "Epoch 57/300\n",
      "epoch:  56\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3990 - acc: 0.8226 - prec: 0.9414 - recall: 0.9562 - prec_1: 0.8492 - recall_1: 0.8718 - prec_2: 0.9283 - recall_2: 0.9087 - prec_3: 0.6705 - recall_3: 0.6158 - prec_4: 0.6304 - recall_4: 0.6869 - prec_5: 0.9179 - recall_5: 0.9314 - f1_m: 0.8190 - recall_m: 0.8023 - precision_m: 0.8367 - precision: 0.8367 - recall_6: 0.8023 - f1score: 0.8190 - val_loss: 1.2103 - val_acc: 0.5595 - val_prec: 0.1759 - val_recall: 0.1358 - val_prec_1: 0.1599 - val_recall_1: 0.0894 - val_prec_2: 0.1905 - val_recall_2: 0.1835 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1763 - val_recall_4: 0.1312 - val_prec_5: 0.1524 - val_recall_5: 0.0712 - val_f1_m: 0.5472 - val_recall_m: 0.5297 - val_precision_m: 0.5683 - val_precision: 0.5683 - val_recall_6: 0.5297 - val_f1score: 0.5472\n",
      "Epoch 58/300\n",
      "epoch:  57\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.4002 - acc: 0.8203 - prec: 0.9399 - recall: 0.9539 - prec_1: 0.8438 - recall_1: 0.8674 - prec_2: 0.9139 - recall_2: 0.8964 - prec_3: 0.6590 - recall_3: 0.6330 - prec_4: 0.6422 - recall_4: 0.6751 - prec_5: 0.9166 - recall_5: 0.9353 - f1_m: 0.8183 - recall_m: 0.8018 - precision_m: 0.8358 - precision: 0.8358 - recall_6: 0.8018 - f1score: 0.8183 - val_loss: 2.7122 - val_acc: 0.4725 - val_prec: 0.1701 - val_recall: 0.1701 - val_prec_1: 0.1735 - val_recall_1: 0.1247 - val_prec_2: 0.1879 - val_recall_2: 0.1334 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0740 - val_f1_m: 0.4705 - val_recall_m: 0.4625 - val_precision_m: 0.4791 - val_precision: 0.4791 - val_recall_6: 0.4625 - val_f1score: 0.4705\n",
      "Epoch 59/300\n",
      "epoch:  58\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.3984 - acc: 0.8226 - prec: 0.9376 - recall: 0.9594 - prec_1: 0.8552 - recall_1: 0.8644 - prec_2: 0.9252 - recall_2: 0.9021 - prec_3: 0.6602 - recall_3: 0.6234 - prec_4: 0.6356 - recall_4: 0.6754 - prec_5: 0.9185 - recall_5: 0.9406 - f1_m: 0.8198 - recall_m: 0.8042 - precision_m: 0.8363 - precision: 0.8363 - recall_6: 0.8042 - f1score: 0.8198 - val_loss: 2.2539 - val_acc: 0.5055 - val_prec: 0.1759 - val_recall: 0.0974 - val_prec_1: 0.1653 - val_recall_1: 0.0982 - val_prec_2: 0.1855 - val_recall_2: 0.1471 - val_prec_3: 0.0640 - val_recall_3: 0.0260 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1637 - val_f1_m: 0.4930 - val_recall_m: 0.4753 - val_precision_m: 0.5142 - val_precision: 0.5142 - val_recall_6: 0.4753 - val_f1score: 0.4930\n",
      "Epoch 60/300\n",
      "epoch:  59\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3898 - acc: 0.8277 - prec: 0.9448 - recall: 0.9584 - prec_1: 0.8555 - recall_1: 0.8672 - prec_2: 0.9297 - recall_2: 0.9054 - prec_3: 0.6711 - recall_3: 0.6502 - prec_4: 0.6475 - recall_4: 0.6683 - prec_5: 0.9216 - recall_5: 0.9427 - f1_m: 0.8240 - recall_m: 0.8080 - precision_m: 0.8409 - precision: 0.8409 - recall_6: 0.8080 - f1score: 0.8240 - val_loss: 2.0579 - val_acc: 0.4995 - val_prec: 0.1759 - val_recall: 0.1092 - val_prec_1: 0.1670 - val_recall_1: 0.0889 - val_prec_2: 0.1748 - val_recall_2: 0.1889 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1439 - val_recall_4: 0.0315 - val_prec_5: 0.1684 - val_recall_5: 0.1160 - val_f1_m: 0.4893 - val_recall_m: 0.4758 - val_precision_m: 0.5057 - val_precision: 0.5057 - val_recall_6: 0.4758 - val_f1score: 0.4893\n",
      "Epoch 61/300\n",
      "epoch:  60\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3924 - acc: 0.8241 - prec: 0.9503 - recall: 0.9595 - prec_1: 0.8610 - recall_1: 0.8686 - prec_2: 0.9275 - recall_2: 0.9045 - prec_3: 0.6667 - recall_3: 0.6175 - prec_4: 0.6372 - recall_4: 0.6896 - prec_5: 0.9170 - recall_5: 0.9436 - f1_m: 0.8216 - recall_m: 0.8067 - precision_m: 0.8373 - precision: 0.8373 - recall_6: 0.8067 - f1score: 0.8216 - val_loss: 0.6946 - val_acc: 0.7296 - val_prec: 0.1759 - val_recall: 0.1653 - val_prec_1: 0.1749 - val_recall_1: 0.1113 - val_prec_2: 0.1887 - val_recall_2: 0.1693 - val_prec_3: 0.1709 - val_recall_3: 0.1314 - val_prec_4: 0.1439 - val_recall_4: 0.0712 - val_prec_5: 0.1684 - val_recall_5: 0.1426 - val_f1_m: 0.7037 - val_recall_m: 0.6697 - val_precision_m: 0.7473 - val_precision: 0.7473 - val_recall_6: 0.6697 - val_f1score: 0.7037\n",
      "Epoch 62/300\n",
      "epoch:  61\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3971 - acc: 0.8206 - prec: 0.9471 - recall: 0.9589 - prec_1: 0.8535 - recall_1: 0.8726 - prec_2: 0.9318 - recall_2: 0.9111 - prec_3: 0.6524 - recall_3: 0.6420 - prec_4: 0.6396 - recall_4: 0.6576 - prec_5: 0.9093 - recall_5: 0.9397 - f1_m: 0.8198 - recall_m: 0.8041 - precision_m: 0.8365 - precision: 0.8365 - recall_6: 0.8041 - f1score: 0.8198 - val_loss: 1.3208 - val_acc: 0.5595 - val_prec: 0.1759 - val_recall: 0.1473 - val_prec_1: 0.1599 - val_recall_1: 0.0904 - val_prec_2: 0.1919 - val_recall_2: 0.1182 - val_prec_3: 0.1759 - val_recall_3: 0.0763 - val_prec_4: 0.0640 - val_recall_4: 0.0073 - val_prec_5: 0.1684 - val_recall_5: 0.1519 - val_f1_m: 0.5522 - val_recall_m: 0.5390 - val_precision_m: 0.5670 - val_precision: 0.5670 - val_recall_6: 0.5390 - val_f1score: 0.5522\n",
      "Epoch 63/300\n",
      "epoch:  62\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3898 - acc: 0.8248 - prec: 0.9343 - recall: 0.9506 - prec_1: 0.8520 - recall_1: 0.8645 - prec_2: 0.9312 - recall_2: 0.9086 - prec_3: 0.6642 - recall_3: 0.6613 - prec_4: 0.6485 - recall_4: 0.6501 - prec_5: 0.9168 - recall_5: 0.9378 - f1_m: 0.8240 - recall_m: 0.8090 - precision_m: 0.8399 - precision: 0.8399 - recall_6: 0.8090 - f1score: 0.8240 - val_loss: 2.0702 - val_acc: 0.5297 - val_prec: 0.1676 - val_recall: 0.1726 - val_prec_1: 0.1731 - val_recall_1: 0.0990 - val_prec_2: 0.1866 - val_recall_2: 0.1422 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1439 - val_recall_4: 0.0655 - val_prec_5: 0.1684 - val_recall_5: 0.0766 - val_f1_m: 0.5244 - val_recall_m: 0.5105 - val_precision_m: 0.5404 - val_precision: 0.5404 - val_recall_6: 0.5105 - val_f1score: 0.5244\n",
      "Epoch 64/300\n",
      "epoch:  63\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3880 - acc: 0.8243 - prec: 0.9345 - recall: 0.9561 - prec_1: 0.8512 - recall_1: 0.8732 - prec_2: 0.9419 - recall_2: 0.9078 - prec_3: 0.6576 - recall_3: 0.6330 - prec_4: 0.6413 - recall_4: 0.6665 - prec_5: 0.9155 - recall_5: 0.9384 - f1_m: 0.8217 - recall_m: 0.8053 - precision_m: 0.8390 - precision: 0.8390 - recall_6: 0.8053 - f1score: 0.8217 - val_loss: 2.1983 - val_acc: 0.5752 - val_prec: 0.1748 - val_recall: 0.1686 - val_prec_1: 0.1743 - val_recall_1: 0.1397 - val_prec_2: 0.1809 - val_recall_2: 0.1647 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1377 - val_f1_m: 0.5788 - val_recall_m: 0.5682 - val_precision_m: 0.5905 - val_precision: 0.5905 - val_recall_6: 0.5682 - val_f1score: 0.5788\n",
      "Epoch 65/300\n",
      "epoch:  64\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3841 - acc: 0.8251 - prec: 0.9490 - recall: 0.9551 - prec_1: 0.8602 - recall_1: 0.8671 - prec_2: 0.9295 - recall_2: 0.9080 - prec_3: 0.6582 - recall_3: 0.6414 - prec_4: 0.6452 - recall_4: 0.6761 - prec_5: 0.9140 - recall_5: 0.9458 - f1_m: 0.8224 - recall_m: 0.8072 - precision_m: 0.8384 - precision: 0.8384 - recall_6: 0.8072 - f1score: 0.8224 - val_loss: 0.8143 - val_acc: 0.6777 - val_prec: 0.1759 - val_recall: 0.1628 - val_prec_1: 0.1734 - val_recall_1: 0.1726 - val_prec_2: 0.1919 - val_recall_2: 0.1134 - val_prec_3: 0.0960 - val_recall_3: 0.0385 - val_prec_4: 0.1759 - val_recall_4: 0.1234 - val_prec_5: 0.1684 - val_recall_5: 0.1116 - val_f1_m: 0.6772 - val_recall_m: 0.6717 - val_precision_m: 0.6829 - val_precision: 0.6829 - val_recall_6: 0.6717 - val_f1score: 0.6772\n",
      "Epoch 66/300\n",
      "epoch:  65\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3834 - acc: 0.8246 - prec: 0.9389 - recall: 0.9541 - prec_1: 0.8618 - recall_1: 0.8751 - prec_2: 0.9169 - recall_2: 0.9063 - prec_3: 0.6716 - recall_3: 0.6481 - prec_4: 0.6398 - recall_4: 0.6650 - prec_5: 0.9208 - recall_5: 0.9332 - f1_m: 0.8237 - recall_m: 0.8082 - precision_m: 0.8401 - precision: 0.8401 - recall_6: 0.8082 - f1score: 0.8237 - val_loss: 0.6221 - val_acc: 0.7289 - val_prec: 0.1759 - val_recall: 0.1681 - val_prec_1: 0.1735 - val_recall_1: 0.1629 - val_prec_2: 0.1919 - val_recall_2: 0.1392 - val_prec_3: 0.0800 - val_recall_3: 0.0169 - val_prec_4: 0.1809 - val_recall_4: 0.1704 - val_prec_5: 0.1684 - val_recall_5: 0.1302 - val_f1_m: 0.7174 - val_recall_m: 0.6989 - val_precision_m: 0.7467 - val_precision: 0.7467 - val_recall_6: 0.6989 - val_f1score: 0.7174\n",
      "Epoch 67/300\n",
      "epoch:  66\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3811 - acc: 0.8257 - prec: 0.9411 - recall: 0.9573 - prec_1: 0.8560 - recall_1: 0.8747 - prec_2: 0.9221 - recall_2: 0.8997 - prec_3: 0.6647 - recall_3: 0.6413 - prec_4: 0.6404 - recall_4: 0.6642 - prec_5: 0.9224 - recall_5: 0.9406 - f1_m: 0.8246 - recall_m: 0.8110 - precision_m: 0.8389 - precision: 0.8389 - recall_6: 0.8110 - f1score: 0.8246 - val_loss: 4.3061 - val_acc: 0.4175 - val_prec: 0.1665 - val_recall: 0.1733 - val_prec_1: 0.1599 - val_recall_1: 0.0779 - val_prec_2: 0.1866 - val_recall_2: 0.1353 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0552 - val_f1_m: 0.4167 - val_recall_m: 0.4085 - val_precision_m: 0.4263 - val_precision: 0.4263 - val_recall_6: 0.4085 - val_f1score: 0.4167\n",
      "Epoch 68/300\n",
      "epoch:  67\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3839 - acc: 0.8317 - prec: 0.9413 - recall: 0.9639 - prec_1: 0.8750 - recall_1: 0.8761 - prec_2: 0.9319 - recall_2: 0.9142 - prec_3: 0.6735 - recall_3: 0.6483 - prec_4: 0.6524 - recall_4: 0.6779 - prec_5: 0.9162 - recall_5: 0.9441 - f1_m: 0.8274 - recall_m: 0.8116 - precision_m: 0.8441 - precision: 0.8441 - recall_6: 0.8116 - f1score: 0.8274 - val_loss: 5.9876 - val_acc: 0.4410 - val_prec: 0.1759 - val_recall: 0.1526 - val_prec_1: 0.1741 - val_recall_1: 0.0429 - val_prec_2: 0.1839 - val_recall_2: 0.1125 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1590 - val_f1_m: 0.4382 - val_recall_m: 0.4300 - val_precision_m: 0.4473 - val_precision: 0.4473 - val_recall_6: 0.4300 - val_f1score: 0.4382\n",
      "Epoch 69/300\n",
      "epoch:  68\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3848 - acc: 0.8272 - prec: 0.9478 - recall: 0.9641 - prec_1: 0.8665 - recall_1: 0.8774 - prec_2: 0.9243 - recall_2: 0.9207 - prec_3: 0.6585 - recall_3: 0.6460 - prec_4: 0.6460 - recall_4: 0.6563 - prec_5: 0.9280 - recall_5: 0.9350 - f1_m: 0.8265 - recall_m: 0.8126 - precision_m: 0.8412 - precision: 0.8412 - recall_6: 0.8126 - f1score: 0.8265 - val_loss: 2.3201 - val_acc: 0.5012 - val_prec: 0.1696 - val_recall: 0.1728 - val_prec_1: 0.1743 - val_recall_1: 0.1025 - val_prec_2: 0.1855 - val_recall_2: 0.1416 - val_prec_3: 0.1759 - val_recall_3: 0.0999 - val_prec_4: 0.0960 - val_recall_4: 0.0040 - val_prec_5: 0.1119 - val_recall_5: 0.0156 - val_f1_m: 0.4954 - val_recall_m: 0.4883 - val_precision_m: 0.5032 - val_precision: 0.5032 - val_recall_6: 0.4883 - val_f1score: 0.4954\n",
      "Epoch 70/300\n",
      "epoch:  69\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3775 - acc: 0.8303 - prec: 0.9524 - recall: 0.9597 - prec_1: 0.8544 - recall_1: 0.8883 - prec_2: 0.9314 - recall_2: 0.9036 - prec_3: 0.6597 - recall_3: 0.6356 - prec_4: 0.6499 - recall_4: 0.6789 - prec_5: 0.9210 - recall_5: 0.9412 - f1_m: 0.8277 - recall_m: 0.8130 - precision_m: 0.8432 - precision: 0.8432 - recall_6: 0.8130 - f1score: 0.8277 - val_loss: 5.4114 - val_acc: 0.4413 - val_prec: 0.1701 - val_recall: 0.1711 - val_prec_1: 0.1599 - val_recall_1: 0.0364 - val_prec_2: 0.1831 - val_recall_2: 0.1818 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0879 - val_f1_m: 0.4349 - val_recall_m: 0.4290 - val_precision_m: 0.4416 - val_precision: 0.4416 - val_recall_6: 0.4290 - val_f1score: 0.4349\n",
      "Epoch 71/300\n",
      "epoch:  70\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3801 - acc: 0.8277 - prec: 0.9478 - recall: 0.9616 - prec_1: 0.8726 - recall_1: 0.8832 - prec_2: 0.9319 - recall_2: 0.9136 - prec_3: 0.6567 - recall_3: 0.6231 - prec_4: 0.6383 - recall_4: 0.6783 - prec_5: 0.9289 - recall_5: 0.9447 - f1_m: 0.8246 - recall_m: 0.8098 - precision_m: 0.8401 - precision: 0.8401 - recall_6: 0.8098 - f1score: 0.8246 - val_loss: 0.8155 - val_acc: 0.6239 - val_prec: 0.1686 - val_recall: 0.1702 - val_prec_1: 0.1755 - val_recall_1: 0.0568 - val_prec_2: 0.1851 - val_recall_2: 0.1394 - val_prec_3: 0.1709 - val_recall_3: 0.1737 - val_prec_4: 0.0960 - val_recall_4: 0.0057 - val_prec_5: 0.1684 - val_recall_5: 0.1179 - val_f1_m: 0.6071 - val_recall_m: 0.5887 - val_precision_m: 0.6293 - val_precision: 0.6293 - val_recall_6: 0.5887 - val_f1score: 0.6071\n",
      "Epoch 72/300\n",
      "epoch:  71\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.3810 - acc: 0.8250 - prec: 0.9490 - recall: 0.9563 - prec_1: 0.8667 - recall_1: 0.8833 - prec_2: 0.9367 - recall_2: 0.9178 - prec_3: 0.6529 - recall_3: 0.6335 - prec_4: 0.6399 - recall_4: 0.6587 - prec_5: 0.9181 - recall_5: 0.9428 - f1_m: 0.8236 - recall_m: 0.8095 - precision_m: 0.8386 - precision: 0.8386 - recall_6: 0.8095 - f1score: 0.8236 - val_loss: 1.8757 - val_acc: 0.5477 - val_prec: 0.1709 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.0900 - val_prec_2: 0.1866 - val_recall_2: 0.1614 - val_prec_3: 0.1599 - val_recall_3: 0.0845 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0787 - val_f1_m: 0.5368 - val_recall_m: 0.5190 - val_precision_m: 0.5603 - val_precision: 0.5603 - val_recall_6: 0.5190 - val_f1score: 0.5368\n",
      "Epoch 73/300\n",
      "epoch:  72\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3720 - acc: 0.8325 - prec: 0.9448 - recall: 0.9594 - prec_1: 0.8656 - recall_1: 0.8806 - prec_2: 0.9325 - recall_2: 0.9179 - prec_3: 0.6649 - recall_3: 0.6408 - prec_4: 0.6528 - recall_4: 0.6723 - prec_5: 0.9289 - recall_5: 0.9407 - f1_m: 0.8315 - recall_m: 0.8180 - precision_m: 0.8458 - precision: 0.8458 - recall_6: 0.8180 - f1score: 0.8315 - val_loss: 0.8357 - val_acc: 0.7304 - val_prec: 0.1748 - val_recall: 0.1583 - val_prec_1: 0.1748 - val_recall_1: 0.1268 - val_prec_2: 0.1896 - val_recall_2: 0.1578 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1809 - val_recall_4: 0.1912 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.7244 - val_recall_m: 0.7074 - val_precision_m: 0.7435 - val_precision: 0.7435 - val_recall_6: 0.7074 - val_f1score: 0.7244\n",
      "Epoch 74/300\n",
      "epoch:  73\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3709 - acc: 0.8346 - prec: 0.9500 - recall: 0.9604 - prec_1: 0.8719 - recall_1: 0.8811 - prec_2: 0.9325 - recall_2: 0.9229 - prec_3: 0.6773 - recall_3: 0.6400 - prec_4: 0.6563 - recall_4: 0.6930 - prec_5: 0.9191 - recall_5: 0.9443 - f1_m: 0.8313 - recall_m: 0.8177 - precision_m: 0.8456 - precision: 0.8456 - recall_6: 0.8177 - f1score: 0.8313 - val_loss: 1.2512 - val_acc: 0.5290 - val_prec: 0.1675 - val_recall: 0.1721 - val_prec_1: 0.1736 - val_recall_1: 0.0849 - val_prec_2: 0.1896 - val_recall_2: 0.1614 - val_prec_3: 0.1599 - val_recall_3: 0.0711 - val_prec_4: 0.0160 - val_recall_4: 2.4988e-04 - val_prec_5: 0.1684 - val_recall_5: 0.0740 - val_f1_m: 0.5083 - val_recall_m: 0.4870 - val_precision_m: 0.5549 - val_precision: 0.5549 - val_recall_6: 0.4870 - val_f1score: 0.5083\n",
      "Epoch 75/300\n",
      "epoch:  74\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3711 - acc: 0.8311 - prec: 0.9338 - recall: 0.9632 - prec_1: 0.8666 - recall_1: 0.8672 - prec_2: 0.9263 - recall_2: 0.9173 - prec_3: 0.6706 - recall_3: 0.6539 - prec_4: 0.6627 - recall_4: 0.6738 - prec_5: 0.9291 - recall_5: 0.9454 - f1_m: 0.8311 - recall_m: 0.8175 - precision_m: 0.8456 - precision: 0.8456 - recall_6: 0.8175 - f1score: 0.8311 - val_loss: 1.3245 - val_acc: 0.6109 - val_prec: 0.1690 - val_recall: 0.1703 - val_prec_1: 0.1738 - val_recall_1: 0.1215 - val_prec_2: 0.1812 - val_recall_2: 0.1478 - val_prec_3: 0.1279 - val_recall_3: 0.0665 - val_prec_4: 0.1439 - val_recall_4: 0.0345 - val_prec_5: 0.1684 - val_recall_5: 0.1039 - val_f1_m: 0.6021 - val_recall_m: 0.5887 - val_precision_m: 0.6172 - val_precision: 0.6172 - val_recall_6: 0.5887 - val_f1score: 0.6021\n",
      "Epoch 76/300\n",
      "epoch:  75\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3688 - acc: 0.8347 - prec: 0.9457 - recall: 0.9609 - prec_1: 0.8802 - recall_1: 0.8866 - prec_2: 0.9334 - recall_2: 0.9188 - prec_3: 0.6687 - recall_3: 0.6420 - prec_4: 0.6476 - recall_4: 0.6738 - prec_5: 0.9308 - recall_5: 0.9482 - f1_m: 0.8318 - recall_m: 0.8167 - precision_m: 0.8476 - precision: 0.8476 - recall_6: 0.8167 - f1score: 0.8318 - val_loss: 0.7849 - val_acc: 0.6504 - val_prec: 0.1759 - val_recall: 0.1658 - val_prec_1: 0.1719 - val_recall_1: 0.0551 - val_prec_2: 0.1919 - val_recall_2: 0.1129 - val_prec_3: 0.1709 - val_recall_3: 0.1717 - val_prec_4: 0.1279 - val_recall_4: 0.0150 - val_prec_5: 0.1684 - val_recall_5: 0.1660 - val_f1_m: 0.6370 - val_recall_m: 0.6222 - val_precision_m: 0.6543 - val_precision: 0.6543 - val_recall_6: 0.6222 - val_f1score: 0.6370\n",
      "Epoch 77/300\n",
      "epoch:  76\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.3718 - acc: 0.8292 - prec: 0.9492 - recall: 0.9625 - prec_1: 0.8688 - recall_1: 0.8843 - prec_2: 0.9292 - recall_2: 0.9213 - prec_3: 0.6621 - recall_3: 0.6350 - prec_4: 0.6422 - recall_4: 0.6644 - prec_5: 0.9292 - recall_5: 0.9411 - f1_m: 0.8282 - recall_m: 0.8151 - precision_m: 0.8421 - precision: 0.8421 - recall_6: 0.8151 - f1score: 0.8282 - val_loss: 2.1867 - val_acc: 0.5855 - val_prec: 0.1759 - val_recall: 0.1285 - val_prec_1: 0.1730 - val_recall_1: 0.1555 - val_prec_2: 0.1828 - val_recall_2: 0.1411 - val_prec_3: 0.1439 - val_recall_3: 0.0472 - val_prec_4: 0.1439 - val_recall_4: 0.0408 - val_prec_5: 0.1684 - val_recall_5: 0.1125 - val_f1_m: 0.5456 - val_recall_m: 0.5410 - val_precision_m: 0.5512 - val_precision: 0.5512 - val_recall_6: 0.5410 - val_f1score: 0.5456\n",
      "Epoch 78/300\n",
      "epoch:  77\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3639 - acc: 0.8357 - prec: 0.9503 - recall: 0.9611 - prec_1: 0.8808 - recall_1: 0.8894 - prec_2: 0.9336 - recall_2: 0.9211 - prec_3: 0.6732 - recall_3: 0.6583 - prec_4: 0.6604 - recall_4: 0.6752 - prec_5: 0.9320 - recall_5: 0.9490 - f1_m: 0.8351 - recall_m: 0.8215 - precision_m: 0.8493 - precision: 0.8493 - recall_6: 0.8215 - f1score: 0.8351 - val_loss: 4.6360 - val_acc: 0.4320 - val_prec: 0.1753 - val_recall: 0.1385 - val_prec_1: 0.1695 - val_recall_1: 0.1158 - val_prec_2: 0.1882 - val_recall_2: 0.1788 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1364 - val_recall_5: 0.0335 - val_f1_m: 0.4308 - val_recall_m: 0.4253 - val_precision_m: 0.4369 - val_precision: 0.4369 - val_recall_6: 0.4253 - val_f1score: 0.4308\n",
      "Epoch 79/300\n",
      "epoch:  78\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.3626 - acc: 0.8368 - prec: 0.9549 - recall: 0.9657 - prec_1: 0.8808 - recall_1: 0.8935 - prec_2: 0.9298 - recall_2: 0.9211 - prec_3: 0.6715 - recall_3: 0.6459 - prec_4: 0.6542 - recall_4: 0.6882 - prec_5: 0.9383 - recall_5: 0.9448 - f1_m: 0.8353 - recall_m: 0.8232 - precision_m: 0.8480 - precision: 0.8480 - recall_6: 0.8232 - f1score: 0.8353 - val_loss: 1.0227 - val_acc: 0.7231 - val_prec: 0.1753 - val_recall: 0.1588 - val_prec_1: 0.1746 - val_recall_1: 0.1549 - val_prec_2: 0.1879 - val_recall_2: 0.1664 - val_prec_3: 0.1279 - val_recall_3: 0.0420 - val_prec_4: 0.1649 - val_recall_4: 0.1119 - val_prec_5: 0.1684 - val_recall_5: 0.1405 - val_f1_m: 0.7157 - val_recall_m: 0.7019 - val_precision_m: 0.7306 - val_precision: 0.7306 - val_recall_6: 0.7019 - val_f1score: 0.7157\n",
      "Epoch 80/300\n",
      "epoch:  79\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3642 - acc: 0.8355 - prec: 0.9467 - recall: 0.9647 - prec_1: 0.8711 - recall_1: 0.8886 - prec_2: 0.9400 - recall_2: 0.9234 - prec_3: 0.6709 - recall_3: 0.6802 - prec_4: 0.6634 - recall_4: 0.6579 - prec_5: 0.9325 - recall_5: 0.9418 - f1_m: 0.8355 - recall_m: 0.8228 - precision_m: 0.8487 - precision: 0.8487 - recall_6: 0.8228 - f1score: 0.8355 - val_loss: 3.5151 - val_acc: 0.5192 - val_prec: 0.1759 - val_recall: 0.1273 - val_prec_1: 0.1730 - val_recall_1: 0.1471 - val_prec_2: 0.1805 - val_recall_2: 0.1664 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1119 - val_f1_m: 0.5179 - val_recall_m: 0.5065 - val_precision_m: 0.5320 - val_precision: 0.5320 - val_recall_6: 0.5065 - val_f1score: 0.5179\n",
      "Epoch 81/300\n",
      "epoch:  80\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.3622 - acc: 0.8383 - prec: 0.9489 - recall: 0.9608 - prec_1: 0.8676 - recall_1: 0.8821 - prec_2: 0.9338 - recall_2: 0.9191 - prec_3: 0.6759 - recall_3: 0.6634 - prec_4: 0.6656 - recall_4: 0.6783 - prec_5: 0.9240 - recall_5: 0.9472 - f1_m: 0.8356 - recall_m: 0.8217 - precision_m: 0.8502 - precision: 0.8502 - recall_6: 0.8217 - f1score: 0.8356 - val_loss: 0.8267 - val_acc: 0.6612 - val_prec: 0.1759 - val_recall: 0.1634 - val_prec_1: 0.1599 - val_recall_1: 0.0411 - val_prec_2: 0.1782 - val_recall_2: 0.1541 - val_prec_3: 0.1709 - val_recall_3: 0.1749 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1640 - val_f1_m: 0.6321 - val_recall_m: 0.6079 - val_precision_m: 0.6638 - val_precision: 0.6638 - val_recall_6: 0.6079 - val_f1score: 0.6321\n",
      "Epoch 82/300\n",
      "epoch:  81\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3636 - acc: 0.8315 - prec: 0.9590 - recall: 0.9593 - prec_1: 0.8700 - recall_1: 0.8895 - prec_2: 0.9298 - recall_2: 0.9157 - prec_3: 0.6664 - recall_3: 0.6370 - prec_4: 0.6489 - recall_4: 0.6772 - prec_5: 0.9286 - recall_5: 0.9455 - f1_m: 0.8296 - recall_m: 0.8172 - precision_m: 0.8426 - precision: 0.8426 - recall_6: 0.8172 - f1score: 0.8296 - val_loss: 2.0036 - val_acc: 0.5777 - val_prec: 0.1671 - val_recall: 0.1732 - val_prec_1: 0.1756 - val_recall_1: 0.0668 - val_prec_2: 0.1839 - val_recall_2: 0.1233 - val_prec_3: 0.1709 - val_recall_3: 0.1307 - val_prec_4: 0.1599 - val_recall_4: 0.0400 - val_prec_5: 0.1684 - val_recall_5: 0.0784 - val_f1_m: 0.5697 - val_recall_m: 0.5597 - val_precision_m: 0.5811 - val_precision: 0.5811 - val_recall_6: 0.5597 - val_f1score: 0.5697\n",
      "Epoch 83/300\n",
      "epoch:  82\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.3585 - acc: 0.8395 - prec: 0.9517 - recall: 0.9687 - prec_1: 0.8783 - recall_1: 0.8957 - prec_2: 0.9496 - recall_2: 0.9188 - prec_3: 0.6761 - recall_3: 0.6630 - prec_4: 0.6633 - recall_4: 0.6776 - prec_5: 0.9232 - recall_5: 0.9459 - f1_m: 0.8388 - recall_m: 0.8267 - precision_m: 0.8514 - precision: 0.8514 - recall_6: 0.8267 - f1score: 0.8388 - val_loss: 3.2539 - val_acc: 0.4595 - val_prec: 0.1730 - val_recall: 0.1742 - val_prec_1: 0.1738 - val_recall_1: 0.1135 - val_prec_2: 0.1759 - val_recall_2: 0.0848 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1107 - val_f1_m: 0.4576 - val_recall_m: 0.4475 - val_precision_m: 0.4690 - val_precision: 0.4690 - val_recall_6: 0.4475 - val_f1score: 0.4576\n",
      "Epoch 84/300\n",
      "epoch:  83\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3550 - acc: 0.8353 - prec: 0.9569 - recall: 0.9688 - prec_1: 0.8798 - recall_1: 0.9006 - prec_2: 0.9349 - recall_2: 0.9212 - prec_3: 0.6649 - recall_3: 0.6492 - prec_4: 0.6487 - recall_4: 0.6704 - prec_5: 0.9374 - recall_5: 0.9474 - f1_m: 0.8346 - recall_m: 0.8223 - precision_m: 0.8474 - precision: 0.8474 - recall_6: 0.8223 - f1score: 0.8346 - val_loss: 6.9528 - val_acc: 0.3631 - val_prec: 0.0800 - val_recall: 0.0037 - val_prec_1: 0.1609 - val_recall_1: 0.0829 - val_prec_2: 0.1789 - val_recall_2: 0.1375 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1575 - val_f1_m: 0.3611 - val_recall_m: 0.3576 - val_precision_m: 0.3647 - val_precision: 0.3647 - val_recall_6: 0.3576 - val_f1score: 0.3611\n",
      "Epoch 85/300\n",
      "epoch:  84\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3558 - acc: 0.8407 - prec: 0.9530 - recall: 0.9636 - prec_1: 0.8823 - recall_1: 0.8966 - prec_2: 0.9389 - recall_2: 0.9249 - prec_3: 0.6800 - recall_3: 0.6528 - prec_4: 0.6585 - recall_4: 0.6847 - prec_5: 0.9398 - recall_5: 0.9453 - f1_m: 0.8380 - recall_m: 0.8260 - precision_m: 0.8506 - precision: 0.8506 - recall_6: 0.8260 - f1score: 0.8380 - val_loss: 2.3296 - val_acc: 0.5052 - val_prec: 0.1748 - val_recall: 0.1651 - val_prec_1: 0.1735 - val_recall_1: 0.1126 - val_prec_2: 0.1839 - val_recall_2: 0.1440 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1653 - val_recall_5: 0.1160 - val_f1_m: 0.4989 - val_recall_m: 0.4823 - val_precision_m: 0.5186 - val_precision: 0.5186 - val_recall_6: 0.4823 - val_f1score: 0.4989\n",
      "Epoch 86/300\n",
      "epoch:  85\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3569 - acc: 0.8385 - prec: 0.9444 - recall: 0.9659 - prec_1: 0.8756 - recall_1: 0.8930 - prec_2: 0.9371 - recall_2: 0.9278 - prec_3: 0.6726 - recall_3: 0.6599 - prec_4: 0.6580 - recall_4: 0.6771 - prec_5: 0.9365 - recall_5: 0.9394 - f1_m: 0.8369 - recall_m: 0.8233 - precision_m: 0.8512 - precision: 0.8512 - recall_6: 0.8233 - f1score: 0.8369 - val_loss: 1.8269 - val_acc: 0.5032 - val_prec: 0.1668 - val_recall: 0.1752 - val_prec_1: 0.1759 - val_recall_1: 0.0699 - val_prec_2: 0.1791 - val_recall_2: 0.1333 - val_prec_3: 0.1279 - val_recall_3: 0.0547 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0975 - val_f1_m: 0.4962 - val_recall_m: 0.4805 - val_precision_m: 0.5168 - val_precision: 0.5168 - val_recall_6: 0.4805 - val_f1score: 0.4962\n",
      "Epoch 87/300\n",
      "epoch:  86\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 6s 692us/step - loss: 0.3568 - acc: 0.8370 - prec: 0.9581 - recall: 0.9635 - prec_1: 0.8881 - recall_1: 0.8997 - prec_2: 0.9346 - recall_2: 0.9252 - prec_3: 0.6770 - recall_3: 0.6323 - prec_4: 0.6436 - recall_4: 0.6871 - prec_5: 0.9287 - recall_5: 0.9455 - f1_m: 0.8354 - recall_m: 0.8248 - precision_m: 0.8463 - precision: 0.8463 - recall_6: 0.8248 - f1score: 0.8354 - val_loss: 2.8570 - val_acc: 0.6299 - val_prec: 0.1759 - val_recall: 0.1616 - val_prec_1: 0.1740 - val_recall_1: 0.1641 - val_prec_2: 0.1919 - val_recall_2: 0.1517 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1439 - val_recall_4: 0.0602 - val_prec_5: 0.1684 - val_recall_5: 0.1287 - val_f1_m: 0.6289 - val_recall_m: 0.6207 - val_precision_m: 0.6376 - val_precision: 0.6376 - val_recall_6: 0.6207 - val_f1score: 0.6289\n",
      "Epoch 88/300\n",
      "epoch:  87\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3506 - acc: 0.8422 - prec: 0.9530 - recall: 0.9636 - prec_1: 0.8971 - recall_1: 0.8972 - prec_2: 0.9312 - recall_2: 0.9299 - prec_3: 0.6782 - recall_3: 0.6700 - prec_4: 0.6616 - recall_4: 0.6730 - prec_5: 0.9375 - recall_5: 0.9527 - f1_m: 0.8402 - recall_m: 0.8291 - precision_m: 0.8518 - precision: 0.8518 - recall_6: 0.8291 - f1score: 0.8402 - val_loss: 4.6208 - val_acc: 0.4690 - val_prec: 0.1723 - val_recall: 0.1747 - val_prec_1: 0.1741 - val_recall_1: 0.1251 - val_prec_2: 0.1919 - val_recall_2: 0.1036 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0953 - val_f1_m: 0.4657 - val_recall_m: 0.4590 - val_precision_m: 0.4730 - val_precision: 0.4730 - val_recall_6: 0.4590 - val_f1score: 0.4657\n",
      "Epoch 89/300\n",
      "epoch:  88\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3548 - acc: 0.8388 - prec: 0.9538 - recall: 0.9615 - prec_1: 0.8832 - recall_1: 0.9006 - prec_2: 0.9373 - recall_2: 0.9280 - prec_3: 0.6788 - recall_3: 0.6409 - prec_4: 0.6499 - recall_4: 0.6864 - prec_5: 0.9389 - recall_5: 0.9520 - f1_m: 0.8368 - recall_m: 0.8256 - precision_m: 0.8485 - precision: 0.8485 - recall_6: 0.8256 - f1score: 0.8368 - val_loss: 2.9707 - val_acc: 0.5007 - val_prec: 0.1713 - val_recall: 0.1509 - val_prec_1: 0.1734 - val_recall_1: 0.1271 - val_prec_2: 0.1759 - val_recall_2: 0.0608 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1649 - val_recall_4: 0.1242 - val_prec_5: 0.1684 - val_recall_5: 0.0696 - val_f1_m: 0.4970 - val_recall_m: 0.4903 - val_precision_m: 0.5043 - val_precision: 0.5043 - val_recall_6: 0.4903 - val_f1score: 0.4970\n",
      "Epoch 90/300\n",
      "epoch:  89\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3532 - acc: 0.8392 - prec: 0.9443 - recall: 0.9612 - prec_1: 0.8876 - recall_1: 0.8927 - prec_2: 0.9357 - recall_2: 0.9281 - prec_3: 0.6784 - recall_3: 0.6533 - prec_4: 0.6543 - recall_4: 0.6690 - prec_5: 0.9393 - recall_5: 0.9507 - f1_m: 0.8381 - recall_m: 0.8260 - precision_m: 0.8509 - precision: 0.8509 - recall_6: 0.8260 - f1score: 0.8381 - val_loss: 5.5853 - val_acc: 0.3656 - val_prec: 0.1667 - val_recall: 0.1732 - val_prec_1: 0.1599 - val_recall_1: 0.0602 - val_prec_2: 0.1599 - val_recall_2: 0.0719 - val_prec_3: 0.1119 - val_recall_3: 0.0250 - val_prec_4: 0.0160 - val_recall_4: 2.4988e-04 - val_prec_5: 0.1684 - val_recall_5: 0.0566 - val_f1_m: 0.3616 - val_recall_m: 0.3583 - val_precision_m: 0.3652 - val_precision: 0.3652 - val_recall_6: 0.3583 - val_f1score: 0.3616\n",
      "Epoch 91/300\n",
      "epoch:  90\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3520 - acc: 0.8393 - prec: 0.9555 - recall: 0.9661 - prec_1: 0.8808 - recall_1: 0.9145 - prec_2: 0.9474 - recall_2: 0.9222 - prec_3: 0.6633 - recall_3: 0.6504 - prec_4: 0.6488 - recall_4: 0.6683 - prec_5: 0.9465 - recall_5: 0.9453 - f1_m: 0.8386 - recall_m: 0.8273 - precision_m: 0.8505 - precision: 0.8505 - recall_6: 0.8273 - f1score: 0.8386 - val_loss: 5.6108 - val_acc: 0.3553 - val_prec: 0.1670 - val_recall: 0.1369 - val_prec_1: 0.1693 - val_recall_1: 0.1175 - val_prec_2: 0.1919 - val_recall_2: 0.1008 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1204 - val_recall_5: 0.0235 - val_f1_m: 0.3542 - val_recall_m: 0.3516 - val_precision_m: 0.3571 - val_precision: 0.3571 - val_recall_6: 0.3516 - val_f1score: 0.3542\n",
      "Epoch 92/300\n",
      "epoch:  91\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.3516 - acc: 0.8365 - prec: 0.9529 - recall: 0.9628 - prec_1: 0.8768 - recall_1: 0.8925 - prec_2: 0.9338 - recall_2: 0.9260 - prec_3: 0.6775 - recall_3: 0.6563 - prec_4: 0.6480 - recall_4: 0.6721 - prec_5: 0.9393 - recall_5: 0.9449 - f1_m: 0.8361 - recall_m: 0.8252 - precision_m: 0.8475 - precision: 0.8475 - recall_6: 0.8252 - f1score: 0.8361 - val_loss: 1.3192 - val_acc: 0.6612 - val_prec: 0.1759 - val_recall: 0.1383 - val_prec_1: 0.1546 - val_recall_1: 0.0401 - val_prec_2: 0.1830 - val_recall_2: 0.1755 - val_prec_3: 0.1279 - val_recall_3: 0.0420 - val_prec_4: 0.1809 - val_recall_4: 0.1689 - val_prec_5: 0.1684 - val_recall_5: 0.1585 - val_f1_m: 0.6594 - val_recall_m: 0.6522 - val_precision_m: 0.6674 - val_precision: 0.6674 - val_recall_6: 0.6522 - val_f1score: 0.6594\n",
      "Epoch 93/300\n",
      "epoch:  92\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.3485 - acc: 0.8413 - prec: 0.9591 - recall: 0.9707 - prec_1: 0.8895 - recall_1: 0.9003 - prec_2: 0.9412 - recall_2: 0.9264 - prec_3: 0.6709 - recall_3: 0.6524 - prec_4: 0.6525 - recall_4: 0.6769 - prec_5: 0.9412 - recall_5: 0.9456 - f1_m: 0.8407 - recall_m: 0.8297 - precision_m: 0.8521 - precision: 0.8521 - recall_6: 0.8297 - f1score: 0.8407 - val_loss: 3.4918 - val_acc: 0.4415 - val_prec: 0.1678 - val_recall: 0.1726 - val_prec_1: 0.1741 - val_recall_1: 0.1012 - val_prec_2: 0.1799 - val_recall_2: 0.1377 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0567 - val_f1_m: 0.4392 - val_recall_m: 0.4320 - val_precision_m: 0.4469 - val_precision: 0.4469 - val_recall_6: 0.4320 - val_f1score: 0.4392\n",
      "Epoch 94/300\n",
      "epoch:  93\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3511 - acc: 0.8416 - prec: 0.9538 - recall: 0.9658 - prec_1: 0.8767 - recall_1: 0.8930 - prec_2: 0.9354 - recall_2: 0.9208 - prec_3: 0.6833 - recall_3: 0.6817 - prec_4: 0.6700 - recall_4: 0.6777 - prec_5: 0.9339 - recall_5: 0.9505 - f1_m: 0.8409 - recall_m: 0.8300 - precision_m: 0.8524 - precision: 0.8524 - recall_6: 0.8300 - f1score: 0.8409 - val_loss: 1.3968 - val_acc: 0.6142 - val_prec: 0.1759 - val_recall: 0.1313 - val_prec_1: 0.1599 - val_recall_1: 0.0611 - val_prec_2: 0.1829 - val_recall_2: 0.1894 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1809 - val_recall_4: 0.1829 - val_prec_5: 0.1684 - val_recall_5: 0.1115 - val_f1_m: 0.6121 - val_recall_m: 0.6077 - val_precision_m: 0.6168 - val_precision: 0.6168 - val_recall_6: 0.6077 - val_f1score: 0.6121\n",
      "Epoch 95/300\n",
      "epoch:  94\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3435 - acc: 0.8468 - prec: 0.9582 - recall: 0.9654 - prec_1: 0.8926 - recall_1: 0.9008 - prec_2: 0.9396 - recall_2: 0.9268 - prec_3: 0.6897 - recall_3: 0.6800 - prec_4: 0.6744 - recall_4: 0.6830 - prec_5: 0.9301 - recall_5: 0.9512 - f1_m: 0.8448 - recall_m: 0.8336 - precision_m: 0.8565 - precision: 0.8565 - recall_6: 0.8336 - f1score: 0.8448 - val_loss: 0.6025 - val_acc: 0.7179 - val_prec: 0.1759 - val_recall: 0.1395 - val_prec_1: 0.1735 - val_recall_1: 0.1419 - val_prec_2: 0.1919 - val_recall_2: 0.1349 - val_prec_3: 0.1709 - val_recall_3: 0.1524 - val_prec_4: 0.1119 - val_recall_4: 0.0422 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.7160 - val_recall_m: 0.7101 - val_precision_m: 0.7222 - val_precision: 0.7222 - val_recall_6: 0.7101 - val_f1score: 0.7160\n",
      "Epoch 96/300\n",
      "epoch:  95\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.3480 - acc: 0.8423 - prec: 0.9461 - recall: 0.9624 - prec_1: 0.8846 - recall_1: 0.9156 - prec_2: 0.9483 - recall_2: 0.9295 - prec_3: 0.6730 - recall_3: 0.6481 - prec_4: 0.6519 - recall_4: 0.6804 - prec_5: 0.9485 - recall_5: 0.9487 - f1_m: 0.8411 - recall_m: 0.8310 - precision_m: 0.8517 - precision: 0.8517 - recall_6: 0.8310 - f1score: 0.8411 - val_loss: 5.2278 - val_acc: 0.4613 - val_prec: 0.1667 - val_recall: 0.1716 - val_prec_1: 0.1759 - val_recall_1: 0.0732 - val_prec_2: 0.1791 - val_recall_2: 0.1670 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0780 - val_f1_m: 0.4575 - val_recall_m: 0.4475 - val_precision_m: 0.4691 - val_precision: 0.4691 - val_recall_6: 0.4475 - val_f1score: 0.4575\n",
      "Epoch 97/300\n",
      "epoch:  96\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.3488 - acc: 0.8411 - prec: 0.9596 - recall: 0.9654 - prec_1: 0.8826 - recall_1: 0.8872 - prec_2: 0.9326 - recall_2: 0.9242 - prec_3: 0.6831 - recall_3: 0.6537 - prec_4: 0.6580 - recall_4: 0.6995 - prec_5: 0.9254 - recall_5: 0.9506 - f1_m: 0.8410 - recall_m: 0.8293 - precision_m: 0.8533 - precision: 0.8533 - recall_6: 0.8293 - f1score: 0.8410 - val_loss: 5.1437 - val_acc: 0.5217 - val_prec: 0.1759 - val_recall: 0.1698 - val_prec_1: 0.1747 - val_recall_1: 0.1126 - val_prec_2: 0.1875 - val_recall_2: 0.1596 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1174 - val_f1_m: 0.5219 - val_recall_m: 0.5157 - val_precision_m: 0.5285 - val_precision: 0.5285 - val_recall_6: 0.5157 - val_f1score: 0.5219\n",
      "Epoch 98/300\n",
      "epoch:  97\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.3487 - acc: 0.8426 - prec: 0.9503 - recall: 0.9660 - prec_1: 0.8895 - recall_1: 0.8999 - prec_2: 0.9401 - recall_2: 0.9292 - prec_3: 0.6738 - recall_3: 0.6624 - prec_4: 0.6615 - recall_4: 0.6751 - prec_5: 0.9302 - recall_5: 0.9467 - f1_m: 0.8420 - recall_m: 0.8305 - precision_m: 0.8540 - precision: 0.8540 - recall_6: 0.8305 - f1score: 0.8420 - val_loss: 1.4217 - val_acc: 0.6377 - val_prec: 0.1759 - val_recall: 0.1653 - val_prec_1: 0.1741 - val_recall_1: 0.1136 - val_prec_2: 0.1803 - val_recall_2: 0.1450 - val_prec_3: 0.0160 - val_recall_3: 0.0025 - val_prec_4: 0.1759 - val_recall_4: 0.1024 - val_prec_5: 0.1684 - val_recall_5: 0.1520 - val_f1_m: 0.6293 - val_recall_m: 0.6149 - val_precision_m: 0.6455 - val_precision: 0.6455 - val_recall_6: 0.6149 - val_f1score: 0.6293\n",
      "Epoch 99/300\n",
      "epoch:  98\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.3470 - acc: 0.8431 - prec: 0.9474 - recall: 0.9674 - prec_1: 0.8927 - recall_1: 0.9023 - prec_2: 0.9372 - recall_2: 0.9267 - prec_3: 0.6677 - recall_3: 0.6584 - prec_4: 0.6596 - recall_4: 0.6793 - prec_5: 0.9442 - recall_5: 0.9484 - f1_m: 0.8411 - recall_m: 0.8308 - precision_m: 0.8517 - precision: 0.8517 - recall_6: 0.8308 - f1score: 0.8411 - val_loss: 1.3024 - val_acc: 0.6252 - val_prec: 0.1759 - val_recall: 0.1686 - val_prec_1: 0.1741 - val_recall_1: 0.1386 - val_prec_2: 0.1866 - val_recall_2: 0.1141 - val_prec_3: 0.1711 - val_recall_3: 0.1232 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1180 - val_f1_m: 0.6233 - val_recall_m: 0.6164 - val_precision_m: 0.6311 - val_precision: 0.6311 - val_recall_6: 0.6164 - val_f1score: 0.6233\n",
      "Epoch 100/300\n",
      "epoch:  99\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3412 - acc: 0.8447 - prec: 0.9576 - recall: 0.9652 - prec_1: 0.8954 - recall_1: 0.9158 - prec_2: 0.9444 - recall_2: 0.9288 - prec_3: 0.6667 - recall_3: 0.6632 - prec_4: 0.6628 - recall_4: 0.6694 - prec_5: 0.9446 - recall_5: 0.9516 - f1_m: 0.8445 - recall_m: 0.8347 - precision_m: 0.8547 - precision: 0.8547 - recall_6: 0.8347 - f1score: 0.8445 - val_loss: 1.2442 - val_acc: 0.6769 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1733 - val_recall_1: 0.1475 - val_prec_2: 0.1839 - val_recall_2: 0.1513 - val_prec_3: 0.1711 - val_recall_3: 0.0977 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1470 - val_f1_m: 0.6741 - val_recall_m: 0.6617 - val_precision_m: 0.6879 - val_precision: 0.6879 - val_recall_6: 0.6617 - val_f1score: 0.6741\n",
      "Epoch 101/300\n",
      "epoch:  100\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3406 - acc: 0.8438 - prec: 0.9519 - recall: 0.9674 - prec_1: 0.8809 - recall_1: 0.8968 - prec_2: 0.9383 - recall_2: 0.9258 - prec_3: 0.6859 - recall_3: 0.6522 - prec_4: 0.6639 - recall_4: 0.6930 - prec_5: 0.9411 - recall_5: 0.9534 - f1_m: 0.8418 - recall_m: 0.8318 - precision_m: 0.8522 - precision: 0.8522 - recall_6: 0.8318 - f1score: 0.8418 - val_loss: 2.9453 - val_acc: 0.4300 - val_prec: 0.1709 - val_recall: 0.1723 - val_prec_1: 0.1679 - val_recall_1: 0.0748 - val_prec_2: 0.1903 - val_recall_2: 0.1684 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0485 - val_f1_m: 0.4266 - val_recall_m: 0.4183 - val_precision_m: 0.4364 - val_precision: 0.4364 - val_recall_6: 0.4183 - val_f1score: 0.4266\n",
      "Epoch 102/300\n",
      "epoch:  101\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.3447 - acc: 0.8412 - prec: 0.9482 - recall: 0.9678 - prec_1: 0.8953 - recall_1: 0.9003 - prec_2: 0.9400 - recall_2: 0.9289 - prec_3: 0.6719 - recall_3: 0.6526 - prec_4: 0.6524 - recall_4: 0.6705 - prec_5: 0.9388 - recall_5: 0.9521 - f1_m: 0.8403 - recall_m: 0.8287 - precision_m: 0.8524 - precision: 0.8524 - recall_6: 0.8287 - f1score: 0.8403 - val_loss: 2.8988 - val_acc: 0.5165 - val_prec: 0.1665 - val_recall: 0.1738 - val_prec_1: 0.1599 - val_recall_1: 0.0764 - val_prec_2: 0.1839 - val_recall_2: 0.1087 - val_prec_3: 0.1279 - val_recall_3: 0.0637 - val_prec_4: 0.1439 - val_recall_4: 0.0530 - val_prec_5: 0.1524 - val_recall_5: 0.0665 - val_f1_m: 0.5118 - val_recall_m: 0.5075 - val_precision_m: 0.5165 - val_precision: 0.5165 - val_recall_6: 0.5075 - val_f1score: 0.5118\n",
      "Epoch 103/300\n",
      "epoch:  102\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3430 - acc: 0.8402 - prec: 0.9585 - recall: 0.9681 - prec_1: 0.8916 - recall_1: 0.8997 - prec_2: 0.9397 - recall_2: 0.9307 - prec_3: 0.6623 - recall_3: 0.6435 - prec_4: 0.6504 - recall_4: 0.6729 - prec_5: 0.9453 - recall_5: 0.9562 - f1_m: 0.8382 - recall_m: 0.8286 - precision_m: 0.8482 - precision: 0.8482 - recall_6: 0.8286 - f1score: 0.8382 - val_loss: 3.6060 - val_acc: 0.4478 - val_prec: 0.1668 - val_recall: 0.1759 - val_prec_1: 0.1755 - val_recall_1: 0.0785 - val_prec_2: 0.1839 - val_recall_2: 0.1218 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0954 - val_f1_m: 0.4415 - val_recall_m: 0.4288 - val_precision_m: 0.4566 - val_precision: 0.4566 - val_recall_6: 0.4288 - val_f1score: 0.4415\n",
      "Epoch 104/300\n",
      "epoch:  103\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 679us/step - loss: 0.3394 - acc: 0.8441 - prec: 0.9457 - recall: 0.9659 - prec_1: 0.8908 - recall_1: 0.9057 - prec_2: 0.9471 - recall_2: 0.9303 - prec_3: 0.6756 - recall_3: 0.6642 - prec_4: 0.6661 - recall_4: 0.6725 - prec_5: 0.9352 - recall_5: 0.9488 - f1_m: 0.8436 - recall_m: 0.8328 - precision_m: 0.8548 - precision: 0.8548 - recall_6: 0.8328 - f1score: 0.8436 - val_loss: 0.9655 - val_acc: 0.6844 - val_prec: 0.1753 - val_recall: 0.1704 - val_prec_1: 0.1759 - val_recall_1: 0.1005 - val_prec_2: 0.1839 - val_recall_2: 0.0808 - val_prec_3: 0.1691 - val_recall_3: 0.1138 - val_prec_4: 0.1759 - val_recall_4: 0.1074 - val_prec_5: 0.1684 - val_recall_5: 0.1513 - val_f1_m: 0.6801 - val_recall_m: 0.6679 - val_precision_m: 0.6942 - val_precision: 0.6942 - val_recall_6: 0.6679 - val_f1score: 0.6801\n",
      "Epoch 105/300\n",
      "epoch:  104\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.3391 - acc: 0.8436 - prec: 0.9586 - recall: 0.9693 - prec_1: 0.9067 - recall_1: 0.9072 - prec_2: 0.9380 - recall_2: 0.9400 - prec_3: 0.6686 - recall_3: 0.6435 - prec_4: 0.6511 - recall_4: 0.6710 - prec_5: 0.9429 - recall_5: 0.9574 - f1_m: 0.8425 - recall_m: 0.8323 - precision_m: 0.8531 - precision: 0.8531 - recall_6: 0.8323 - f1score: 0.8425 - val_loss: 0.6885 - val_acc: 0.7206 - val_prec: 0.1759 - val_recall: 0.1641 - val_prec_1: 0.1748 - val_recall_1: 0.0838 - val_prec_2: 0.1821 - val_recall_2: 0.1894 - val_prec_3: 0.1439 - val_recall_3: 0.0985 - val_prec_4: 0.1759 - val_recall_4: 0.1204 - val_prec_5: 0.1684 - val_recall_5: 0.1223 - val_f1_m: 0.7092 - val_recall_m: 0.6922 - val_precision_m: 0.7293 - val_precision: 0.7293 - val_recall_6: 0.6922 - val_f1score: 0.7092\n",
      "Epoch 106/300\n",
      "epoch:  105\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.3356 - acc: 0.8473 - prec: 0.9540 - recall: 0.9705 - prec_1: 0.8944 - recall_1: 0.9051 - prec_2: 0.9469 - recall_2: 0.9332 - prec_3: 0.6735 - recall_3: 0.6746 - prec_4: 0.6752 - recall_4: 0.6740 - prec_5: 0.9470 - recall_5: 0.9534 - f1_m: 0.8471 - recall_m: 0.8370 - precision_m: 0.8577 - precision: 0.8577 - recall_6: 0.8370 - f1score: 0.8471 - val_loss: 1.4911 - val_acc: 0.5760 - val_prec: 0.1733 - val_recall: 0.1728 - val_prec_1: 0.1741 - val_recall_1: 0.1273 - val_prec_2: 0.1919 - val_recall_2: 0.1148 - val_prec_3: 0.1119 - val_recall_3: 0.0402 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1485 - val_f1_m: 0.5685 - val_recall_m: 0.5525 - val_precision_m: 0.5896 - val_precision: 0.5896 - val_recall_6: 0.5525 - val_f1score: 0.5685\n",
      "Epoch 107/300\n",
      "epoch:  106\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.3446 - acc: 0.8405 - prec: 0.9638 - recall: 0.9684 - prec_1: 0.8827 - recall_1: 0.9119 - prec_2: 0.9360 - recall_2: 0.9224 - prec_3: 0.6635 - recall_3: 0.6608 - prec_4: 0.6587 - recall_4: 0.6579 - prec_5: 0.9441 - recall_5: 0.9513 - f1_m: 0.8381 - recall_m: 0.8273 - precision_m: 0.8494 - precision: 0.8494 - recall_6: 0.8273 - f1score: 0.8381 - val_loss: 4.1596 - val_acc: 0.3158 - val_prec: 0.1759 - val_recall: 0.0864 - val_prec_1: 0.1599 - val_recall_1: 0.0407 - val_prec_2: 0.1818 - val_recall_2: 0.1914 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0320 - val_recall_4: 0.0025 - val_prec_5: 0.1364 - val_recall_5: 0.0300 - val_f1_m: 0.3112 - val_recall_m: 0.3103 - val_precision_m: 0.3121 - val_precision: 0.3121 - val_recall_6: 0.3103 - val_f1score: 0.3112\n",
      "Epoch 108/300\n",
      "epoch:  107\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3390 - acc: 0.8468 - prec: 0.9530 - recall: 0.9693 - prec_1: 0.9000 - recall_1: 0.9082 - prec_2: 0.9443 - recall_2: 0.9342 - prec_3: 0.6843 - recall_3: 0.6639 - prec_4: 0.6746 - recall_4: 0.6889 - prec_5: 0.9390 - recall_5: 0.9491 - f1_m: 0.8467 - recall_m: 0.8367 - precision_m: 0.8571 - precision: 0.8571 - recall_6: 0.8367 - f1score: 0.8467 - val_loss: 1.8943 - val_acc: 0.4868 - val_prec: 0.1759 - val_recall: 0.1626 - val_prec_1: 0.1439 - val_recall_1: 0.0332 - val_prec_2: 0.1638 - val_recall_2: 0.1066 - val_prec_3: 0.1119 - val_recall_3: 0.0407 - val_prec_4: 0.0160 - val_recall_4: 2.4988e-04 - val_prec_5: 0.1684 - val_recall_5: 0.1663 - val_f1_m: 0.4812 - val_recall_m: 0.4755 - val_precision_m: 0.4872 - val_precision: 0.4872 - val_recall_6: 0.4755 - val_f1score: 0.4812\n",
      "Epoch 109/300\n",
      "epoch:  108\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3418 - acc: 0.8441 - prec: 0.9566 - recall: 0.9672 - prec_1: 0.8909 - recall_1: 0.9103 - prec_2: 0.9428 - recall_2: 0.9348 - prec_3: 0.6766 - recall_3: 0.6676 - prec_4: 0.6666 - recall_4: 0.6742 - prec_5: 0.9371 - recall_5: 0.9438 - f1_m: 0.8421 - recall_m: 0.8313 - precision_m: 0.8533 - precision: 0.8533 - recall_6: 0.8313 - f1score: 0.8421 - val_loss: 2.7934 - val_acc: 0.5890 - val_prec: 0.1759 - val_recall: 0.1708 - val_prec_1: 0.1742 - val_recall_1: 0.1382 - val_prec_2: 0.1802 - val_recall_2: 0.1683 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0160 - val_recall_4: 2.4988e-04 - val_prec_5: 0.1684 - val_recall_5: 0.1495 - val_f1_m: 0.5895 - val_recall_m: 0.5795 - val_precision_m: 0.6003 - val_precision: 0.6003 - val_recall_6: 0.5795 - val_f1score: 0.5895\n",
      "Epoch 110/300\n",
      "epoch:  109\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 676us/step - loss: 0.3340 - acc: 0.8475 - prec: 0.9544 - recall: 0.9694 - prec_1: 0.8955 - recall_1: 0.9033 - prec_2: 0.9476 - recall_2: 0.9372 - prec_3: 0.6785 - recall_3: 0.6844 - prec_4: 0.6758 - recall_4: 0.6662 - prec_5: 0.9378 - recall_5: 0.9524 - f1_m: 0.8464 - recall_m: 0.8373 - precision_m: 0.8558 - precision: 0.8558 - recall_6: 0.8373 - f1score: 0.8464 - val_loss: 3.9120 - val_acc: 0.5662 - val_prec: 0.1738 - val_recall: 0.1686 - val_prec_1: 0.1750 - val_recall_1: 0.0923 - val_prec_2: 0.1873 - val_recall_2: 0.1598 - val_prec_3: 0.0640 - val_recall_3: 0.0285 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1537 - val_f1_m: 0.5581 - val_recall_m: 0.5435 - val_precision_m: 0.5760 - val_precision: 0.5760 - val_recall_6: 0.5435 - val_f1score: 0.5581\n",
      "Epoch 111/300\n",
      "epoch:  110\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3323 - acc: 0.8455 - prec: 0.9503 - recall: 0.9700 - prec_1: 0.8955 - recall_1: 0.9071 - prec_2: 0.9420 - recall_2: 0.9328 - prec_3: 0.6764 - recall_3: 0.6465 - prec_4: 0.6550 - recall_4: 0.6917 - prec_5: 0.9504 - recall_5: 0.9540 - f1_m: 0.8447 - recall_m: 0.8346 - precision_m: 0.8553 - precision: 0.8553 - recall_6: 0.8346 - f1score: 0.8447 - val_loss: 6.3673 - val_acc: 0.3436 - val_prec: 0.1667 - val_recall: 0.1754 - val_prec_1: 0.1439 - val_recall_1: 0.0320 - val_prec_2: 0.1279 - val_recall_2: 0.0380 - val_prec_3: 0.1279 - val_recall_3: 0.0460 - val_prec_4: 0.1439 - val_recall_4: 0.0332 - val_prec_5: 0.1364 - val_recall_5: 0.0442 - val_f1_m: 0.3377 - val_recall_m: 0.3358 - val_precision_m: 0.3398 - val_precision: 0.3398 - val_recall_6: 0.3358 - val_f1score: 0.3377\n",
      "Epoch 112/300\n",
      "epoch:  111\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3316 - acc: 0.8473 - prec: 0.9547 - recall: 0.9719 - prec_1: 0.8951 - recall_1: 0.9128 - prec_2: 0.9411 - recall_2: 0.9295 - prec_3: 0.6780 - recall_3: 0.6690 - prec_4: 0.6732 - recall_4: 0.6761 - prec_5: 0.9446 - recall_5: 0.9538 - f1_m: 0.8460 - recall_m: 0.8351 - precision_m: 0.8573 - precision: 0.8573 - recall_6: 0.8351 - f1score: 0.8460 - val_loss: 1.9293 - val_acc: 0.5927 - val_prec: 0.1759 - val_recall: 0.1225 - val_prec_1: 0.1599 - val_recall_1: 0.0295 - val_prec_2: 0.1809 - val_recall_2: 0.1897 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1809 - val_recall_4: 0.1912 - val_prec_5: 0.1684 - val_recall_5: 0.1206 - val_f1_m: 0.5902 - val_recall_m: 0.5845 - val_precision_m: 0.5964 - val_precision: 0.5964 - val_recall_6: 0.5845 - val_f1score: 0.5902\n",
      "Epoch 113/300\n",
      "epoch:  112\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3334 - acc: 0.8482 - prec: 0.9576 - recall: 0.9673 - prec_1: 0.8981 - recall_1: 0.9176 - prec_2: 0.9474 - recall_2: 0.9325 - prec_3: 0.6849 - recall_3: 0.6528 - prec_4: 0.6645 - recall_4: 0.7024 - prec_5: 0.9488 - recall_5: 0.9530 - f1_m: 0.8478 - recall_m: 0.8378 - precision_m: 0.8581 - precision: 0.8581 - recall_6: 0.8378 - f1score: 0.8478 - val_loss: 3.2080 - val_acc: 0.5247 - val_prec: 0.1667 - val_recall: 0.1738 - val_prec_1: 0.1702 - val_recall_1: 0.0721 - val_prec_2: 0.1819 - val_recall_2: 0.1494 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1439 - val_recall_4: 0.0647 - val_prec_5: 0.1684 - val_recall_5: 0.0902 - val_f1_m: 0.5204 - val_recall_m: 0.5120 - val_precision_m: 0.5295 - val_precision: 0.5295 - val_recall_6: 0.5120 - val_f1score: 0.5204\n",
      "Epoch 114/300\n",
      "epoch:  113\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 676us/step - loss: 0.3296 - acc: 0.8486 - prec: 0.9575 - recall: 0.9705 - prec_1: 0.9010 - recall_1: 0.9223 - prec_2: 0.9507 - recall_2: 0.9420 - prec_3: 0.6843 - recall_3: 0.6613 - prec_4: 0.6633 - recall_4: 0.6848 - prec_5: 0.9487 - recall_5: 0.9543 - f1_m: 0.8460 - recall_m: 0.8367 - precision_m: 0.8556 - precision: 0.8556 - recall_6: 0.8367 - f1score: 0.8460 - val_loss: 3.6253 - val_acc: 0.4473 - val_prec: 0.1719 - val_recall: 0.1752 - val_prec_1: 0.1744 - val_recall_1: 0.1231 - val_prec_2: 0.1919 - val_recall_2: 0.0924 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0860 - val_recall_4: 0.0354 - val_prec_5: 0.1684 - val_recall_5: 0.0574 - val_f1_m: 0.4441 - val_recall_m: 0.4405 - val_precision_m: 0.4479 - val_precision: 0.4479 - val_recall_6: 0.4405 - val_f1score: 0.4441\n",
      "Epoch 115/300\n",
      "epoch:  114\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.3336 - acc: 0.8457 - prec: 0.9487 - recall: 0.9694 - prec_1: 0.8959 - recall_1: 0.9045 - prec_2: 0.9467 - recall_2: 0.9393 - prec_3: 0.6847 - recall_3: 0.6593 - prec_4: 0.6620 - recall_4: 0.6848 - prec_5: 0.9342 - recall_5: 0.9524 - f1_m: 0.8454 - recall_m: 0.8355 - precision_m: 0.8557 - precision: 0.8557 - recall_6: 0.8355 - f1score: 0.8454 - val_loss: 0.9709 - val_acc: 0.6922 - val_prec: 0.1759 - val_recall: 0.1668 - val_prec_1: 0.1754 - val_recall_1: 0.1168 - val_prec_2: 0.1919 - val_recall_2: 0.1258 - val_prec_3: 0.0960 - val_recall_3: 0.0202 - val_prec_4: 0.1809 - val_recall_4: 0.1526 - val_prec_5: 0.1684 - val_recall_5: 0.1643 - val_f1_m: 0.6898 - val_recall_m: 0.6792 - val_precision_m: 0.7040 - val_precision: 0.7040 - val_recall_6: 0.6792 - val_f1score: 0.6898\n",
      "Epoch 116/300\n",
      "epoch:  115\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.3337 - acc: 0.8432 - prec: 0.9542 - recall: 0.9679 - prec_1: 0.8967 - recall_1: 0.9091 - prec_2: 0.9440 - recall_2: 0.9301 - prec_3: 0.6684 - recall_3: 0.6602 - prec_4: 0.6579 - recall_4: 0.6690 - prec_5: 0.9405 - recall_5: 0.9559 - f1_m: 0.8410 - recall_m: 0.8298 - precision_m: 0.8526 - precision: 0.8526 - recall_6: 0.8298 - f1score: 0.8410 - val_loss: 2.5754 - val_acc: 0.5737 - val_prec: 0.1759 - val_recall: 0.1244 - val_prec_1: 0.1755 - val_recall_1: 0.1267 - val_prec_2: 0.1855 - val_recall_2: 0.1571 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1279 - val_recall_4: 0.0427 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.5613 - val_recall_m: 0.5510 - val_precision_m: 0.5732 - val_precision: 0.5732 - val_recall_6: 0.5510 - val_f1score: 0.5613\n",
      "Epoch 117/300\n",
      "epoch:  116\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.3301 - acc: 0.8497 - prec: 0.9651 - recall: 0.9716 - prec_1: 0.9003 - recall_1: 0.9280 - prec_2: 0.9503 - recall_2: 0.9352 - prec_3: 0.6675 - recall_3: 0.6615 - prec_4: 0.6712 - recall_4: 0.6819 - prec_5: 0.9501 - recall_5: 0.9527 - f1_m: 0.8494 - recall_m: 0.8406 - precision_m: 0.8587 - precision: 0.8587 - recall_6: 0.8406 - f1score: 0.8494 - val_loss: 1.4018 - val_acc: 0.6594 - val_prec: 0.1759 - val_recall: 0.1709 - val_prec_1: 0.1748 - val_recall_1: 0.1532 - val_prec_2: 0.1866 - val_recall_2: 0.1104 - val_prec_3: 0.1279 - val_recall_3: 0.0565 - val_prec_4: 0.1439 - val_recall_4: 0.0550 - val_prec_5: 0.1684 - val_recall_5: 0.1475 - val_f1_m: 0.6534 - val_recall_m: 0.6464 - val_precision_m: 0.6609 - val_precision: 0.6609 - val_recall_6: 0.6464 - val_f1score: 0.6534\n",
      "Epoch 118/300\n",
      "epoch:  117\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3298 - acc: 0.8457 - prec: 0.9527 - recall: 0.9685 - prec_1: 0.8982 - recall_1: 0.9149 - prec_2: 0.9546 - recall_2: 0.9356 - prec_3: 0.6687 - recall_3: 0.6644 - prec_4: 0.6555 - recall_4: 0.6598 - prec_5: 0.9451 - recall_5: 0.9560 - f1_m: 0.8436 - recall_m: 0.8327 - precision_m: 0.8550 - precision: 0.8550 - recall_6: 0.8327 - f1score: 0.8436 - val_loss: 4.4968 - val_acc: 0.4320 - val_prec: 0.1689 - val_recall: 0.1727 - val_prec_1: 0.1755 - val_recall_1: 0.0735 - val_prec_2: 0.1866 - val_recall_2: 0.1495 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0669 - val_f1_m: 0.4290 - val_recall_m: 0.4198 - val_precision_m: 0.4394 - val_precision: 0.4394 - val_recall_6: 0.4198 - val_f1score: 0.4290\n",
      "Epoch 119/300\n",
      "epoch:  118\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.3322 - acc: 0.8478 - prec: 0.9533 - recall: 0.9727 - prec_1: 0.9035 - recall_1: 0.9156 - prec_2: 0.9487 - recall_2: 0.9381 - prec_3: 0.6740 - recall_3: 0.6757 - prec_4: 0.6627 - recall_4: 0.6628 - prec_5: 0.9457 - recall_5: 0.9569 - f1_m: 0.8455 - recall_m: 0.8353 - precision_m: 0.8562 - precision: 0.8562 - recall_6: 0.8353 - f1score: 0.8455 - val_loss: 2.9741 - val_acc: 0.4790 - val_prec: 0.1759 - val_recall: 0.1024 - val_prec_1: 0.1279 - val_recall_1: 0.0233 - val_prec_2: 0.1799 - val_recall_2: 0.0970 - val_prec_3: 0.1711 - val_recall_3: 0.0734 - val_prec_4: 0.1439 - val_recall_4: 0.0532 - val_prec_5: 0.1684 - val_recall_5: 0.1684 - val_f1_m: 0.4755 - val_recall_m: 0.4718 - val_precision_m: 0.4796 - val_precision: 0.4796 - val_recall_6: 0.4718 - val_f1score: 0.4755\n",
      "Epoch 120/300\n",
      "epoch:  119\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3290 - acc: 0.8511 - prec: 0.9599 - recall: 0.9692 - prec_1: 0.9031 - recall_1: 0.9176 - prec_2: 0.9491 - recall_2: 0.9409 - prec_3: 0.6818 - recall_3: 0.6857 - prec_4: 0.6808 - recall_4: 0.6751 - prec_5: 0.9474 - recall_5: 0.9526 - f1_m: 0.8506 - recall_m: 0.8416 - precision_m: 0.8600 - precision: 0.8600 - recall_6: 0.8416 - f1score: 0.8506 - val_loss: 1.1242 - val_acc: 0.6924 - val_prec: 0.1759 - val_recall: 0.1403 - val_prec_1: 0.1742 - val_recall_1: 0.1040 - val_prec_2: 0.1919 - val_recall_2: 0.1138 - val_prec_3: 0.1439 - val_recall_3: 0.0927 - val_prec_4: 0.1759 - val_recall_4: 0.1274 - val_prec_5: 0.1684 - val_recall_5: 0.1642 - val_f1_m: 0.6895 - val_recall_m: 0.6837 - val_precision_m: 0.6957 - val_precision: 0.6957 - val_recall_6: 0.6837 - val_f1score: 0.6895\n",
      "Epoch 121/300\n",
      "epoch:  120\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 675us/step - loss: 0.3239 - acc: 0.8506 - prec: 0.9535 - recall: 0.9710 - prec_1: 0.9036 - recall_1: 0.9234 - prec_2: 0.9553 - recall_2: 0.9417 - prec_3: 0.6821 - recall_3: 0.6457 - prec_4: 0.6569 - recall_4: 0.6984 - prec_5: 0.9537 - recall_5: 0.9525 - f1_m: 0.8486 - recall_m: 0.8407 - precision_m: 0.8567 - precision: 0.8567 - recall_6: 0.8407 - f1score: 0.8486 - val_loss: 5.1653 - val_acc: 0.4903 - val_prec: 0.1759 - val_recall: 0.1054 - val_prec_1: 0.1741 - val_recall_1: 0.0920 - val_prec_2: 0.1812 - val_recall_2: 0.1614 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1637 - val_f1_m: 0.4845 - val_recall_m: 0.4760 - val_precision_m: 0.4939 - val_precision: 0.4939 - val_recall_6: 0.4760 - val_f1score: 0.4845\n",
      "Epoch 122/300\n",
      "epoch:  121\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3267 - acc: 0.8498 - prec: 0.9592 - recall: 0.9670 - prec_1: 0.9070 - recall_1: 0.9301 - prec_2: 0.9513 - recall_2: 0.9343 - prec_3: 0.6795 - recall_3: 0.6656 - prec_4: 0.6635 - recall_4: 0.6762 - prec_5: 0.9478 - recall_5: 0.9550 - f1_m: 0.8488 - recall_m: 0.8387 - precision_m: 0.8594 - precision: 0.8594 - recall_6: 0.8387 - f1score: 0.8488 - val_loss: 0.6278 - val_acc: 0.7276 - val_prec: 0.1759 - val_recall: 0.1691 - val_prec_1: 0.1742 - val_recall_1: 0.1120 - val_prec_2: 0.1839 - val_recall_2: 0.1707 - val_prec_3: 0.1709 - val_recall_3: 0.1359 - val_prec_4: 0.1439 - val_recall_4: 0.0222 - val_prec_5: 0.1684 - val_recall_5: 0.1600 - val_f1_m: 0.7163 - val_recall_m: 0.7026 - val_precision_m: 0.7315 - val_precision: 0.7315 - val_recall_6: 0.7026 - val_f1score: 0.7163\n",
      "Epoch 123/300\n",
      "epoch:  122\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3290 - acc: 0.8482 - prec: 0.9504 - recall: 0.9673 - prec_1: 0.9022 - recall_1: 0.9060 - prec_2: 0.9482 - recall_2: 0.9410 - prec_3: 0.6741 - recall_3: 0.6654 - prec_4: 0.6699 - recall_4: 0.6810 - prec_5: 0.9432 - recall_5: 0.9523 - f1_m: 0.8474 - recall_m: 0.8378 - precision_m: 0.8574 - precision: 0.8574 - recall_6: 0.8378 - f1score: 0.8474 - val_loss: 0.6995 - val_acc: 0.7246 - val_prec: 0.1759 - val_recall: 0.1684 - val_prec_1: 0.1753 - val_recall_1: 0.1224 - val_prec_2: 0.1861 - val_recall_2: 0.1784 - val_prec_3: 0.1279 - val_recall_3: 0.0525 - val_prec_4: 0.1759 - val_recall_4: 0.1067 - val_prec_5: 0.1684 - val_recall_5: 0.1535 - val_f1_m: 0.7184 - val_recall_m: 0.7036 - val_precision_m: 0.7347 - val_precision: 0.7347 - val_recall_6: 0.7036 - val_f1score: 0.7184\n",
      "Epoch 124/300\n",
      "epoch:  123\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3187 - acc: 0.8540 - prec: 0.9547 - recall: 0.9686 - prec_1: 0.9010 - recall_1: 0.9285 - prec_2: 0.9543 - recall_2: 0.9333 - prec_3: 0.6869 - recall_3: 0.6842 - prec_4: 0.6808 - recall_4: 0.6894 - prec_5: 0.9530 - recall_5: 0.9584 - f1_m: 0.8547 - recall_m: 0.8453 - precision_m: 0.8644 - precision: 0.8644 - recall_6: 0.8453 - f1score: 0.8547 - val_loss: 0.6571 - val_acc: 0.7579 - val_prec: 0.1759 - val_recall: 0.1448 - val_prec_1: 0.1741 - val_recall_1: 0.1406 - val_prec_2: 0.1839 - val_recall_2: 0.1749 - val_prec_3: 0.1653 - val_recall_3: 0.1046 - val_prec_4: 0.1759 - val_recall_4: 0.1169 - val_prec_5: 0.1684 - val_recall_5: 0.1337 - val_f1_m: 0.7512 - val_recall_m: 0.7349 - val_precision_m: 0.7694 - val_precision: 0.7694 - val_recall_6: 0.7349 - val_f1score: 0.7512\n",
      "Epoch 125/300\n",
      "epoch:  124\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3214 - acc: 0.8517 - prec: 0.9546 - recall: 0.9701 - prec_1: 0.9061 - recall_1: 0.9086 - prec_2: 0.9438 - recall_2: 0.9436 - prec_3: 0.6855 - recall_3: 0.6618 - prec_4: 0.6700 - recall_4: 0.6988 - prec_5: 0.9450 - recall_5: 0.9542 - f1_m: 0.8509 - recall_m: 0.8425 - precision_m: 0.8597 - precision: 0.8597 - recall_6: 0.8425 - f1score: 0.8509 - val_loss: 6.6074 - val_acc: 0.3356 - val_prec: 0.1667 - val_recall: 0.1752 - val_prec_1: 0.1439 - val_recall_1: 0.0484 - val_prec_2: 0.1460 - val_recall_2: 0.0723 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0557 - val_f1_m: 0.3338 - val_recall_m: 0.3308 - val_precision_m: 0.3370 - val_precision: 0.3370 - val_recall_6: 0.3308 - val_f1score: 0.3338\n",
      "Epoch 126/300\n",
      "epoch:  125\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3239 - acc: 0.8522 - prec: 0.9539 - recall: 0.9715 - prec_1: 0.9104 - recall_1: 0.9252 - prec_2: 0.9561 - recall_2: 0.9370 - prec_3: 0.6826 - recall_3: 0.6671 - prec_4: 0.6609 - recall_4: 0.6795 - prec_5: 0.9518 - recall_5: 0.9560 - f1_m: 0.8511 - recall_m: 0.8426 - precision_m: 0.8599 - precision: 0.8599 - recall_6: 0.8426 - f1score: 0.8511 - val_loss: 0.9788 - val_acc: 0.6609 - val_prec: 0.1759 - val_recall: 0.1624 - val_prec_1: 0.1753 - val_recall_1: 0.1370 - val_prec_2: 0.1866 - val_recall_2: 0.1313 - val_prec_3: 0.1759 - val_recall_3: 0.0924 - val_prec_4: 0.0800 - val_recall_4: 0.0102 - val_prec_5: 0.1684 - val_recall_5: 0.1638 - val_f1_m: 0.6435 - val_recall_m: 0.6319 - val_precision_m: 0.6596 - val_precision: 0.6596 - val_recall_6: 0.6319 - val_f1score: 0.6435\n",
      "Epoch 127/300\n",
      "epoch:  126\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.3188 - acc: 0.8580 - prec: 0.9586 - recall: 0.9729 - prec_1: 0.9196 - recall_1: 0.9165 - prec_2: 0.9500 - recall_2: 0.9464 - prec_3: 0.6969 - recall_3: 0.6938 - prec_4: 0.6802 - recall_4: 0.6870 - prec_5: 0.9499 - recall_5: 0.9630 - f1_m: 0.8557 - recall_m: 0.8460 - precision_m: 0.8658 - precision: 0.8658 - recall_6: 0.8460 - f1score: 0.8557 - val_loss: 0.5665 - val_acc: 0.7966 - val_prec: 0.1759 - val_recall: 0.1663 - val_prec_1: 0.1748 - val_recall_1: 0.1325 - val_prec_2: 0.1852 - val_recall_2: 0.1771 - val_prec_3: 0.1703 - val_recall_3: 0.1224 - val_prec_4: 0.1759 - val_recall_4: 0.1027 - val_prec_5: 0.1684 - val_recall_5: 0.1581 - val_f1_m: 0.7797 - val_recall_m: 0.7601 - val_precision_m: 0.8043 - val_precision: 0.8043 - val_recall_6: 0.7601 - val_f1score: 0.7797\n",
      "Epoch 128/300\n",
      "epoch:  127\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3215 - acc: 0.8512 - prec: 0.9590 - recall: 0.9719 - prec_1: 0.9035 - recall_1: 0.9268 - prec_2: 0.9624 - recall_2: 0.9449 - prec_3: 0.6828 - recall_3: 0.6547 - prec_4: 0.6568 - recall_4: 0.6857 - prec_5: 0.9514 - recall_5: 0.9536 - f1_m: 0.8501 - recall_m: 0.8413 - precision_m: 0.8591 - precision: 0.8591 - recall_6: 0.8413 - f1score: 0.8501 - val_loss: 0.6254 - val_acc: 0.7416 - val_prec: 0.1759 - val_recall: 0.1378 - val_prec_1: 0.1737 - val_recall_1: 0.1598 - val_prec_2: 0.1879 - val_recall_2: 0.1583 - val_prec_3: 0.1709 - val_recall_3: 0.1719 - val_prec_4: 0.0960 - val_recall_4: 0.0082 - val_prec_5: 0.1684 - val_recall_5: 0.1474 - val_f1_m: 0.7411 - val_recall_m: 0.7306 - val_precision_m: 0.7529 - val_precision: 0.7529 - val_recall_6: 0.7306 - val_f1score: 0.7411\n",
      "Epoch 129/300\n",
      "epoch:  128\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3236 - acc: 0.8472 - prec: 0.9556 - recall: 0.9704 - prec_1: 0.9080 - recall_1: 0.9222 - prec_2: 0.9538 - recall_2: 0.9398 - prec_3: 0.6704 - recall_3: 0.6660 - prec_4: 0.6622 - recall_4: 0.6644 - prec_5: 0.9451 - recall_5: 0.9594 - f1_m: 0.8465 - recall_m: 0.8381 - precision_m: 0.8551 - precision: 0.8551 - recall_6: 0.8381 - f1score: 0.8465 - val_loss: 8.1131 - val_acc: 0.2784 - val_prec: 0.1439 - val_recall: 0.0266 - val_prec_1: 0.1279 - val_recall_1: 0.0259 - val_prec_2: 0.1690 - val_recall_2: 0.1912 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1524 - val_recall_5: 0.0635 - val_f1_m: 0.2765 - val_recall_m: 0.2756 - val_precision_m: 0.2774 - val_precision: 0.2774 - val_recall_6: 0.2756 - val_f1score: 0.2765\n",
      "Epoch 130/300\n",
      "epoch:  129\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3145 - acc: 0.8560 - prec: 0.9653 - recall: 0.9701 - prec_1: 0.9171 - recall_1: 0.9335 - prec_2: 0.9547 - recall_2: 0.9447 - prec_3: 0.6821 - recall_3: 0.6654 - prec_4: 0.6664 - recall_4: 0.6860 - prec_5: 0.9607 - recall_5: 0.9622 - f1_m: 0.8550 - recall_m: 0.8461 - precision_m: 0.8643 - precision: 0.8643 - recall_6: 0.8461 - f1score: 0.8550 - val_loss: 2.0885 - val_acc: 0.6349 - val_prec: 0.1748 - val_recall: 0.1706 - val_prec_1: 0.1737 - val_recall_1: 0.1208 - val_prec_2: 0.1866 - val_recall_2: 0.1238 - val_prec_3: 0.1439 - val_recall_3: 0.0735 - val_prec_4: 0.1599 - val_recall_4: 0.0752 - val_prec_5: 0.1684 - val_recall_5: 0.1063 - val_f1_m: 0.6280 - val_recall_m: 0.6152 - val_precision_m: 0.6446 - val_precision: 0.6446 - val_recall_6: 0.6152 - val_f1score: 0.6280\n",
      "Epoch 131/300\n",
      "epoch:  130\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.3191 - acc: 0.8551 - prec: 0.9649 - recall: 0.9708 - prec_1: 0.9066 - recall_1: 0.9236 - prec_2: 0.9508 - recall_2: 0.9446 - prec_3: 0.6705 - recall_3: 0.6738 - prec_4: 0.6866 - recall_4: 0.6789 - prec_5: 0.9537 - recall_5: 0.9600 - f1_m: 0.8530 - recall_m: 0.8443 - precision_m: 0.8619 - precision: 0.8619 - recall_6: 0.8443 - f1score: 0.8530 - val_loss: 0.9553 - val_acc: 0.6679 - val_prec: 0.1759 - val_recall: 0.1488 - val_prec_1: 0.1748 - val_recall_1: 0.0650 - val_prec_2: 0.1820 - val_recall_2: 0.1840 - val_prec_3: 0.1502 - val_recall_3: 0.0865 - val_prec_4: 0.1759 - val_recall_4: 0.0782 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.6624 - val_recall_m: 0.6462 - val_precision_m: 0.6827 - val_precision: 0.6827 - val_recall_6: 0.6462 - val_f1score: 0.6624\n",
      "Epoch 132/300\n",
      "epoch:  131\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.3184 - acc: 0.8588 - prec: 0.9512 - recall: 0.9759 - prec_1: 0.9123 - recall_1: 0.9188 - prec_2: 0.9487 - recall_2: 0.9467 - prec_3: 0.6958 - recall_3: 0.6813 - prec_4: 0.6843 - recall_4: 0.7031 - prec_5: 0.9514 - recall_5: 0.9585 - f1_m: 0.8578 - recall_m: 0.8485 - precision_m: 0.8675 - precision: 0.8675 - recall_6: 0.8485 - f1score: 0.8578 - val_loss: 0.7129 - val_acc: 0.7276 - val_prec: 0.1759 - val_recall: 0.1683 - val_prec_1: 0.1736 - val_recall_1: 0.1647 - val_prec_2: 0.1919 - val_recall_2: 0.1315 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1809 - val_recall_4: 0.1917 - val_prec_5: 0.1684 - val_recall_5: 0.1288 - val_f1_m: 0.7277 - val_recall_m: 0.7234 - val_precision_m: 0.7322 - val_precision: 0.7322 - val_recall_6: 0.7234 - val_f1score: 0.7277\n",
      "Epoch 133/300\n",
      "epoch:  132\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 680us/step - loss: 0.3155 - acc: 0.8533 - prec: 0.9644 - recall: 0.9770 - prec_1: 0.9061 - recall_1: 0.9328 - prec_2: 0.9594 - recall_2: 0.9401 - prec_3: 0.6745 - recall_3: 0.6657 - prec_4: 0.6685 - recall_4: 0.6746 - prec_5: 0.9574 - recall_5: 0.9524 - f1_m: 0.8533 - recall_m: 0.8447 - precision_m: 0.8622 - precision: 0.8622 - recall_6: 0.8447 - f1score: 0.8533 - val_loss: 0.7467 - val_acc: 0.6982 - val_prec: 0.1759 - val_recall: 0.1691 - val_prec_1: 0.1741 - val_recall_1: 0.1611 - val_prec_2: 0.1919 - val_recall_2: 0.1337 - val_prec_3: 0.1709 - val_recall_3: 0.1229 - val_prec_4: 0.1279 - val_recall_4: 0.0547 - val_prec_5: 0.1684 - val_recall_5: 0.1146 - val_f1_m: 0.6959 - val_recall_m: 0.6892 - val_precision_m: 0.7033 - val_precision: 0.7033 - val_recall_6: 0.6892 - val_f1score: 0.6959\n",
      "Epoch 134/300\n",
      "epoch:  133\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.3168 - acc: 0.8542 - prec: 0.9587 - recall: 0.9753 - prec_1: 0.9159 - recall_1: 0.9156 - prec_2: 0.9473 - recall_2: 0.9448 - prec_3: 0.6787 - recall_3: 0.6611 - prec_4: 0.6686 - recall_4: 0.6799 - prec_5: 0.9441 - recall_5: 0.9629 - f1_m: 0.8538 - recall_m: 0.8453 - precision_m: 0.8626 - precision: 0.8626 - recall_6: 0.8453 - f1score: 0.8538 - val_loss: 1.0546 - val_acc: 0.6467 - val_prec: 0.1759 - val_recall: 0.1231 - val_prec_1: 0.1738 - val_recall_1: 0.1460 - val_prec_2: 0.1879 - val_recall_2: 0.1171 - val_prec_3: 0.1279 - val_recall_3: 0.0647 - val_prec_4: 0.1759 - val_recall_4: 0.1144 - val_prec_5: 0.1684 - val_recall_5: 0.1283 - val_f1_m: 0.6190 - val_recall_m: 0.5987 - val_precision_m: 0.6447 - val_precision: 0.6447 - val_recall_6: 0.5987 - val_f1score: 0.6190\n",
      "Epoch 135/300\n",
      "epoch:  134\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3143 - acc: 0.8606 - prec: 0.9559 - recall: 0.9757 - prec_1: 0.9087 - recall_1: 0.9179 - prec_2: 0.9448 - recall_2: 0.9433 - prec_3: 0.6953 - recall_3: 0.6970 - prec_4: 0.7001 - recall_4: 0.6990 - prec_5: 0.9497 - recall_5: 0.9545 - f1_m: 0.8597 - recall_m: 0.8516 - precision_m: 0.8682 - precision: 0.8682 - recall_6: 0.8516 - f1score: 0.8597 - val_loss: 4.2926 - val_acc: 0.5105 - val_prec: 0.1759 - val_recall: 0.1412 - val_prec_1: 0.1736 - val_recall_1: 0.1062 - val_prec_2: 0.1710 - val_recall_2: 0.1857 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.1156 - val_f1_m: 0.5072 - val_recall_m: 0.5015 - val_precision_m: 0.5132 - val_precision: 0.5132 - val_recall_6: 0.5015 - val_f1score: 0.5072\n",
      "Epoch 136/300\n",
      "epoch:  135\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.3125 - acc: 0.8601 - prec: 0.9614 - recall: 0.9724 - prec_1: 0.9116 - recall_1: 0.9281 - prec_2: 0.9584 - recall_2: 0.9465 - prec_3: 0.7000 - recall_3: 0.6883 - prec_4: 0.6922 - recall_4: 0.7091 - prec_5: 0.9457 - recall_5: 0.9556 - f1_m: 0.8592 - recall_m: 0.8505 - precision_m: 0.8682 - precision: 0.8682 - recall_6: 0.8505 - f1score: 0.8592 - val_loss: 1.5667 - val_acc: 0.5975 - val_prec: 0.1743 - val_recall: 0.1693 - val_prec_1: 0.1706 - val_recall_1: 0.0406 - val_prec_2: 0.1816 - val_recall_2: 0.1892 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1649 - val_recall_4: 0.1404 - val_prec_5: 0.1684 - val_recall_5: 0.1052 - val_f1_m: 0.5936 - val_recall_m: 0.5850 - val_precision_m: 0.6047 - val_precision: 0.6047 - val_recall_6: 0.5850 - val_f1score: 0.5936\n",
      "Epoch 137/300\n",
      "epoch:  136\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.3101 - acc: 0.8602 - prec: 0.9675 - recall: 0.9731 - prec_1: 0.9095 - recall_1: 0.9237 - prec_2: 0.9529 - recall_2: 0.9465 - prec_3: 0.7037 - recall_3: 0.6663 - prec_4: 0.6835 - recall_4: 0.7145 - prec_5: 0.9453 - recall_5: 0.9539 - f1_m: 0.8585 - recall_m: 0.8502 - precision_m: 0.8671 - precision: 0.8671 - recall_6: 0.8502 - f1score: 0.8585 - val_loss: 0.8372 - val_acc: 0.7301 - val_prec: 0.1759 - val_recall: 0.1668 - val_prec_1: 0.1743 - val_recall_1: 0.1399 - val_prec_2: 0.1896 - val_recall_2: 0.1763 - val_prec_3: 0.1279 - val_recall_3: 0.0737 - val_prec_4: 0.1759 - val_recall_4: 0.1287 - val_prec_5: 0.1684 - val_recall_5: 0.0995 - val_f1_m: 0.7286 - val_recall_m: 0.7204 - val_precision_m: 0.7375 - val_precision: 0.7375 - val_recall_6: 0.7204 - val_f1score: 0.7286\n",
      "Epoch 138/300\n",
      "epoch:  137\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3130 - acc: 0.8546 - prec: 0.9641 - recall: 0.9712 - prec_1: 0.9178 - recall_1: 0.9194 - prec_2: 0.9546 - recall_2: 0.9513 - prec_3: 0.6887 - recall_3: 0.6752 - prec_4: 0.6751 - recall_4: 0.6872 - prec_5: 0.9484 - recall_5: 0.9628 - f1_m: 0.8534 - recall_m: 0.8450 - precision_m: 0.8622 - precision: 0.8622 - recall_6: 0.8450 - f1score: 0.8534 - val_loss: 1.5579 - val_acc: 0.5547 - val_prec: 0.1729 - val_recall: 0.1716 - val_prec_1: 0.1736 - val_recall_1: 0.1386 - val_prec_2: 0.1919 - val_recall_2: 0.0597 - val_prec_3: 0.1700 - val_recall_3: 0.1223 - val_prec_4: 0.1439 - val_recall_4: 0.0330 - val_prec_5: 0.1638 - val_recall_5: 0.0635 - val_f1_m: 0.5262 - val_recall_m: 0.5160 - val_precision_m: 0.5394 - val_precision: 0.5394 - val_recall_6: 0.5160 - val_f1score: 0.5262\n",
      "Epoch 139/300\n",
      "epoch:  138\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3148 - acc: 0.8586 - prec: 0.9622 - recall: 0.9723 - prec_1: 0.9068 - recall_1: 0.9210 - prec_2: 0.9520 - recall_2: 0.9427 - prec_3: 0.6968 - recall_3: 0.7090 - prec_4: 0.6928 - recall_4: 0.6854 - prec_5: 0.9468 - recall_5: 0.9546 - f1_m: 0.8569 - recall_m: 0.8471 - precision_m: 0.8670 - precision: 0.8670 - recall_6: 0.8471 - f1score: 0.8569 - val_loss: 1.1545 - val_acc: 0.6952 - val_prec: 0.1759 - val_recall: 0.1691 - val_prec_1: 0.1738 - val_recall_1: 0.1486 - val_prec_2: 0.1887 - val_recall_2: 0.1542 - val_prec_3: 0.1279 - val_recall_3: 0.0660 - val_prec_4: 0.1279 - val_recall_4: 0.0545 - val_prec_5: 0.1684 - val_recall_5: 0.1559 - val_f1_m: 0.6939 - val_recall_m: 0.6824 - val_precision_m: 0.7064 - val_precision: 0.7064 - val_recall_6: 0.6824 - val_f1score: 0.6939\n",
      "Epoch 140/300\n",
      "epoch:  139\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.3077 - acc: 0.8622 - prec: 0.9649 - recall: 0.9724 - prec_1: 0.9085 - recall_1: 0.9338 - prec_2: 0.9677 - recall_2: 0.9405 - prec_3: 0.6998 - recall_3: 0.6937 - prec_4: 0.6916 - recall_4: 0.6991 - prec_5: 0.9471 - recall_5: 0.9608 - f1_m: 0.8602 - recall_m: 0.8508 - precision_m: 0.8699 - precision: 0.8699 - recall_6: 0.8508 - f1score: 0.8602 - val_loss: 0.6141 - val_acc: 0.7234 - val_prec: 0.1759 - val_recall: 0.1619 - val_prec_1: 0.1744 - val_recall_1: 0.1339 - val_prec_2: 0.1873 - val_recall_2: 0.1500 - val_prec_3: 0.1709 - val_recall_3: 0.1334 - val_prec_4: 0.1599 - val_recall_4: 0.0827 - val_prec_5: 0.1684 - val_recall_5: 0.1217 - val_f1_m: 0.7219 - val_recall_m: 0.7071 - val_precision_m: 0.7383 - val_precision: 0.7383 - val_recall_6: 0.7071 - val_f1score: 0.7219\n",
      "Epoch 141/300\n",
      "epoch:  140\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3093 - acc: 0.8587 - prec: 0.9582 - recall: 0.9767 - prec_1: 0.9210 - recall_1: 0.9239 - prec_2: 0.9571 - recall_2: 0.9500 - prec_3: 0.6893 - recall_3: 0.6833 - prec_4: 0.6777 - recall_4: 0.6863 - prec_5: 0.9429 - recall_5: 0.9565 - f1_m: 0.8578 - recall_m: 0.8498 - precision_m: 0.8661 - precision: 0.8661 - recall_6: 0.8498 - f1score: 0.8578 - val_loss: 1.2393 - val_acc: 0.6932 - val_prec: 0.1759 - val_recall: 0.1643 - val_prec_1: 0.1733 - val_recall_1: 0.1570 - val_prec_2: 0.1866 - val_recall_2: 0.1287 - val_prec_3: 0.1279 - val_recall_3: 0.0682 - val_prec_4: 0.1759 - val_recall_4: 0.1284 - val_prec_5: 0.1684 - val_recall_5: 0.0962 - val_f1_m: 0.6922 - val_recall_m: 0.6869 - val_precision_m: 0.6978 - val_precision: 0.6978 - val_recall_6: 0.6869 - val_f1score: 0.6922\n",
      "Epoch 142/300\n",
      "epoch:  141\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3101 - acc: 0.8573 - prec: 0.9636 - recall: 0.9739 - prec_1: 0.9133 - recall_1: 0.9145 - prec_2: 0.9573 - recall_2: 0.9473 - prec_3: 0.6957 - recall_3: 0.6903 - prec_4: 0.6736 - recall_4: 0.6874 - prec_5: 0.9421 - recall_5: 0.9564 - f1_m: 0.8561 - recall_m: 0.8480 - precision_m: 0.8645 - precision: 0.8645 - recall_6: 0.8480 - f1score: 0.8561 - val_loss: 1.0731 - val_acc: 0.6779 - val_prec: 0.1753 - val_recall: 0.1670 - val_prec_1: 0.1729 - val_recall_1: 0.1578 - val_prec_2: 0.1866 - val_recall_2: 0.1131 - val_prec_3: 0.1700 - val_recall_3: 0.1091 - val_prec_4: 0.1759 - val_recall_4: 0.0995 - val_prec_5: 0.1684 - val_recall_5: 0.0840 - val_f1_m: 0.6494 - val_recall_m: 0.6304 - val_precision_m: 0.6888 - val_precision: 0.6888 - val_recall_6: 0.6304 - val_f1score: 0.6494\n",
      "Epoch 143/300\n",
      "epoch:  142\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3048 - acc: 0.8632 - prec: 0.9701 - recall: 0.9749 - prec_1: 0.9243 - recall_1: 0.9398 - prec_2: 0.9645 - recall_2: 0.9505 - prec_3: 0.6904 - recall_3: 0.6770 - prec_4: 0.6776 - recall_4: 0.6983 - prec_5: 0.9602 - recall_5: 0.9649 - f1_m: 0.8623 - recall_m: 0.8535 - precision_m: 0.8714 - precision: 0.8714 - recall_6: 0.8535 - f1score: 0.8623 - val_loss: 0.8099 - val_acc: 0.7041 - val_prec: 0.1759 - val_recall: 0.1522 - val_prec_1: 0.1734 - val_recall_1: 0.1522 - val_prec_2: 0.1919 - val_recall_2: 0.1135 - val_prec_3: 0.1709 - val_recall_3: 0.1727 - val_prec_4: 0.0800 - val_recall_4: 0.0035 - val_prec_5: 0.1684 - val_recall_5: 0.1488 - val_f1_m: 0.6948 - val_recall_m: 0.6864 - val_precision_m: 0.7072 - val_precision: 0.7072 - val_recall_6: 0.6864 - val_f1score: 0.6948\n",
      "Epoch 144/300\n",
      "epoch:  143\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.3086 - acc: 0.8570 - prec: 0.9604 - recall: 0.9694 - prec_1: 0.9127 - recall_1: 0.9355 - prec_2: 0.9584 - recall_2: 0.9439 - prec_3: 0.6789 - recall_3: 0.6895 - prec_4: 0.6845 - recall_4: 0.6730 - prec_5: 0.9593 - recall_5: 0.9541 - f1_m: 0.8557 - recall_m: 0.8467 - precision_m: 0.8651 - precision: 0.8651 - recall_6: 0.8467 - f1score: 0.8557 - val_loss: 2.1389 - val_acc: 0.5217 - val_prec: 0.1759 - val_recall: 0.1728 - val_prec_1: 0.1741 - val_recall_1: 0.1129 - val_prec_2: 0.1919 - val_recall_2: 0.1799 - val_prec_3: 0.0800 - val_recall_3: 0.0145 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1684 - val_recall_5: 0.0778 - val_f1_m: 0.5166 - val_recall_m: 0.5057 - val_precision_m: 0.5295 - val_precision: 0.5295 - val_recall_6: 0.5057 - val_f1score: 0.5166\n",
      "Epoch 145/300\n",
      "epoch:  144\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.3074 - acc: 0.8611 - prec: 0.9671 - recall: 0.9713 - prec_1: 0.9142 - recall_1: 0.9304 - prec_2: 0.9489 - recall_2: 0.9423 - prec_3: 0.6843 - recall_3: 0.6953 - prec_4: 0.6885 - recall_4: 0.6885 - prec_5: 0.9585 - recall_5: 0.9570 - f1_m: 0.8600 - recall_m: 0.8508 - precision_m: 0.8695 - precision: 0.8695 - recall_6: 0.8508 - f1score: 0.8600 - val_loss: 1.1002 - val_acc: 0.6049 - val_prec: 0.1737 - val_recall: 0.1529 - val_prec_1: 0.1743 - val_recall_1: 0.0473 - val_prec_2: 0.1770 - val_recall_2: 0.1667 - val_prec_3: 0.1759 - val_recall_3: 0.0762 - val_prec_4: 0.1279 - val_recall_4: 0.0542 - val_prec_5: 0.1684 - val_recall_5: 0.1523 - val_f1_m: 0.5964 - val_recall_m: 0.5835 - val_precision_m: 0.6113 - val_precision: 0.6113 - val_recall_6: 0.5835 - val_f1score: 0.5964\n",
      "Epoch 146/300\n",
      "epoch:  145\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.3062 - acc: 0.8565 - prec: 0.9580 - recall: 0.9725 - prec_1: 0.9258 - recall_1: 0.9290 - prec_2: 0.9541 - recall_2: 0.9447 - prec_3: 0.6778 - recall_3: 0.6776 - prec_4: 0.6738 - recall_4: 0.6756 - prec_5: 0.9541 - recall_5: 0.9687 - f1_m: 0.8561 - recall_m: 0.8475 - precision_m: 0.8651 - precision: 0.8651 - recall_6: 0.8475 - f1score: 0.8561 - val_loss: 9.5958 - val_acc: 0.2389 - val_prec: 0.1667 - val_recall: 0.1759 - val_prec_1: 0.1439 - val_recall_1: 0.0279 - val_prec_2: 0.1119 - val_recall_2: 0.0247 - val_prec_3: 0.0160 - val_recall_3: 9.9950e-04 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1044 - val_recall_5: 0.0202 - val_f1_m: 0.2373 - val_recall_m: 0.2371 - val_precision_m: 0.2374 - val_precision: 0.2374 - val_recall_6: 0.2371 - val_f1score: 0.2373\n",
      "Epoch 147/300\n",
      "epoch:  146\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.3121 - acc: 0.8601 - prec: 0.9665 - recall: 0.9736 - prec_1: 0.9153 - recall_1: 0.9287 - prec_2: 0.9571 - recall_2: 0.9428 - prec_3: 0.6962 - recall_3: 0.6795 - prec_4: 0.6835 - recall_4: 0.7024 - prec_5: 0.9437 - recall_5: 0.9592 - f1_m: 0.8591 - recall_m: 0.8508 - precision_m: 0.8677 - precision: 0.8677 - recall_6: 0.8508 - f1score: 0.8591 - val_loss: 1.5217 - val_acc: 0.6682 - val_prec: 0.1696 - val_recall: 0.1733 - val_prec_1: 0.1744 - val_recall_1: 0.1278 - val_prec_2: 0.1879 - val_recall_2: 0.1646 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1613 - val_recall_4: 0.0942 - val_prec_5: 0.1684 - val_recall_5: 0.1443 - val_f1_m: 0.6623 - val_recall_m: 0.6472 - val_precision_m: 0.6813 - val_precision: 0.6813 - val_recall_6: 0.6472 - val_f1score: 0.6623\n",
      "Epoch 148/300\n",
      "epoch:  147\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.3054 - acc: 0.8612 - prec: 0.9665 - recall: 0.9762 - prec_1: 0.9227 - recall_1: 0.9309 - prec_2: 0.9535 - recall_2: 0.9483 - prec_3: 0.6937 - recall_3: 0.6765 - prec_4: 0.6801 - recall_4: 0.6924 - prec_5: 0.9573 - recall_5: 0.9637 - f1_m: 0.8594 - recall_m: 0.8527 - precision_m: 0.8663 - precision: 0.8663 - recall_6: 0.8527 - f1score: 0.8594 - val_loss: 0.5600 - val_acc: 0.7419 - val_prec: 0.1759 - val_recall: 0.1642 - val_prec_1: 0.1740 - val_recall_1: 0.1324 - val_prec_2: 0.1879 - val_recall_2: 0.1392 - val_prec_3: 0.1279 - val_recall_3: 0.0647 - val_prec_4: 0.1759 - val_recall_4: 0.1337 - val_prec_5: 0.1684 - val_recall_5: 0.1577 - val_f1_m: 0.7386 - val_recall_m: 0.7254 - val_precision_m: 0.7533 - val_precision: 0.7533 - val_recall_6: 0.7254 - val_f1score: 0.7386\n",
      "Epoch 149/300\n",
      "epoch:  148\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.3071 - acc: 0.8583 - prec: 0.9630 - recall: 0.9702 - prec_1: 0.9094 - recall_1: 0.9207 - prec_2: 0.9508 - recall_2: 0.9409 - prec_3: 0.7016 - recall_3: 0.6806 - prec_4: 0.6815 - recall_4: 0.7070 - prec_5: 0.9475 - recall_5: 0.9616 - f1_m: 0.8560 - recall_m: 0.8461 - precision_m: 0.8664 - precision: 0.8664 - recall_6: 0.8461 - f1score: 0.8560 - val_loss: 3.4117 - val_acc: 0.5405 - val_prec: 0.1668 - val_recall: 0.1747 - val_prec_1: 0.1759 - val_recall_1: 0.0674 - val_prec_2: 0.1866 - val_recall_2: 0.1184 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1809 - val_recall_4: 0.1872 - val_prec_5: 0.1524 - val_recall_5: 0.0412 - val_f1_m: 0.5404 - val_recall_m: 0.5385 - val_precision_m: 0.5424 - val_precision: 0.5424 - val_recall_6: 0.5385 - val_f1score: 0.5404\n",
      "Epoch 150/300\n",
      "epoch:  149\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.3083 - acc: 0.8572 - prec: 0.9701 - recall: 0.9724 - prec_1: 0.9060 - recall_1: 0.9265 - prec_2: 0.9550 - recall_2: 0.9499 - prec_3: 0.6907 - recall_3: 0.6798 - prec_4: 0.6837 - recall_4: 0.6939 - prec_5: 0.9422 - recall_5: 0.9566 - f1_m: 0.8564 - recall_m: 0.8488 - precision_m: 0.8643 - precision: 0.8643 - recall_6: 0.8488 - f1score: 0.8564 - val_loss: 0.5080 - val_acc: 0.7751 - val_prec: 0.1759 - val_recall: 0.1689 - val_prec_1: 0.1752 - val_recall_1: 0.1222 - val_prec_2: 0.1844 - val_recall_2: 0.1830 - val_prec_3: 0.1709 - val_recall_3: 0.1469 - val_prec_4: 0.1439 - val_recall_4: 0.0580 - val_prec_5: 0.1684 - val_recall_5: 0.1596 - val_f1_m: 0.7722 - val_recall_m: 0.7606 - val_precision_m: 0.7850 - val_precision: 0.7850 - val_recall_6: 0.7606 - val_f1score: 0.7722\n",
      "Epoch 151/300\n",
      "epoch:  150\n",
      "Learning rate:  0.01\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.3073 - acc: 0.8605 - prec: 0.9707 - recall: 0.9711 - prec_1: 0.9075 - recall_1: 0.9333 - prec_2: 0.9530 - recall_2: 0.9371 - prec_3: 0.6873 - recall_3: 0.6961 - prec_4: 0.6926 - recall_4: 0.6828 - prec_5: 0.9574 - recall_5: 0.9593 - f1_m: 0.8591 - recall_m: 0.8511 - precision_m: 0.8674 - precision: 0.8674 - recall_6: 0.8511 - f1score: 0.8591 - val_loss: 2.9192 - val_acc: 0.5322 - val_prec: 0.1759 - val_recall: 0.1617 - val_prec_1: 0.1759 - val_recall_1: 0.0803 - val_prec_2: 0.1828 - val_recall_2: 0.1495 - val_prec_3: 0.0480 - val_recall_3: 0.0072 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1666 - val_recall_5: 0.1631 - val_f1_m: 0.5228 - val_recall_m: 0.5112 - val_precision_m: 0.5356 - val_precision: 0.5356 - val_recall_6: 0.5112 - val_f1score: 0.5228\n",
      "Epoch 152/300\n",
      "epoch:  151\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2899 - acc: 0.8660 - prec: 0.9675 - recall: 0.9809 - prec_1: 0.9149 - recall_1: 0.9498 - prec_2: 0.9615 - recall_2: 0.9411 - prec_3: 0.7159 - recall_3: 0.6405 - prec_4: 0.6675 - recall_4: 0.7473 - prec_5: 0.9674 - recall_5: 0.9636 - f1_m: 0.8659 - recall_m: 0.8585 - precision_m: 0.8736 - precision: 0.8736 - recall_6: 0.8585 - f1score: 0.8659 - val_loss: 0.4686 - val_acc: 0.7581 - val_prec: 0.1759 - val_recall: 0.1704 - val_prec_1: 0.1740 - val_recall_1: 0.1704 - val_prec_2: 0.1919 - val_recall_2: 0.1463 - val_prec_3: 0.0960 - val_recall_3: 0.0300 - val_prec_4: 0.1649 - val_recall_4: 0.1427 - val_prec_5: 0.1684 - val_recall_5: 0.1435 - val_f1_m: 0.7575 - val_recall_m: 0.7501 - val_precision_m: 0.7653 - val_precision: 0.7653 - val_recall_6: 0.7501 - val_f1score: 0.7575\n",
      "Epoch 153/300\n",
      "epoch:  152\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2885 - acc: 0.8703 - prec: 0.9700 - recall: 0.9776 - prec_1: 0.9334 - recall_1: 0.9415 - prec_2: 0.9612 - recall_2: 0.9609 - prec_3: 0.7137 - recall_3: 0.6664 - prec_4: 0.6775 - recall_4: 0.7334 - prec_5: 0.9620 - recall_5: 0.9666 - f1_m: 0.8688 - recall_m: 0.8612 - precision_m: 0.8767 - precision: 0.8767 - recall_6: 0.8612 - f1score: 0.8688 - val_loss: 0.5470 - val_acc: 0.7484 - val_prec: 0.1753 - val_recall: 0.1743 - val_prec_1: 0.1738 - val_recall_1: 0.1624 - val_prec_2: 0.1919 - val_recall_2: 0.1386 - val_prec_3: 0.1756 - val_recall_3: 0.1514 - val_prec_4: 0.1599 - val_recall_4: 0.0412 - val_prec_5: 0.1684 - val_recall_5: 0.1335 - val_f1_m: 0.7443 - val_recall_m: 0.7336 - val_precision_m: 0.7567 - val_precision: 0.7567 - val_recall_6: 0.7336 - val_f1score: 0.7443\n",
      "Epoch 154/300\n",
      "epoch:  153\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2846 - acc: 0.8776 - prec: 0.9702 - recall: 0.9791 - prec_1: 0.9340 - recall_1: 0.9461 - prec_2: 0.9721 - recall_2: 0.9635 - prec_3: 0.7141 - recall_3: 0.7054 - prec_4: 0.6978 - recall_4: 0.7310 - prec_5: 0.9639 - recall_5: 0.9700 - f1_m: 0.8768 - recall_m: 0.8695 - precision_m: 0.8844 - precision: 0.8844 - recall_6: 0.8695 - f1score: 0.8768 - val_loss: 0.3056 - val_acc: 0.8716 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1746 - val_recall_1: 0.1613 - val_prec_2: 0.1896 - val_recall_2: 0.1661 - val_prec_3: 0.1759 - val_recall_3: 0.1417 - val_prec_4: 0.1905 - val_recall_4: 0.1335 - val_prec_5: 0.1684 - val_recall_5: 0.1605 - val_f1_m: 0.8720 - val_recall_m: 0.8646 - val_precision_m: 0.8798 - val_precision: 0.8798 - val_recall_6: 0.8646 - val_f1score: 0.8720\n",
      "Epoch 155/300\n",
      "epoch:  154\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2846 - acc: 0.8760 - prec: 0.9650 - recall: 0.9776 - prec_1: 0.9320 - recall_1: 0.9510 - prec_2: 0.9738 - recall_2: 0.9601 - prec_3: 0.7131 - recall_3: 0.7116 - prec_4: 0.6998 - recall_4: 0.7085 - prec_5: 0.9701 - recall_5: 0.9730 - f1_m: 0.8745 - recall_m: 0.8661 - precision_m: 0.8833 - precision: 0.8833 - recall_6: 0.8661 - f1score: 0.8745 - val_loss: 0.4474 - val_acc: 0.8063 - val_prec: 0.1759 - val_recall: 0.1738 - val_prec_1: 0.1741 - val_recall_1: 0.1665 - val_prec_2: 0.1887 - val_recall_2: 0.1459 - val_prec_3: 0.1439 - val_recall_3: 0.0697 - val_prec_4: 0.1807 - val_recall_4: 0.1648 - val_prec_5: 0.1684 - val_recall_5: 0.1428 - val_f1_m: 0.8066 - val_recall_m: 0.8018 - val_precision_m: 0.8116 - val_precision: 0.8116 - val_recall_6: 0.8018 - val_f1score: 0.8066\n",
      "Epoch 156/300\n",
      "epoch:  155\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 679us/step - loss: 0.2832 - acc: 0.8728 - prec: 0.9677 - recall: 0.9761 - prec_1: 0.9472 - recall_1: 0.9499 - prec_2: 0.9630 - recall_2: 0.9627 - prec_3: 0.7075 - recall_3: 0.6925 - prec_4: 0.6931 - recall_4: 0.7179 - prec_5: 0.9637 - recall_5: 0.9710 - f1_m: 0.8720 - recall_m: 0.8642 - precision_m: 0.8800 - precision: 0.8800 - recall_6: 0.8642 - f1score: 0.8720 - val_loss: 0.7219 - val_acc: 0.8146 - val_prec: 0.1759 - val_recall: 0.1721 - val_prec_1: 0.1749 - val_recall_1: 0.1616 - val_prec_2: 0.1901 - val_recall_2: 0.1778 - val_prec_3: 0.1439 - val_recall_3: 0.0759 - val_prec_4: 0.1759 - val_recall_4: 0.1292 - val_prec_5: 0.1684 - val_recall_5: 0.1572 - val_f1_m: 0.8124 - val_recall_m: 0.8013 - val_precision_m: 0.8263 - val_precision: 0.8263 - val_recall_6: 0.8013 - val_f1score: 0.8124\n",
      "Epoch 157/300\n",
      "epoch:  156\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2852 - acc: 0.8691 - prec: 0.9670 - recall: 0.9755 - prec_1: 0.9378 - recall_1: 0.9450 - prec_2: 0.9713 - recall_2: 0.9601 - prec_3: 0.7044 - recall_3: 0.6755 - prec_4: 0.6829 - recall_4: 0.7214 - prec_5: 0.9556 - recall_5: 0.9693 - f1_m: 0.8681 - recall_m: 0.8607 - precision_m: 0.8758 - precision: 0.8758 - recall_6: 0.8607 - f1score: 0.8681 - val_loss: 0.3441 - val_acc: 0.8343 - val_prec: 0.1759 - val_recall: 0.1698 - val_prec_1: 0.1748 - val_recall_1: 0.1548 - val_prec_2: 0.1899 - val_recall_2: 0.1747 - val_prec_3: 0.1756 - val_recall_3: 0.1637 - val_prec_4: 0.1759 - val_recall_4: 0.0659 - val_prec_5: 0.1684 - val_recall_5: 0.1638 - val_f1_m: 0.8294 - val_recall_m: 0.8218 - val_precision_m: 0.8373 - val_precision: 0.8373 - val_recall_6: 0.8218 - val_f1score: 0.8294\n",
      "Epoch 158/300\n",
      "epoch:  157\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2873 - acc: 0.8720 - prec: 0.9660 - recall: 0.9755 - prec_1: 0.9375 - recall_1: 0.9458 - prec_2: 0.9709 - recall_2: 0.9600 - prec_3: 0.7016 - recall_3: 0.6963 - prec_4: 0.6925 - recall_4: 0.7105 - prec_5: 0.9608 - recall_5: 0.9683 - f1_m: 0.8707 - recall_m: 0.8623 - precision_m: 0.8795 - precision: 0.8795 - recall_6: 0.8623 - f1score: 0.8707 - val_loss: 0.3724 - val_acc: 0.8493 - val_prec: 0.1759 - val_recall: 0.1728 - val_prec_1: 0.1749 - val_recall_1: 0.1603 - val_prec_2: 0.1901 - val_recall_2: 0.1776 - val_prec_3: 0.1599 - val_recall_3: 0.0730 - val_prec_4: 0.1809 - val_recall_4: 0.1772 - val_prec_5: 0.1684 - val_recall_5: 0.1587 - val_f1_m: 0.8485 - val_recall_m: 0.8408 - val_precision_m: 0.8566 - val_precision: 0.8566 - val_recall_6: 0.8408 - val_f1score: 0.8485\n",
      "Epoch 159/300\n",
      "epoch:  158\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2801 - acc: 0.8760 - prec: 0.9695 - recall: 0.9800 - prec_1: 0.9436 - recall_1: 0.9537 - prec_2: 0.9751 - recall_2: 0.9613 - prec_3: 0.7148 - recall_3: 0.6802 - prec_4: 0.6911 - recall_4: 0.7325 - prec_5: 0.9583 - recall_5: 0.9718 - f1_m: 0.8745 - recall_m: 0.8676 - precision_m: 0.8817 - precision: 0.8817 - recall_6: 0.8676 - f1score: 0.8745 - val_loss: 0.3252 - val_acc: 0.8466 - val_prec: 0.1759 - val_recall: 0.1708 - val_prec_1: 0.1743 - val_recall_1: 0.1637 - val_prec_2: 0.1892 - val_recall_2: 0.1553 - val_prec_3: 0.1759 - val_recall_3: 0.1195 - val_prec_4: 0.1832 - val_recall_4: 0.1315 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.8444 - val_recall_m: 0.8388 - val_precision_m: 0.8503 - val_precision: 0.8503 - val_recall_6: 0.8388 - val_f1score: 0.8444\n",
      "Epoch 160/300\n",
      "epoch:  159\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2838 - acc: 0.8762 - prec: 0.9637 - recall: 0.9814 - prec_1: 0.9381 - recall_1: 0.9543 - prec_2: 0.9727 - recall_2: 0.9560 - prec_3: 0.7167 - recall_3: 0.7108 - prec_4: 0.7020 - recall_4: 0.7202 - prec_5: 0.9656 - recall_5: 0.9685 - f1_m: 0.8745 - recall_m: 0.8671 - precision_m: 0.8822 - precision: 0.8822 - recall_6: 0.8671 - f1score: 0.8745 - val_loss: 0.3160 - val_acc: 0.8623 - val_prec: 0.1759 - val_recall: 0.1712 - val_prec_1: 0.1753 - val_recall_1: 0.1536 - val_prec_2: 0.1899 - val_recall_2: 0.1729 - val_prec_3: 0.1759 - val_recall_3: 0.1469 - val_prec_4: 0.1745 - val_recall_4: 0.1139 - val_prec_5: 0.1684 - val_recall_5: 0.1640 - val_f1_m: 0.8617 - val_recall_m: 0.8546 - val_precision_m: 0.8691 - val_precision: 0.8691 - val_recall_6: 0.8546 - val_f1score: 0.8617\n",
      "Epoch 161/300\n",
      "epoch:  160\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2830 - acc: 0.8727 - prec: 0.9669 - recall: 0.9800 - prec_1: 0.9442 - recall_1: 0.9417 - prec_2: 0.9605 - recall_2: 0.9623 - prec_3: 0.7054 - recall_3: 0.7047 - prec_4: 0.7074 - recall_4: 0.7115 - prec_5: 0.9583 - recall_5: 0.9717 - f1_m: 0.8728 - recall_m: 0.8655 - precision_m: 0.8805 - precision: 0.8805 - recall_6: 0.8655 - f1score: 0.8728 - val_loss: 0.3237 - val_acc: 0.8418 - val_prec: 0.1759 - val_recall: 0.1708 - val_prec_1: 0.1743 - val_recall_1: 0.1663 - val_prec_2: 0.1892 - val_recall_2: 0.1628 - val_prec_3: 0.1756 - val_recall_3: 0.1399 - val_prec_4: 0.1919 - val_recall_4: 0.1131 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8414 - val_recall_m: 0.8346 - val_precision_m: 0.8484 - val_precision: 0.8484 - val_recall_6: 0.8346 - val_f1score: 0.8414\n",
      "Epoch 162/300\n",
      "epoch:  161\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 675us/step - loss: 0.2863 - acc: 0.8711 - prec: 0.9705 - recall: 0.9795 - prec_1: 0.9374 - recall_1: 0.9483 - prec_2: 0.9586 - recall_2: 0.9597 - prec_3: 0.6973 - recall_3: 0.6732 - prec_4: 0.6857 - recall_4: 0.7174 - prec_5: 0.9662 - recall_5: 0.9664 - f1_m: 0.8697 - recall_m: 0.8616 - precision_m: 0.8781 - precision: 0.8781 - recall_6: 0.8616 - f1score: 0.8697 - val_loss: 0.6880 - val_acc: 0.7954 - val_prec: 0.1759 - val_recall: 0.1684 - val_prec_1: 0.1759 - val_recall_1: 0.1243 - val_prec_2: 0.1882 - val_recall_2: 0.1882 - val_prec_3: 0.1279 - val_recall_3: 0.0465 - val_prec_4: 0.1809 - val_recall_4: 0.1804 - val_prec_5: 0.1684 - val_recall_5: 0.1584 - val_f1_m: 0.7954 - val_recall_m: 0.7879 - val_precision_m: 0.8034 - val_precision: 0.8034 - val_recall_6: 0.7879 - val_f1score: 0.7954\n",
      "Epoch 163/300\n",
      "epoch:  162\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2851 - acc: 0.8692 - prec: 0.9691 - recall: 0.9782 - prec_1: 0.9302 - recall_1: 0.9477 - prec_2: 0.9702 - recall_2: 0.9601 - prec_3: 0.6917 - recall_3: 0.6830 - prec_4: 0.6791 - recall_4: 0.6991 - prec_5: 0.9625 - recall_5: 0.9663 - f1_m: 0.8683 - recall_m: 0.8608 - precision_m: 0.8760 - precision: 0.8760 - recall_6: 0.8608 - f1score: 0.8683 - val_loss: 0.3044 - val_acc: 0.8631 - val_prec: 0.1759 - val_recall: 0.1728 - val_prec_1: 0.1749 - val_recall_1: 0.1616 - val_prec_2: 0.1884 - val_recall_2: 0.1732 - val_prec_3: 0.1755 - val_recall_3: 0.1601 - val_prec_4: 0.1897 - val_recall_4: 0.0947 - val_prec_5: 0.1684 - val_recall_5: 0.1612 - val_f1_m: 0.8600 - val_recall_m: 0.8531 - val_precision_m: 0.8672 - val_precision: 0.8672 - val_recall_6: 0.8531 - val_f1score: 0.8600\n",
      "Epoch 164/300\n",
      "epoch:  163\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2765 - acc: 0.8792 - prec: 0.9710 - recall: 0.9809 - prec_1: 0.9455 - recall_1: 0.9550 - prec_2: 0.9750 - recall_2: 0.9623 - prec_3: 0.7100 - recall_3: 0.7008 - prec_4: 0.7031 - recall_4: 0.7194 - prec_5: 0.9667 - recall_5: 0.9700 - f1_m: 0.8776 - recall_m: 0.8700 - precision_m: 0.8854 - precision: 0.8854 - recall_6: 0.8700 - f1score: 0.8776 - val_loss: 0.3732 - val_acc: 0.8123 - val_prec: 0.1759 - val_recall: 0.1696 - val_prec_1: 0.1749 - val_recall_1: 0.1592 - val_prec_2: 0.1901 - val_recall_2: 0.1793 - val_prec_3: 0.1279 - val_recall_3: 0.0216 - val_prec_4: 0.1810 - val_recall_4: 0.1914 - val_prec_5: 0.1684 - val_recall_5: 0.1581 - val_f1_m: 0.8126 - val_recall_m: 0.8043 - val_precision_m: 0.8215 - val_precision: 0.8215 - val_recall_6: 0.8043 - val_f1score: 0.8126\n",
      "Epoch 165/300\n",
      "epoch:  164\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2812 - acc: 0.8743 - prec: 0.9688 - recall: 0.9783 - prec_1: 0.9388 - recall_1: 0.9494 - prec_2: 0.9768 - recall_2: 0.9617 - prec_3: 0.7036 - recall_3: 0.6928 - prec_4: 0.6947 - recall_4: 0.7159 - prec_5: 0.9641 - recall_5: 0.9689 - f1_m: 0.8733 - recall_m: 0.8665 - precision_m: 0.8803 - precision: 0.8803 - recall_6: 0.8665 - f1score: 0.8733 - val_loss: 0.7305 - val_acc: 0.7964 - val_prec: 0.1759 - val_recall: 0.1707 - val_prec_1: 0.1753 - val_recall_1: 0.1603 - val_prec_2: 0.1901 - val_recall_2: 0.1780 - val_prec_3: 0.0640 - val_recall_3: 0.0190 - val_prec_4: 0.1809 - val_recall_4: 0.1710 - val_prec_5: 0.1684 - val_recall_5: 0.1537 - val_f1_m: 0.7967 - val_recall_m: 0.7891 - val_precision_m: 0.8049 - val_precision: 0.8049 - val_recall_6: 0.7891 - val_f1score: 0.7967\n",
      "Epoch 166/300\n",
      "epoch:  165\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2853 - acc: 0.8727 - prec: 0.9645 - recall: 0.9802 - prec_1: 0.9341 - recall_1: 0.9561 - prec_2: 0.9716 - recall_2: 0.9624 - prec_3: 0.7061 - recall_3: 0.6908 - prec_4: 0.6930 - recall_4: 0.7136 - prec_5: 0.9626 - recall_5: 0.9638 - f1_m: 0.8714 - recall_m: 0.8633 - precision_m: 0.8797 - precision: 0.8797 - recall_6: 0.8633 - f1score: 0.8714 - val_loss: 0.3732 - val_acc: 0.8351 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1743 - val_recall_1: 0.1661 - val_prec_2: 0.1919 - val_recall_2: 0.1605 - val_prec_3: 0.1756 - val_recall_3: 0.1682 - val_prec_4: 0.1759 - val_recall_4: 0.0778 - val_prec_5: 0.1684 - val_recall_5: 0.1576 - val_f1_m: 0.8320 - val_recall_m: 0.8258 - val_precision_m: 0.8385 - val_precision: 0.8385 - val_recall_6: 0.8258 - val_f1score: 0.8320\n",
      "Epoch 167/300\n",
      "epoch:  166\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2793 - acc: 0.8718 - prec: 0.9638 - recall: 0.9798 - prec_1: 0.9391 - recall_1: 0.9462 - prec_2: 0.9761 - recall_2: 0.9581 - prec_3: 0.7032 - recall_3: 0.6800 - prec_4: 0.6801 - recall_4: 0.7121 - prec_5: 0.9642 - recall_5: 0.9737 - f1_m: 0.8714 - recall_m: 0.8635 - precision_m: 0.8796 - precision: 0.8796 - recall_6: 0.8635 - f1score: 0.8714 - val_loss: 0.3176 - val_acc: 0.8438 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1746 - val_recall_1: 0.1648 - val_prec_2: 0.1896 - val_recall_2: 0.1656 - val_prec_3: 0.1759 - val_recall_3: 0.1482 - val_prec_4: 0.1745 - val_recall_4: 0.0917 - val_prec_5: 0.1684 - val_recall_5: 0.1589 - val_f1_m: 0.8432 - val_recall_m: 0.8358 - val_precision_m: 0.8509 - val_precision: 0.8509 - val_recall_6: 0.8358 - val_f1score: 0.8432\n",
      "Epoch 168/300\n",
      "epoch:  167\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2845 - acc: 0.8743 - prec: 0.9645 - recall: 0.9798 - prec_1: 0.9342 - recall_1: 0.9512 - prec_2: 0.9687 - recall_2: 0.9596 - prec_3: 0.7112 - recall_3: 0.6925 - prec_4: 0.7011 - recall_4: 0.7296 - prec_5: 0.9641 - recall_5: 0.9682 - f1_m: 0.8736 - recall_m: 0.8663 - precision_m: 0.8810 - precision: 0.8810 - recall_6: 0.8663 - f1score: 0.8736 - val_loss: 0.5331 - val_acc: 0.8008 - val_prec: 0.1759 - val_recall: 0.1729 - val_prec_1: 0.1753 - val_recall_1: 0.1595 - val_prec_2: 0.1901 - val_recall_2: 0.1778 - val_prec_3: 0.0960 - val_recall_3: 0.0230 - val_prec_4: 0.1769 - val_recall_4: 0.1656 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.7996 - val_recall_m: 0.7919 - val_precision_m: 0.8078 - val_precision: 0.8078 - val_recall_6: 0.7919 - val_f1score: 0.7996\n",
      "Epoch 169/300\n",
      "epoch:  168\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2801 - acc: 0.8746 - prec: 0.9703 - recall: 0.9778 - prec_1: 0.9387 - recall_1: 0.9554 - prec_2: 0.9664 - recall_2: 0.9604 - prec_3: 0.6944 - recall_3: 0.7153 - prec_4: 0.7015 - recall_4: 0.6922 - prec_5: 0.9695 - recall_5: 0.9723 - f1_m: 0.8728 - recall_m: 0.8647 - precision_m: 0.8813 - precision: 0.8813 - recall_6: 0.8647 - f1score: 0.8728 - val_loss: 0.2935 - val_acc: 0.8678 - val_prec: 0.1759 - val_recall: 0.1728 - val_prec_1: 0.1749 - val_recall_1: 0.1621 - val_prec_2: 0.1899 - val_recall_2: 0.1720 - val_prec_3: 0.1749 - val_recall_3: 0.1216 - val_prec_4: 0.1822 - val_recall_4: 0.1418 - val_prec_5: 0.1684 - val_recall_5: 0.1612 - val_f1_m: 0.8657 - val_recall_m: 0.8571 - val_precision_m: 0.8748 - val_precision: 0.8748 - val_recall_6: 0.8571 - val_f1score: 0.8657\n",
      "Epoch 170/300\n",
      "epoch:  169\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2842 - acc: 0.8737 - prec: 0.9661 - recall: 0.9776 - prec_1: 0.9385 - recall_1: 0.9470 - prec_2: 0.9715 - recall_2: 0.9604 - prec_3: 0.7001 - recall_3: 0.7061 - prec_4: 0.6964 - recall_4: 0.7008 - prec_5: 0.9596 - recall_5: 0.9688 - f1_m: 0.8730 - recall_m: 0.8651 - precision_m: 0.8813 - precision: 0.8813 - recall_6: 0.8651 - f1score: 0.8730 - val_loss: 0.3686 - val_acc: 0.8276 - val_prec: 0.1759 - val_recall: 0.1696 - val_prec_1: 0.1745 - val_recall_1: 0.1542 - val_prec_2: 0.1873 - val_recall_2: 0.1578 - val_prec_3: 0.1716 - val_recall_3: 0.1347 - val_prec_4: 0.1919 - val_recall_4: 0.1086 - val_prec_5: 0.1684 - val_recall_5: 0.1638 - val_f1_m: 0.8276 - val_recall_m: 0.8201 - val_precision_m: 0.8356 - val_precision: 0.8356 - val_recall_6: 0.8201 - val_f1score: 0.8276\n",
      "Epoch 171/300\n",
      "epoch:  170\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2817 - acc: 0.8708 - prec: 0.9702 - recall: 0.9759 - prec_1: 0.9225 - recall_1: 0.9561 - prec_2: 0.9715 - recall_2: 0.9560 - prec_3: 0.6971 - recall_3: 0.6930 - prec_4: 0.6930 - recall_4: 0.7166 - prec_5: 0.9667 - recall_5: 0.9648 - f1_m: 0.8705 - recall_m: 0.8630 - precision_m: 0.8784 - precision: 0.8784 - recall_6: 0.8630 - f1score: 0.8705 - val_loss: 0.7689 - val_acc: 0.7374 - val_prec: 0.1759 - val_recall: 0.1743 - val_prec_1: 0.1741 - val_recall_1: 0.1633 - val_prec_2: 0.1887 - val_recall_2: 0.1512 - val_prec_3: 0.1119 - val_recall_3: 0.0430 - val_prec_4: 0.1603 - val_recall_4: 0.1035 - val_prec_5: 0.1684 - val_recall_5: 0.1378 - val_f1_m: 0.7370 - val_recall_m: 0.7324 - val_precision_m: 0.7419 - val_precision: 0.7419 - val_recall_6: 0.7324 - val_f1score: 0.7370\n",
      "Epoch 172/300\n",
      "epoch:  171\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2800 - acc: 0.8763 - prec: 0.9724 - recall: 0.9808 - prec_1: 0.9409 - recall_1: 0.9542 - prec_2: 0.9665 - recall_2: 0.9616 - prec_3: 0.7051 - recall_3: 0.7146 - prec_4: 0.7074 - recall_4: 0.7144 - prec_5: 0.9684 - recall_5: 0.9695 - f1_m: 0.8753 - recall_m: 0.8668 - precision_m: 0.8842 - precision: 0.8842 - recall_6: 0.8668 - f1score: 0.8753 - val_loss: 0.4129 - val_acc: 0.8033 - val_prec: 0.1759 - val_recall: 0.1668 - val_prec_1: 0.1750 - val_recall_1: 0.1334 - val_prec_2: 0.1890 - val_recall_2: 0.1863 - val_prec_3: 0.1709 - val_recall_3: 0.1429 - val_prec_4: 0.1599 - val_recall_4: 0.0770 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8028 - val_recall_m: 0.7946 - val_precision_m: 0.8116 - val_precision: 0.8116 - val_recall_6: 0.7946 - val_f1score: 0.8028\n",
      "Epoch 173/300\n",
      "epoch:  172\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 678us/step - loss: 0.2777 - acc: 0.8778 - prec: 0.9645 - recall: 0.9806 - prec_1: 0.9378 - recall_1: 0.9540 - prec_2: 0.9721 - recall_2: 0.9634 - prec_3: 0.7129 - recall_3: 0.6982 - prec_4: 0.7034 - recall_4: 0.7256 - prec_5: 0.9699 - recall_5: 0.9657 - f1_m: 0.8765 - recall_m: 0.8682 - precision_m: 0.8852 - precision: 0.8852 - recall_6: 0.8682 - f1score: 0.8765 - val_loss: 0.7749 - val_acc: 0.8008 - val_prec: 0.1759 - val_recall: 0.1706 - val_prec_1: 0.1748 - val_recall_1: 0.1393 - val_prec_2: 0.1887 - val_recall_2: 0.1837 - val_prec_3: 0.0960 - val_recall_3: 0.0233 - val_prec_4: 0.1809 - val_recall_4: 0.1894 - val_prec_5: 0.1684 - val_recall_5: 0.1607 - val_f1_m: 0.8004 - val_recall_m: 0.7924 - val_precision_m: 0.8091 - val_precision: 0.8091 - val_recall_6: 0.7924 - val_f1score: 0.8004\n",
      "Epoch 174/300\n",
      "epoch:  173\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2815 - acc: 0.8751 - prec: 0.9653 - recall: 0.9803 - prec_1: 0.9410 - recall_1: 0.9495 - prec_2: 0.9702 - recall_2: 0.9662 - prec_3: 0.7036 - recall_3: 0.7031 - prec_4: 0.7000 - recall_4: 0.7088 - prec_5: 0.9597 - recall_5: 0.9690 - f1_m: 0.8739 - recall_m: 0.8662 - precision_m: 0.8818 - precision: 0.8818 - recall_6: 0.8662 - f1score: 0.8739 - val_loss: 0.3565 - val_acc: 0.8268 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1648 - val_prec_2: 0.1901 - val_recall_2: 0.1726 - val_prec_3: 0.1599 - val_recall_3: 0.0436 - val_prec_4: 0.1816 - val_recall_4: 0.1834 - val_prec_5: 0.1684 - val_recall_5: 0.1575 - val_f1_m: 0.8249 - val_recall_m: 0.8181 - val_precision_m: 0.8320 - val_precision: 0.8320 - val_recall_6: 0.8181 - val_f1score: 0.8249\n",
      "Epoch 175/300\n",
      "epoch:  174\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2787 - acc: 0.8768 - prec: 0.9633 - recall: 0.9790 - prec_1: 0.9378 - recall_1: 0.9525 - prec_2: 0.9742 - recall_2: 0.9578 - prec_3: 0.7180 - recall_3: 0.6900 - prec_4: 0.7008 - recall_4: 0.7320 - prec_5: 0.9673 - recall_5: 0.9690 - f1_m: 0.8762 - recall_m: 0.8683 - precision_m: 0.8843 - precision: 0.8843 - recall_6: 0.8683 - f1score: 0.8762 - val_loss: 0.4249 - val_acc: 0.8111 - val_prec: 0.1759 - val_recall: 0.1693 - val_prec_1: 0.1748 - val_recall_1: 0.1525 - val_prec_2: 0.1884 - val_recall_2: 0.1786 - val_prec_3: 0.1756 - val_recall_3: 0.1712 - val_prec_4: 0.1439 - val_recall_4: 0.0362 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8094 - val_recall_m: 0.8003 - val_precision_m: 0.8190 - val_precision: 0.8190 - val_recall_6: 0.8003 - val_f1score: 0.8094\n",
      "Epoch 176/300\n",
      "epoch:  175\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2805 - acc: 0.8748 - prec: 0.9600 - recall: 0.9775 - prec_1: 0.9362 - recall_1: 0.9457 - prec_2: 0.9703 - recall_2: 0.9569 - prec_3: 0.7119 - recall_3: 0.7015 - prec_4: 0.6984 - recall_4: 0.7180 - prec_5: 0.9660 - recall_5: 0.9710 - f1_m: 0.8746 - recall_m: 0.8671 - precision_m: 0.8824 - precision: 0.8824 - recall_6: 0.8671 - f1score: 0.8746 - val_loss: 0.3666 - val_acc: 0.8251 - val_prec: 0.1759 - val_recall: 0.1709 - val_prec_1: 0.1753 - val_recall_1: 0.1493 - val_prec_2: 0.1887 - val_recall_2: 0.1825 - val_prec_3: 0.1670 - val_recall_3: 0.1248 - val_prec_4: 0.1599 - val_recall_4: 0.0862 - val_prec_5: 0.1684 - val_recall_5: 0.1578 - val_f1_m: 0.8244 - val_recall_m: 0.8153 - val_precision_m: 0.8341 - val_precision: 0.8341 - val_recall_6: 0.8153 - val_f1score: 0.8244\n",
      "Epoch 177/300\n",
      "epoch:  176\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2807 - acc: 0.8762 - prec: 0.9688 - recall: 0.9784 - prec_1: 0.9375 - recall_1: 0.9542 - prec_2: 0.9777 - recall_2: 0.9595 - prec_3: 0.7070 - recall_3: 0.7115 - prec_4: 0.7051 - recall_4: 0.7093 - prec_5: 0.9657 - recall_5: 0.9685 - f1_m: 0.8764 - recall_m: 0.8686 - precision_m: 0.8845 - precision: 0.8845 - recall_6: 0.8686 - f1score: 0.8764 - val_loss: 0.3045 - val_acc: 0.8643 - val_prec: 0.1759 - val_recall: 0.1696 - val_prec_1: 0.1749 - val_recall_1: 0.1662 - val_prec_2: 0.1901 - val_recall_2: 0.1708 - val_prec_3: 0.1755 - val_recall_3: 0.1526 - val_prec_4: 0.1911 - val_recall_4: 0.1100 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8611 - val_recall_m: 0.8533 - val_precision_m: 0.8693 - val_precision: 0.8693 - val_recall_6: 0.8533 - val_f1score: 0.8611\n",
      "Epoch 178/300\n",
      "epoch:  177\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2824 - acc: 0.8731 - prec: 0.9658 - recall: 0.9775 - prec_1: 0.9410 - recall_1: 0.9521 - prec_2: 0.9703 - recall_2: 0.9606 - prec_3: 0.6946 - recall_3: 0.6969 - prec_4: 0.6999 - recall_4: 0.7001 - prec_5: 0.9638 - recall_5: 0.9707 - f1_m: 0.8717 - recall_m: 0.8646 - precision_m: 0.8791 - precision: 0.8791 - recall_6: 0.8646 - f1score: 0.8717 - val_loss: 0.3423 - val_acc: 0.8273 - val_prec: 0.1759 - val_recall: 0.1731 - val_prec_1: 0.1746 - val_recall_1: 0.1608 - val_prec_2: 0.1892 - val_recall_2: 0.1655 - val_prec_3: 0.1599 - val_recall_3: 0.0840 - val_prec_4: 0.1807 - val_recall_4: 0.1458 - val_prec_5: 0.1684 - val_recall_5: 0.1607 - val_f1_m: 0.8237 - val_recall_m: 0.8141 - val_precision_m: 0.8340 - val_precision: 0.8340 - val_recall_6: 0.8141 - val_f1score: 0.8237\n",
      "Epoch 179/300\n",
      "epoch:  178\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 675us/step - loss: 0.2822 - acc: 0.8732 - prec: 0.9623 - recall: 0.9762 - prec_1: 0.9263 - recall_1: 0.9529 - prec_2: 0.9718 - recall_2: 0.9590 - prec_3: 0.7054 - recall_3: 0.7079 - prec_4: 0.6984 - recall_4: 0.7096 - prec_5: 0.9621 - recall_5: 0.9715 - f1_m: 0.8721 - recall_m: 0.8641 - precision_m: 0.8803 - precision: 0.8803 - recall_6: 0.8641 - f1score: 0.8721 - val_loss: 0.5038 - val_acc: 0.8061 - val_prec: 0.1759 - val_recall: 0.1696 - val_prec_1: 0.1747 - val_recall_1: 0.1444 - val_prec_2: 0.1901 - val_recall_2: 0.1805 - val_prec_3: 0.1279 - val_recall_3: 0.0695 - val_prec_4: 0.1800 - val_recall_4: 0.1454 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8044 - val_recall_m: 0.7966 - val_precision_m: 0.8128 - val_precision: 0.8128 - val_recall_6: 0.7966 - val_f1score: 0.8044\n",
      "Epoch 180/300\n",
      "epoch:  179\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2811 - acc: 0.8725 - prec: 0.9686 - recall: 0.9730 - prec_1: 0.9373 - recall_1: 0.9470 - prec_2: 0.9698 - recall_2: 0.9626 - prec_3: 0.7020 - recall_3: 0.7011 - prec_4: 0.6932 - recall_4: 0.7031 - prec_5: 0.9597 - recall_5: 0.9689 - f1_m: 0.8710 - recall_m: 0.8627 - precision_m: 0.8796 - precision: 0.8796 - recall_6: 0.8627 - f1score: 0.8710 - val_loss: 0.7307 - val_acc: 0.7951 - val_prec: 0.1759 - val_recall: 0.1736 - val_prec_1: 0.1741 - val_recall_1: 0.1653 - val_prec_2: 0.1887 - val_recall_2: 0.1502 - val_prec_3: 0.1439 - val_recall_3: 0.0759 - val_prec_4: 0.1778 - val_recall_4: 0.1427 - val_prec_5: 0.1684 - val_recall_5: 0.1443 - val_f1_m: 0.7932 - val_recall_m: 0.7866 - val_precision_m: 0.8002 - val_precision: 0.8002 - val_recall_6: 0.7866 - val_f1score: 0.7932\n",
      "Epoch 181/300\n",
      "epoch:  180\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2734 - acc: 0.8781 - prec: 0.9733 - recall: 0.9782 - prec_1: 0.9441 - recall_1: 0.9483 - prec_2: 0.9713 - recall_2: 0.9682 - prec_3: 0.7184 - recall_3: 0.6971 - prec_4: 0.6997 - recall_4: 0.7369 - prec_5: 0.9680 - recall_5: 0.9730 - f1_m: 0.8772 - recall_m: 0.8695 - precision_m: 0.8852 - precision: 0.8852 - recall_6: 0.8695 - f1score: 0.8772 - val_loss: 0.4706 - val_acc: 0.8003 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1507 - val_prec_2: 0.1887 - val_recall_2: 0.1815 - val_prec_3: 0.0800 - val_recall_3: 0.0125 - val_prec_4: 0.1809 - val_recall_4: 0.1882 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8017 - val_recall_m: 0.7949 - val_precision_m: 0.8090 - val_precision: 0.8090 - val_recall_6: 0.7949 - val_f1score: 0.8017\n",
      "Epoch 182/300\n",
      "epoch:  181\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2778 - acc: 0.8767 - prec: 0.9635 - recall: 0.9794 - prec_1: 0.9374 - recall_1: 0.9533 - prec_2: 0.9734 - recall_2: 0.9621 - prec_3: 0.7101 - recall_3: 0.7106 - prec_4: 0.7025 - recall_4: 0.7063 - prec_5: 0.9651 - recall_5: 0.9686 - f1_m: 0.8753 - recall_m: 0.8672 - precision_m: 0.8837 - precision: 0.8837 - recall_6: 0.8672 - f1score: 0.8753 - val_loss: 0.3626 - val_acc: 0.8066 - val_prec: 0.1759 - val_recall: 0.1709 - val_prec_1: 0.1747 - val_recall_1: 0.1615 - val_prec_2: 0.1892 - val_recall_2: 0.1568 - val_prec_3: 0.1279 - val_recall_3: 0.0282 - val_prec_4: 0.1812 - val_recall_4: 0.1897 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8064 - val_recall_m: 0.8013 - val_precision_m: 0.8117 - val_precision: 0.8117 - val_recall_6: 0.8013 - val_f1score: 0.8064\n",
      "Epoch 183/300\n",
      "epoch:  182\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.2827 - acc: 0.8715 - prec: 0.9732 - recall: 0.9793 - prec_1: 0.9295 - recall_1: 0.9551 - prec_2: 0.9677 - recall_2: 0.9563 - prec_3: 0.7033 - recall_3: 0.7012 - prec_4: 0.6958 - recall_4: 0.7082 - prec_5: 0.9622 - recall_5: 0.9688 - f1_m: 0.8722 - recall_m: 0.8643 - precision_m: 0.8803 - precision: 0.8803 - recall_6: 0.8643 - f1score: 0.8722 - val_loss: 0.5378 - val_acc: 0.8108 - val_prec: 0.1759 - val_recall: 0.1696 - val_prec_1: 0.1747 - val_recall_1: 0.1416 - val_prec_2: 0.1871 - val_recall_2: 0.1806 - val_prec_3: 0.1279 - val_recall_3: 0.0577 - val_prec_4: 0.1809 - val_recall_4: 0.1654 - val_prec_5: 0.1684 - val_recall_5: 0.1635 - val_f1_m: 0.8109 - val_recall_m: 0.8033 - val_precision_m: 0.8189 - val_precision: 0.8189 - val_recall_6: 0.8033 - val_f1score: 0.8109\n",
      "Epoch 184/300\n",
      "epoch:  183\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2784 - acc: 0.8756 - prec: 0.9656 - recall: 0.9771 - prec_1: 0.9322 - recall_1: 0.9473 - prec_2: 0.9706 - recall_2: 0.9558 - prec_3: 0.7086 - recall_3: 0.7119 - prec_4: 0.7059 - recall_4: 0.7079 - prec_5: 0.9690 - recall_5: 0.9734 - f1_m: 0.8747 - recall_m: 0.8671 - precision_m: 0.8826 - precision: 0.8826 - recall_6: 0.8671 - f1score: 0.8747 - val_loss: 0.3912 - val_acc: 0.8191 - val_prec: 0.1759 - val_recall: 0.1701 - val_prec_1: 0.1751 - val_recall_1: 0.1570 - val_prec_2: 0.1899 - val_recall_2: 0.1704 - val_prec_3: 0.1759 - val_recall_3: 0.0979 - val_prec_4: 0.1664 - val_recall_4: 0.1129 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8156 - val_recall_m: 0.8083 - val_precision_m: 0.8232 - val_precision: 0.8232 - val_recall_6: 0.8083 - val_f1score: 0.8156\n",
      "Epoch 185/300\n",
      "epoch:  184\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 675us/step - loss: 0.2794 - acc: 0.8725 - prec: 0.9669 - recall: 0.9796 - prec_1: 0.9448 - recall_1: 0.9523 - prec_2: 0.9695 - recall_2: 0.9615 - prec_3: 0.6969 - recall_3: 0.6728 - prec_4: 0.6836 - recall_4: 0.7213 - prec_5: 0.9665 - recall_5: 0.9725 - f1_m: 0.8715 - recall_m: 0.8635 - precision_m: 0.8798 - precision: 0.8798 - recall_6: 0.8635 - f1score: 0.8715 - val_loss: 0.3608 - val_acc: 0.8328 - val_prec: 0.1759 - val_recall: 0.1678 - val_prec_1: 0.1747 - val_recall_1: 0.1375 - val_prec_2: 0.1887 - val_recall_2: 0.1832 - val_prec_3: 0.1599 - val_recall_3: 0.0786 - val_prec_4: 0.1810 - val_recall_4: 0.1724 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8316 - val_recall_m: 0.8233 - val_precision_m: 0.8405 - val_precision: 0.8405 - val_recall_6: 0.8233 - val_f1score: 0.8316\n",
      "Epoch 186/300\n",
      "epoch:  185\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2854 - acc: 0.8692 - prec: 0.9715 - recall: 0.9768 - prec_1: 0.9240 - recall_1: 0.9491 - prec_2: 0.9674 - recall_2: 0.9532 - prec_3: 0.6855 - recall_3: 0.6898 - prec_4: 0.6851 - recall_4: 0.6998 - prec_5: 0.9641 - recall_5: 0.9673 - f1_m: 0.8686 - recall_m: 0.8611 - precision_m: 0.8763 - precision: 0.8763 - recall_6: 0.8611 - f1score: 0.8686 - val_loss: 0.3524 - val_acc: 0.8298 - val_prec: 0.1759 - val_recall: 0.1691 - val_prec_1: 0.1749 - val_recall_1: 0.1603 - val_prec_2: 0.1901 - val_recall_2: 0.1795 - val_prec_3: 0.1740 - val_recall_3: 0.1419 - val_prec_4: 0.1919 - val_recall_4: 0.0922 - val_prec_5: 0.1684 - val_recall_5: 0.1597 - val_f1_m: 0.8278 - val_recall_m: 0.8203 - val_precision_m: 0.8357 - val_precision: 0.8357 - val_recall_6: 0.8203 - val_f1score: 0.8278\n",
      "Epoch 187/300\n",
      "epoch:  186\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2814 - acc: 0.8717 - prec: 0.9692 - recall: 0.9810 - prec_1: 0.9386 - recall_1: 0.9569 - prec_2: 0.9738 - recall_2: 0.9604 - prec_3: 0.6945 - recall_3: 0.6806 - prec_4: 0.6785 - recall_4: 0.7075 - prec_5: 0.9729 - recall_5: 0.9695 - f1_m: 0.8720 - recall_m: 0.8648 - precision_m: 0.8793 - precision: 0.8793 - recall_6: 0.8648 - f1score: 0.8720 - val_loss: 0.3793 - val_acc: 0.8361 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1604 - val_prec_2: 0.1899 - val_recall_2: 0.1690 - val_prec_3: 0.1693 - val_recall_3: 0.1064 - val_prec_4: 0.1780 - val_recall_4: 0.1286 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8347 - val_recall_m: 0.8268 - val_precision_m: 0.8430 - val_precision: 0.8430 - val_recall_6: 0.8268 - val_f1score: 0.8347\n",
      "Epoch 188/300\n",
      "epoch:  187\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2795 - acc: 0.8767 - prec: 0.9656 - recall: 0.9819 - prec_1: 0.9405 - recall_1: 0.9519 - prec_2: 0.9728 - recall_2: 0.9610 - prec_3: 0.7063 - recall_3: 0.7146 - prec_4: 0.7092 - recall_4: 0.7083 - prec_5: 0.9636 - recall_5: 0.9697 - f1_m: 0.8740 - recall_m: 0.8658 - precision_m: 0.8825 - precision: 0.8825 - recall_6: 0.8658 - f1score: 0.8740 - val_loss: 0.5409 - val_acc: 0.8061 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1536 - val_prec_2: 0.1887 - val_recall_2: 0.1813 - val_prec_3: 0.1439 - val_recall_3: 0.0682 - val_prec_4: 0.1759 - val_recall_4: 0.1307 - val_prec_5: 0.1684 - val_recall_5: 0.1594 - val_f1_m: 0.8055 - val_recall_m: 0.7964 - val_precision_m: 0.8154 - val_precision: 0.8154 - val_recall_6: 0.7964 - val_f1score: 0.8055\n",
      "Epoch 189/300\n",
      "epoch:  188\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2762 - acc: 0.8761 - prec: 0.9711 - recall: 0.9813 - prec_1: 0.9310 - recall_1: 0.9548 - prec_2: 0.9717 - recall_2: 0.9567 - prec_3: 0.7106 - recall_3: 0.6933 - prec_4: 0.6958 - recall_4: 0.7218 - prec_5: 0.9667 - recall_5: 0.9697 - f1_m: 0.8755 - recall_m: 0.8678 - precision_m: 0.8834 - precision: 0.8834 - recall_6: 0.8678 - f1score: 0.8755 - val_loss: 0.4234 - val_acc: 0.8268 - val_prec: 0.1759 - val_recall: 0.1706 - val_prec_1: 0.1750 - val_recall_1: 0.1493 - val_prec_2: 0.1896 - val_recall_2: 0.1686 - val_prec_3: 0.1439 - val_recall_3: 0.0852 - val_prec_4: 0.1807 - val_recall_4: 0.1539 - val_prec_5: 0.1684 - val_recall_5: 0.1648 - val_f1_m: 0.8236 - val_recall_m: 0.8163 - val_precision_m: 0.8313 - val_precision: 0.8313 - val_recall_6: 0.8163 - val_f1score: 0.8236\n",
      "Epoch 190/300\n",
      "epoch:  189\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.2763 - acc: 0.8780 - prec: 0.9671 - recall: 0.9753 - prec_1: 0.9334 - recall_1: 0.9526 - prec_2: 0.9746 - recall_2: 0.9615 - prec_3: 0.7157 - recall_3: 0.7257 - prec_4: 0.7100 - recall_4: 0.7126 - prec_5: 0.9638 - recall_5: 0.9687 - f1_m: 0.8782 - recall_m: 0.8705 - precision_m: 0.8861 - precision: 0.8861 - recall_6: 0.8705 - f1score: 0.8782 - val_loss: 0.3668 - val_acc: 0.8451 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1749 - val_recall_1: 0.1671 - val_prec_2: 0.1919 - val_recall_2: 0.1705 - val_prec_3: 0.1599 - val_recall_3: 0.0965 - val_prec_4: 0.1809 - val_recall_4: 0.1423 - val_prec_5: 0.1684 - val_recall_5: 0.1579 - val_f1_m: 0.8418 - val_recall_m: 0.8336 - val_precision_m: 0.8503 - val_precision: 0.8503 - val_recall_6: 0.8336 - val_f1score: 0.8418\n",
      "Epoch 191/300\n",
      "epoch:  190\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2797 - acc: 0.8751 - prec: 0.9678 - recall: 0.9789 - prec_1: 0.9426 - recall_1: 0.9476 - prec_2: 0.9676 - recall_2: 0.9641 - prec_3: 0.7115 - recall_3: 0.7072 - prec_4: 0.7045 - recall_4: 0.7207 - prec_5: 0.9652 - recall_5: 0.9705 - f1_m: 0.8745 - recall_m: 0.8660 - precision_m: 0.8834 - precision: 0.8834 - recall_6: 0.8660 - f1score: 0.8745 - val_loss: 0.5349 - val_acc: 0.8138 - val_prec: 0.1759 - val_recall: 0.1686 - val_prec_1: 0.1748 - val_recall_1: 0.1474 - val_prec_2: 0.1875 - val_recall_2: 0.1830 - val_prec_3: 0.1119 - val_recall_3: 0.0340 - val_prec_4: 0.1809 - val_recall_4: 0.1854 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8136 - val_recall_m: 0.8056 - val_precision_m: 0.8221 - val_precision: 0.8221 - val_recall_6: 0.8056 - val_f1score: 0.8136\n",
      "Epoch 192/300\n",
      "epoch:  191\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2787 - acc: 0.8770 - prec: 0.9680 - recall: 0.9772 - prec_1: 0.9324 - recall_1: 0.9498 - prec_2: 0.9632 - recall_2: 0.9593 - prec_3: 0.7166 - recall_3: 0.7192 - prec_4: 0.7127 - recall_4: 0.7186 - prec_5: 0.9674 - recall_5: 0.9666 - f1_m: 0.8766 - recall_m: 0.8687 - precision_m: 0.8847 - precision: 0.8847 - recall_6: 0.8687 - f1score: 0.8766 - val_loss: 0.3242 - val_acc: 0.8458 - val_prec: 0.1759 - val_recall: 0.1698 - val_prec_1: 0.1745 - val_recall_1: 0.1561 - val_prec_2: 0.1892 - val_recall_2: 0.1645 - val_prec_3: 0.1752 - val_recall_3: 0.1141 - val_prec_4: 0.1831 - val_recall_4: 0.1449 - val_prec_5: 0.1684 - val_recall_5: 0.1632 - val_f1_m: 0.8439 - val_recall_m: 0.8373 - val_precision_m: 0.8507 - val_precision: 0.8507 - val_recall_6: 0.8373 - val_f1score: 0.8439\n",
      "Epoch 193/300\n",
      "epoch:  192\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2804 - acc: 0.8782 - prec: 0.9714 - recall: 0.9771 - prec_1: 0.9406 - recall_1: 0.9490 - prec_2: 0.9718 - recall_2: 0.9615 - prec_3: 0.7074 - recall_3: 0.7290 - prec_4: 0.7224 - recall_4: 0.7109 - prec_5: 0.9632 - recall_5: 0.9737 - f1_m: 0.8779 - recall_m: 0.8693 - precision_m: 0.8869 - precision: 0.8869 - recall_6: 0.8693 - f1score: 0.8779 - val_loss: 0.5504 - val_acc: 0.7911 - val_prec: 0.1759 - val_recall: 0.1688 - val_prec_1: 0.1750 - val_recall_1: 0.1482 - val_prec_2: 0.1901 - val_recall_2: 0.1810 - val_prec_3: 0.0800 - val_recall_3: 0.0112 - val_prec_4: 0.1809 - val_recall_4: 0.1854 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.7917 - val_recall_m: 0.7846 - val_precision_m: 0.7992 - val_precision: 0.7992 - val_recall_6: 0.7846 - val_f1score: 0.7917\n",
      "Epoch 194/300\n",
      "epoch:  193\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2789 - acc: 0.8770 - prec: 0.9647 - recall: 0.9790 - prec_1: 0.9378 - recall_1: 0.9536 - prec_2: 0.9758 - recall_2: 0.9595 - prec_3: 0.7112 - recall_3: 0.7026 - prec_4: 0.7059 - recall_4: 0.7277 - prec_5: 0.9687 - recall_5: 0.9695 - f1_m: 0.8752 - recall_m: 0.8665 - precision_m: 0.8843 - precision: 0.8843 - recall_6: 0.8665 - f1score: 0.8752 - val_loss: 0.5840 - val_acc: 0.7911 - val_prec: 0.1759 - val_recall: 0.1736 - val_prec_1: 0.1743 - val_recall_1: 0.1676 - val_prec_2: 0.1892 - val_recall_2: 0.1598 - val_prec_3: 0.1439 - val_recall_3: 0.0722 - val_prec_4: 0.1759 - val_recall_4: 0.1322 - val_prec_5: 0.1684 - val_recall_5: 0.1400 - val_f1_m: 0.7913 - val_recall_m: 0.7849 - val_precision_m: 0.7981 - val_precision: 0.7981 - val_recall_6: 0.7849 - val_f1score: 0.7913\n",
      "Epoch 195/300\n",
      "epoch:  194\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2786 - acc: 0.8783 - prec: 0.9664 - recall: 0.9760 - prec_1: 0.9401 - recall_1: 0.9608 - prec_2: 0.9774 - recall_2: 0.9675 - prec_3: 0.7087 - recall_3: 0.6986 - prec_4: 0.7011 - recall_4: 0.7192 - prec_5: 0.9712 - recall_5: 0.9663 - f1_m: 0.8777 - recall_m: 0.8710 - precision_m: 0.8847 - precision: 0.8847 - recall_6: 0.8710 - f1score: 0.8777 - val_loss: 0.3404 - val_acc: 0.8413 - val_prec: 0.1759 - val_recall: 0.1728 - val_prec_1: 0.1749 - val_recall_1: 0.1598 - val_prec_2: 0.1884 - val_recall_2: 0.1722 - val_prec_3: 0.1620 - val_recall_3: 0.1135 - val_prec_4: 0.1759 - val_recall_4: 0.1151 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.8405 - val_recall_m: 0.8338 - val_precision_m: 0.8474 - val_precision: 0.8474 - val_recall_6: 0.8338 - val_f1score: 0.8405\n",
      "Epoch 196/300\n",
      "epoch:  195\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.2754 - acc: 0.8796 - prec: 0.9677 - recall: 0.9811 - prec_1: 0.9398 - recall_1: 0.9497 - prec_2: 0.9682 - recall_2: 0.9618 - prec_3: 0.7156 - recall_3: 0.7090 - prec_4: 0.7083 - recall_4: 0.7244 - prec_5: 0.9684 - recall_5: 0.9724 - f1_m: 0.8794 - recall_m: 0.8716 - precision_m: 0.8874 - precision: 0.8874 - recall_6: 0.8716 - f1score: 0.8794 - val_loss: 0.4230 - val_acc: 0.8251 - val_prec: 0.1759 - val_recall: 0.1688 - val_prec_1: 0.1748 - val_recall_1: 0.1571 - val_prec_2: 0.1884 - val_recall_2: 0.1732 - val_prec_3: 0.1119 - val_recall_3: 0.0505 - val_prec_4: 0.1809 - val_recall_4: 0.1804 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8238 - val_recall_m: 0.8178 - val_precision_m: 0.8300 - val_precision: 0.8300 - val_recall_6: 0.8178 - val_f1score: 0.8238\n",
      "Epoch 197/300\n",
      "epoch:  196\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2799 - acc: 0.8763 - prec: 0.9671 - recall: 0.9774 - prec_1: 0.9463 - recall_1: 0.9537 - prec_2: 0.9728 - recall_2: 0.9634 - prec_3: 0.6997 - recall_3: 0.7107 - prec_4: 0.7036 - recall_4: 0.7061 - prec_5: 0.9691 - recall_5: 0.9708 - f1_m: 0.8746 - recall_m: 0.8670 - precision_m: 0.8826 - precision: 0.8826 - recall_6: 0.8670 - f1score: 0.8746 - val_loss: 0.4238 - val_acc: 0.8241 - val_prec: 0.1759 - val_recall: 0.1736 - val_prec_1: 0.1743 - val_recall_1: 0.1656 - val_prec_2: 0.1887 - val_recall_2: 0.1577 - val_prec_3: 0.1759 - val_recall_3: 0.1192 - val_prec_4: 0.1662 - val_recall_4: 0.1022 - val_prec_5: 0.1684 - val_recall_5: 0.1576 - val_f1_m: 0.8218 - val_recall_m: 0.8146 - val_precision_m: 0.8293 - val_precision: 0.8293 - val_recall_6: 0.8146 - val_f1score: 0.8218\n",
      "Epoch 198/300\n",
      "epoch:  197\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2794 - acc: 0.8752 - prec: 0.9699 - recall: 0.9828 - prec_1: 0.9374 - recall_1: 0.9493 - prec_2: 0.9701 - recall_2: 0.9567 - prec_3: 0.7077 - recall_3: 0.7043 - prec_4: 0.6988 - recall_4: 0.7081 - prec_5: 0.9628 - recall_5: 0.9682 - f1_m: 0.8758 - recall_m: 0.8681 - precision_m: 0.8837 - precision: 0.8837 - recall_6: 0.8681 - f1score: 0.8758 - val_loss: 0.4520 - val_acc: 0.8146 - val_prec: 0.1759 - val_recall: 0.1708 - val_prec_1: 0.1748 - val_recall_1: 0.1599 - val_prec_2: 0.1884 - val_recall_2: 0.1715 - val_prec_3: 0.1279 - val_recall_3: 0.0655 - val_prec_4: 0.1807 - val_recall_4: 0.1519 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8142 - val_recall_m: 0.8078 - val_precision_m: 0.8207 - val_precision: 0.8207 - val_recall_6: 0.8078 - val_f1score: 0.8142\n",
      "Epoch 199/300\n",
      "epoch:  198\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2749 - acc: 0.8791 - prec: 0.9700 - recall: 0.9763 - prec_1: 0.9364 - recall_1: 0.9554 - prec_2: 0.9651 - recall_2: 0.9553 - prec_3: 0.7209 - recall_3: 0.7069 - prec_4: 0.7102 - recall_4: 0.7303 - prec_5: 0.9686 - recall_5: 0.9682 - f1_m: 0.8789 - recall_m: 0.8711 - precision_m: 0.8870 - precision: 0.8870 - recall_6: 0.8711 - f1score: 0.8789 - val_loss: 1.0085 - val_acc: 0.7936 - val_prec: 0.1759 - val_recall: 0.1701 - val_prec_1: 0.1748 - val_recall_1: 0.1474 - val_prec_2: 0.1884 - val_recall_2: 0.1787 - val_prec_3: 0.1279 - val_recall_3: 0.0532 - val_prec_4: 0.1763 - val_recall_4: 0.1397 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.7918 - val_recall_m: 0.7834 - val_precision_m: 0.8008 - val_precision: 0.8008 - val_recall_6: 0.7834 - val_f1score: 0.7918\n",
      "Epoch 200/300\n",
      "epoch:  199\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2774 - acc: 0.8766 - prec: 0.9714 - recall: 0.9794 - prec_1: 0.9430 - recall_1: 0.9528 - prec_2: 0.9684 - recall_2: 0.9689 - prec_3: 0.7099 - recall_3: 0.6978 - prec_4: 0.7011 - recall_4: 0.7235 - prec_5: 0.9664 - recall_5: 0.9701 - f1_m: 0.8757 - recall_m: 0.8682 - precision_m: 0.8835 - precision: 0.8835 - recall_6: 0.8682 - f1score: 0.8757 - val_loss: 0.3070 - val_acc: 0.8673 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1608 - val_prec_2: 0.1901 - val_recall_2: 0.1778 - val_prec_3: 0.1756 - val_recall_3: 0.1592 - val_prec_4: 0.1919 - val_recall_4: 0.1025 - val_prec_5: 0.1684 - val_recall_5: 0.1597 - val_f1_m: 0.8671 - val_recall_m: 0.8601 - val_precision_m: 0.8743 - val_precision: 0.8743 - val_recall_6: 0.8601 - val_f1score: 0.8671\n",
      "Epoch 201/300\n",
      "epoch:  200\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2766 - acc: 0.8787 - prec: 0.9707 - recall: 0.9748 - prec_1: 0.9427 - recall_1: 0.9565 - prec_2: 0.9706 - recall_2: 0.9629 - prec_3: 0.7042 - recall_3: 0.7128 - prec_4: 0.7120 - recall_4: 0.7076 - prec_5: 0.9661 - recall_5: 0.9741 - f1_m: 0.8772 - recall_m: 0.8685 - precision_m: 0.8862 - precision: 0.8862 - recall_6: 0.8685 - f1score: 0.8772 - val_loss: 0.5996 - val_acc: 0.8196 - val_prec: 0.1759 - val_recall: 0.1694 - val_prec_1: 0.1753 - val_recall_1: 0.1586 - val_prec_2: 0.1899 - val_recall_2: 0.1742 - val_prec_3: 0.1119 - val_recall_3: 0.0305 - val_prec_4: 0.1809 - val_recall_4: 0.1912 - val_prec_5: 0.1684 - val_recall_5: 0.1612 - val_f1_m: 0.8144 - val_recall_m: 0.8068 - val_precision_m: 0.8224 - val_precision: 0.8224 - val_recall_6: 0.8068 - val_f1score: 0.8144\n",
      "Epoch 202/300\n",
      "epoch:  201\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 676us/step - loss: 0.2781 - acc: 0.8753 - prec: 0.9682 - recall: 0.9807 - prec_1: 0.9339 - recall_1: 0.9535 - prec_2: 0.9728 - recall_2: 0.9593 - prec_3: 0.7126 - recall_3: 0.6836 - prec_4: 0.6923 - recall_4: 0.7345 - prec_5: 0.9644 - recall_5: 0.9700 - f1_m: 0.8740 - recall_m: 0.8663 - precision_m: 0.8819 - precision: 0.8819 - recall_6: 0.8663 - f1score: 0.8740 - val_loss: 0.3836 - val_acc: 0.8511 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1749 - val_recall_1: 0.1590 - val_prec_2: 0.1901 - val_recall_2: 0.1780 - val_prec_3: 0.1599 - val_recall_3: 0.0912 - val_prec_4: 0.1807 - val_recall_4: 0.1520 - val_prec_5: 0.1684 - val_recall_5: 0.1612 - val_f1_m: 0.8489 - val_recall_m: 0.8408 - val_precision_m: 0.8574 - val_precision: 0.8574 - val_recall_6: 0.8408 - val_f1score: 0.8489\n",
      "Epoch 203/300\n",
      "epoch:  202\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2706 - acc: 0.8780 - prec: 0.9707 - recall: 0.9806 - prec_1: 0.9418 - recall_1: 0.9524 - prec_2: 0.9728 - recall_2: 0.9580 - prec_3: 0.7109 - recall_3: 0.7207 - prec_4: 0.7152 - recall_4: 0.7138 - prec_5: 0.9671 - recall_5: 0.9702 - f1_m: 0.8792 - recall_m: 0.8715 - precision_m: 0.8871 - precision: 0.8871 - recall_6: 0.8715 - f1score: 0.8792 - val_loss: 0.3965 - val_acc: 0.8298 - val_prec: 0.1759 - val_recall: 0.1686 - val_prec_1: 0.1748 - val_recall_1: 0.1602 - val_prec_2: 0.1901 - val_recall_2: 0.1775 - val_prec_3: 0.1439 - val_recall_3: 0.0520 - val_prec_4: 0.1817 - val_recall_4: 0.1794 - val_prec_5: 0.1684 - val_recall_5: 0.1597 - val_f1_m: 0.8295 - val_recall_m: 0.8221 - val_precision_m: 0.8373 - val_precision: 0.8373 - val_recall_6: 0.8221 - val_f1score: 0.8295\n",
      "Epoch 204/300\n",
      "epoch:  203\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2730 - acc: 0.8771 - prec: 0.9681 - recall: 0.9769 - prec_1: 0.9399 - recall_1: 0.9591 - prec_2: 0.9737 - recall_2: 0.9634 - prec_3: 0.7070 - recall_3: 0.7147 - prec_4: 0.7003 - recall_4: 0.7068 - prec_5: 0.9675 - recall_5: 0.9702 - f1_m: 0.8770 - recall_m: 0.8703 - precision_m: 0.8839 - precision: 0.8839 - recall_6: 0.8703 - f1score: 0.8770 - val_loss: 0.3979 - val_acc: 0.8391 - val_prec: 0.1759 - val_recall: 0.1731 - val_prec_1: 0.1749 - val_recall_1: 0.1621 - val_prec_2: 0.1887 - val_recall_2: 0.1751 - val_prec_3: 0.1709 - val_recall_3: 0.1334 - val_prec_4: 0.1759 - val_recall_4: 0.1037 - val_prec_5: 0.1684 - val_recall_5: 0.1556 - val_f1_m: 0.8377 - val_recall_m: 0.8293 - val_precision_m: 0.8464 - val_precision: 0.8464 - val_recall_6: 0.8293 - val_f1score: 0.8377\n",
      "Epoch 205/300\n",
      "epoch:  204\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2707 - acc: 0.8820 - prec: 0.9671 - recall: 0.9800 - prec_1: 0.9377 - recall_1: 0.9550 - prec_2: 0.9750 - recall_2: 0.9625 - prec_3: 0.7325 - recall_3: 0.7010 - prec_4: 0.7067 - recall_4: 0.7431 - prec_5: 0.9697 - recall_5: 0.9682 - f1_m: 0.8807 - recall_m: 0.8738 - precision_m: 0.8878 - precision: 0.8878 - recall_6: 0.8738 - f1score: 0.8807 - val_loss: 0.3128 - val_acc: 0.8538 - val_prec: 0.1759 - val_recall: 0.1673 - val_prec_1: 0.1748 - val_recall_1: 0.1551 - val_prec_2: 0.1899 - val_recall_2: 0.1764 - val_prec_3: 0.1756 - val_recall_3: 0.1469 - val_prec_4: 0.1919 - val_recall_4: 0.1161 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8532 - val_recall_m: 0.8461 - val_precision_m: 0.8607 - val_precision: 0.8607 - val_recall_6: 0.8461 - val_f1score: 0.8532\n",
      "Epoch 206/300\n",
      "epoch:  205\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2737 - acc: 0.8772 - prec: 0.9706 - recall: 0.9785 - prec_1: 0.9492 - recall_1: 0.9518 - prec_2: 0.9654 - recall_2: 0.9650 - prec_3: 0.7071 - recall_3: 0.6952 - prec_4: 0.7027 - recall_4: 0.7185 - prec_5: 0.9600 - recall_5: 0.9743 - f1_m: 0.8760 - recall_m: 0.8685 - precision_m: 0.8838 - precision: 0.8838 - recall_6: 0.8685 - f1score: 0.8760 - val_loss: 0.4505 - val_acc: 0.7981 - val_prec: 0.1759 - val_recall: 0.1693 - val_prec_1: 0.1749 - val_recall_1: 0.1624 - val_prec_2: 0.1899 - val_recall_2: 0.1739 - val_prec_3: 0.0640 - val_recall_3: 0.0045 - val_prec_4: 0.1809 - val_recall_4: 0.1917 - val_prec_5: 0.1684 - val_recall_5: 0.1612 - val_f1_m: 0.7986 - val_recall_m: 0.7939 - val_precision_m: 0.8036 - val_precision: 0.8036 - val_recall_6: 0.7939 - val_f1score: 0.7986\n",
      "Epoch 207/300\n",
      "epoch:  206\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2788 - acc: 0.8785 - prec: 0.9726 - recall: 0.9768 - prec_1: 0.9385 - recall_1: 0.9597 - prec_2: 0.9721 - recall_2: 0.9678 - prec_3: 0.7184 - recall_3: 0.6904 - prec_4: 0.7021 - recall_4: 0.7449 - prec_5: 0.9708 - recall_5: 0.9651 - f1_m: 0.8773 - recall_m: 0.8693 - precision_m: 0.8856 - precision: 0.8856 - recall_6: 0.8693 - f1score: 0.8773 - val_loss: 0.3113 - val_acc: 0.8601 - val_prec: 0.1759 - val_recall: 0.1706 - val_prec_1: 0.1751 - val_recall_1: 0.1511 - val_prec_2: 0.1899 - val_recall_2: 0.1702 - val_prec_3: 0.1755 - val_recall_3: 0.1559 - val_prec_4: 0.1904 - val_recall_4: 0.1133 - val_prec_5: 0.1684 - val_recall_5: 0.1645 - val_f1_m: 0.8578 - val_recall_m: 0.8493 - val_precision_m: 0.8667 - val_precision: 0.8667 - val_recall_6: 0.8493 - val_f1score: 0.8578\n",
      "Epoch 208/300\n",
      "epoch:  207\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2796 - acc: 0.8757 - prec: 0.9694 - recall: 0.9796 - prec_1: 0.9395 - recall_1: 0.9512 - prec_2: 0.9649 - recall_2: 0.9594 - prec_3: 0.7036 - recall_3: 0.7037 - prec_4: 0.7022 - recall_4: 0.7152 - prec_5: 0.9647 - recall_5: 0.9659 - f1_m: 0.8752 - recall_m: 0.8676 - precision_m: 0.8831 - precision: 0.8831 - recall_6: 0.8676 - f1score: 0.8752 - val_loss: 0.6485 - val_acc: 0.8086 - val_prec: 0.1759 - val_recall: 0.1678 - val_prec_1: 0.1748 - val_recall_1: 0.1489 - val_prec_2: 0.1899 - val_recall_2: 0.1769 - val_prec_3: 0.1279 - val_recall_3: 0.0419 - val_prec_4: 0.1809 - val_recall_4: 0.1782 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8070 - val_recall_m: 0.8003 - val_precision_m: 0.8140 - val_precision: 0.8140 - val_recall_6: 0.8003 - val_f1score: 0.8070\n",
      "Epoch 209/300\n",
      "epoch:  208\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2748 - acc: 0.8758 - prec: 0.9636 - recall: 0.9756 - prec_1: 0.9373 - recall_1: 0.9605 - prec_2: 0.9686 - recall_2: 0.9601 - prec_3: 0.7151 - recall_3: 0.6931 - prec_4: 0.6978 - recall_4: 0.7344 - prec_5: 0.9710 - recall_5: 0.9667 - f1_m: 0.8761 - recall_m: 0.8683 - precision_m: 0.8842 - precision: 0.8842 - recall_6: 0.8683 - f1score: 0.8761 - val_loss: 0.3208 - val_acc: 0.8506 - val_prec: 0.1759 - val_recall: 0.1721 - val_prec_1: 0.1749 - val_recall_1: 0.1639 - val_prec_2: 0.1887 - val_recall_2: 0.1760 - val_prec_3: 0.1759 - val_recall_3: 0.1015 - val_prec_4: 0.1813 - val_recall_4: 0.1469 - val_prec_5: 0.1684 - val_recall_5: 0.1572 - val_f1_m: 0.8484 - val_recall_m: 0.8416 - val_precision_m: 0.8556 - val_precision: 0.8556 - val_recall_6: 0.8416 - val_f1score: 0.8484\n",
      "Epoch 210/300\n",
      "epoch:  209\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 680us/step - loss: 0.2743 - acc: 0.8825 - prec: 0.9706 - recall: 0.9805 - prec_1: 0.9437 - recall_1: 0.9524 - prec_2: 0.9736 - recall_2: 0.9658 - prec_3: 0.7201 - recall_3: 0.7265 - prec_4: 0.7306 - recall_4: 0.7280 - prec_5: 0.9615 - recall_5: 0.9696 - f1_m: 0.8813 - recall_m: 0.8735 - precision_m: 0.8895 - precision: 0.8895 - recall_6: 0.8735 - f1score: 0.8813 - val_loss: 0.7638 - val_acc: 0.7954 - val_prec: 0.1759 - val_recall: 0.1687 - val_prec_1: 0.1753 - val_recall_1: 0.1627 - val_prec_2: 0.1901 - val_recall_2: 0.1767 - val_prec_3: 0.0160 - val_recall_3: 0.0032 - val_prec_4: 0.1809 - val_recall_4: 0.1919 - val_prec_5: 0.1684 - val_recall_5: 0.1594 - val_f1_m: 0.7952 - val_recall_m: 0.7881 - val_precision_m: 0.8027 - val_precision: 0.8027 - val_recall_6: 0.7881 - val_f1score: 0.7952\n",
      "Epoch 211/300\n",
      "epoch:  210\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2806 - acc: 0.8733 - prec: 0.9664 - recall: 0.9797 - prec_1: 0.9262 - recall_1: 0.9533 - prec_2: 0.9715 - recall_2: 0.9621 - prec_3: 0.7083 - recall_3: 0.7116 - prec_4: 0.7030 - recall_4: 0.7156 - prec_5: 0.9650 - recall_5: 0.9647 - f1_m: 0.8728 - recall_m: 0.8650 - precision_m: 0.8810 - precision: 0.8810 - recall_6: 0.8650 - f1score: 0.8728 - val_loss: 0.3732 - val_acc: 0.8258 - val_prec: 0.1759 - val_recall: 0.1703 - val_prec_1: 0.1749 - val_recall_1: 0.1567 - val_prec_2: 0.1899 - val_recall_2: 0.1771 - val_prec_3: 0.1756 - val_recall_3: 0.1714 - val_prec_4: 0.1759 - val_recall_4: 0.0502 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8236 - val_recall_m: 0.8166 - val_precision_m: 0.8310 - val_precision: 0.8310 - val_recall_6: 0.8166 - val_f1score: 0.8236\n",
      "Epoch 212/300\n",
      "epoch:  211\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2791 - acc: 0.8773 - prec: 0.9628 - recall: 0.9759 - prec_1: 0.9454 - recall_1: 0.9510 - prec_2: 0.9688 - recall_2: 0.9641 - prec_3: 0.6982 - recall_3: 0.7062 - prec_4: 0.6984 - recall_4: 0.7027 - prec_5: 0.9732 - recall_5: 0.9740 - f1_m: 0.8758 - recall_m: 0.8690 - precision_m: 0.8829 - precision: 0.8829 - recall_6: 0.8690 - f1score: 0.8758 - val_loss: 0.6248 - val_acc: 0.8233 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1749 - val_recall_1: 0.1666 - val_prec_2: 0.1901 - val_recall_2: 0.1760 - val_prec_3: 0.1439 - val_recall_3: 0.0797 - val_prec_4: 0.1759 - val_recall_4: 0.1359 - val_prec_5: 0.1684 - val_recall_5: 0.1539 - val_f1_m: 0.8204 - val_recall_m: 0.8121 - val_precision_m: 0.8293 - val_precision: 0.8293 - val_recall_6: 0.8121 - val_f1score: 0.8204\n",
      "Epoch 213/300\n",
      "epoch:  212\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2765 - acc: 0.8766 - prec: 0.9682 - recall: 0.9803 - prec_1: 0.9305 - recall_1: 0.9522 - prec_2: 0.9674 - recall_2: 0.9602 - prec_3: 0.7124 - recall_3: 0.7071 - prec_4: 0.7069 - recall_4: 0.7203 - prec_5: 0.9641 - recall_5: 0.9697 - f1_m: 0.8761 - recall_m: 0.8686 - precision_m: 0.8838 - precision: 0.8838 - recall_6: 0.8686 - f1score: 0.8761 - val_loss: 1.4065 - val_acc: 0.7849 - val_prec: 0.1759 - val_recall: 0.1679 - val_prec_1: 0.1753 - val_recall_1: 0.1492 - val_prec_2: 0.1901 - val_recall_2: 0.1805 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1809 - val_recall_4: 0.1919 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.7846 - val_recall_m: 0.7756 - val_precision_m: 0.7944 - val_precision: 0.7944 - val_recall_6: 0.7756 - val_f1score: 0.7846\n",
      "Epoch 214/300\n",
      "epoch:  213\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2759 - acc: 0.8771 - prec: 0.9712 - recall: 0.9776 - prec_1: 0.9414 - recall_1: 0.9534 - prec_2: 0.9701 - recall_2: 0.9626 - prec_3: 0.7140 - recall_3: 0.7095 - prec_4: 0.6980 - recall_4: 0.7108 - prec_5: 0.9609 - recall_5: 0.9700 - f1_m: 0.8750 - recall_m: 0.8667 - precision_m: 0.8836 - precision: 0.8836 - recall_6: 0.8667 - f1score: 0.8750 - val_loss: 0.7592 - val_acc: 0.7966 - val_prec: 0.1759 - val_recall: 0.1736 - val_prec_1: 0.1741 - val_recall_1: 0.1671 - val_prec_2: 0.1887 - val_recall_2: 0.1509 - val_prec_3: 0.1439 - val_recall_3: 0.0814 - val_prec_4: 0.1759 - val_recall_4: 0.1319 - val_prec_5: 0.1684 - val_recall_5: 0.1453 - val_f1_m: 0.7965 - val_recall_m: 0.7906 - val_precision_m: 0.8027 - val_precision: 0.8027 - val_recall_6: 0.7906 - val_f1score: 0.7965\n",
      "Epoch 215/300\n",
      "epoch:  214\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2738 - acc: 0.8793 - prec: 0.9751 - recall: 0.9804 - prec_1: 0.9438 - recall_1: 0.9554 - prec_2: 0.9733 - recall_2: 0.9639 - prec_3: 0.7061 - recall_3: 0.6990 - prec_4: 0.6984 - recall_4: 0.7144 - prec_5: 0.9711 - recall_5: 0.9759 - f1_m: 0.8779 - recall_m: 0.8700 - precision_m: 0.8861 - precision: 0.8861 - recall_6: 0.8700 - f1score: 0.8779 - val_loss: 0.5611 - val_acc: 0.7484 - val_prec: 0.1759 - val_recall: 0.1698 - val_prec_1: 0.1747 - val_recall_1: 0.1446 - val_prec_2: 0.1901 - val_recall_2: 0.1800 - val_prec_3: 0.1695 - val_recall_3: 0.1303 - val_prec_4: 0.0640 - val_recall_4: 0.0087 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.7361 - val_recall_m: 0.7221 - val_precision_m: 0.7542 - val_precision: 0.7542 - val_recall_6: 0.7221 - val_f1score: 0.7361\n",
      "Epoch 216/300\n",
      "epoch:  215\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2771 - acc: 0.8753 - prec: 0.9700 - recall: 0.9792 - prec_1: 0.9372 - recall_1: 0.9523 - prec_2: 0.9756 - recall_2: 0.9574 - prec_3: 0.6953 - recall_3: 0.7076 - prec_4: 0.7031 - recall_4: 0.7038 - prec_5: 0.9644 - recall_5: 0.9750 - f1_m: 0.8755 - recall_m: 0.8675 - precision_m: 0.8839 - precision: 0.8839 - recall_6: 0.8675 - f1score: 0.8755 - val_loss: 0.3753 - val_acc: 0.8303 - val_prec: 0.1759 - val_recall: 0.1728 - val_prec_1: 0.1743 - val_recall_1: 0.1618 - val_prec_2: 0.1892 - val_recall_2: 0.1603 - val_prec_3: 0.1653 - val_recall_3: 0.1059 - val_prec_4: 0.1796 - val_recall_4: 0.1318 - val_prec_5: 0.1684 - val_recall_5: 0.1597 - val_f1_m: 0.8300 - val_recall_m: 0.8246 - val_precision_m: 0.8358 - val_precision: 0.8358 - val_recall_6: 0.8246 - val_f1score: 0.8300\n",
      "Epoch 217/300\n",
      "epoch:  216\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2778 - acc: 0.8768 - prec: 0.9652 - recall: 0.9814 - prec_1: 0.9419 - recall_1: 0.9496 - prec_2: 0.9740 - recall_2: 0.9663 - prec_3: 0.7085 - recall_3: 0.6901 - prec_4: 0.6956 - recall_4: 0.7276 - prec_5: 0.9653 - recall_5: 0.9735 - f1_m: 0.8762 - recall_m: 0.8686 - precision_m: 0.8840 - precision: 0.8840 - recall_6: 0.8686 - f1score: 0.8762 - val_loss: 0.4822 - val_acc: 0.7866 - val_prec: 0.1759 - val_recall: 0.1673 - val_prec_1: 0.1749 - val_recall_1: 0.1292 - val_prec_2: 0.1890 - val_recall_2: 0.1848 - val_prec_3: 0.1279 - val_recall_3: 0.0725 - val_prec_4: 0.1759 - val_recall_4: 0.1299 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.7851 - val_recall_m: 0.7776 - val_precision_m: 0.7931 - val_precision: 0.7931 - val_recall_6: 0.7776 - val_f1score: 0.7851\n",
      "Epoch 218/300\n",
      "epoch:  217\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2810 - acc: 0.8745 - prec: 0.9624 - recall: 0.9778 - prec_1: 0.9369 - recall_1: 0.9617 - prec_2: 0.9774 - recall_2: 0.9653 - prec_3: 0.6991 - recall_3: 0.6973 - prec_4: 0.6917 - recall_4: 0.6996 - prec_5: 0.9677 - recall_5: 0.9682 - f1_m: 0.8728 - recall_m: 0.8646 - precision_m: 0.8812 - precision: 0.8812 - recall_6: 0.8646 - f1score: 0.8728 - val_loss: 0.4884 - val_acc: 0.8251 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1469 - val_prec_2: 0.1875 - val_recall_2: 0.1825 - val_prec_3: 0.1691 - val_recall_3: 0.1440 - val_prec_4: 0.1599 - val_recall_4: 0.0665 - val_prec_5: 0.1684 - val_recall_5: 0.1610 - val_f1_m: 0.8240 - val_recall_m: 0.8151 - val_precision_m: 0.8336 - val_precision: 0.8336 - val_recall_6: 0.8151 - val_f1score: 0.8240\n",
      "Epoch 219/300\n",
      "epoch:  218\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2771 - acc: 0.8745 - prec: 0.9702 - recall: 0.9818 - prec_1: 0.9409 - recall_1: 0.9510 - prec_2: 0.9729 - recall_2: 0.9615 - prec_3: 0.7006 - recall_3: 0.7081 - prec_4: 0.7046 - recall_4: 0.7097 - prec_5: 0.9608 - recall_5: 0.9653 - f1_m: 0.8743 - recall_m: 0.8661 - precision_m: 0.8828 - precision: 0.8828 - recall_6: 0.8661 - f1score: 0.8743 - val_loss: 0.8634 - val_acc: 0.7904 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1738 - val_recall_1: 0.1680 - val_prec_2: 0.1887 - val_recall_2: 0.1492 - val_prec_3: 0.1279 - val_recall_3: 0.0617 - val_prec_4: 0.1763 - val_recall_4: 0.1435 - val_prec_5: 0.1684 - val_recall_5: 0.1491 - val_f1_m: 0.7909 - val_recall_m: 0.7861 - val_precision_m: 0.7959 - val_precision: 0.7959 - val_recall_6: 0.7861 - val_f1score: 0.7909\n",
      "Epoch 220/300\n",
      "epoch:  219\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2762 - acc: 0.8805 - prec: 0.9703 - recall: 0.9804 - prec_1: 0.9409 - recall_1: 0.9524 - prec_2: 0.9707 - recall_2: 0.9631 - prec_3: 0.7126 - recall_3: 0.7238 - prec_4: 0.7157 - recall_4: 0.7106 - prec_5: 0.9651 - recall_5: 0.9700 - f1_m: 0.8794 - recall_m: 0.8716 - precision_m: 0.8876 - precision: 0.8876 - recall_6: 0.8716 - f1score: 0.8794 - val_loss: 0.3919 - val_acc: 0.8316 - val_prec: 0.1759 - val_recall: 0.1731 - val_prec_1: 0.1749 - val_recall_1: 0.1593 - val_prec_2: 0.1901 - val_recall_2: 0.1785 - val_prec_3: 0.1599 - val_recall_3: 0.1019 - val_prec_4: 0.1802 - val_recall_4: 0.1143 - val_prec_5: 0.1684 - val_recall_5: 0.1602 - val_f1_m: 0.8301 - val_recall_m: 0.8228 - val_precision_m: 0.8378 - val_precision: 0.8378 - val_recall_6: 0.8228 - val_f1score: 0.8301\n",
      "Epoch 221/300\n",
      "epoch:  220\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2801 - acc: 0.8732 - prec: 0.9671 - recall: 0.9818 - prec_1: 0.9426 - recall_1: 0.9387 - prec_2: 0.9678 - recall_2: 0.9605 - prec_3: 0.7062 - recall_3: 0.7028 - prec_4: 0.6990 - recall_4: 0.7182 - prec_5: 0.9509 - recall_5: 0.9658 - f1_m: 0.8749 - recall_m: 0.8671 - precision_m: 0.8829 - precision: 0.8829 - recall_6: 0.8671 - f1score: 0.8749 - val_loss: 0.3677 - val_acc: 0.8466 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1633 - val_prec_2: 0.1901 - val_recall_2: 0.1785 - val_prec_3: 0.1706 - val_recall_3: 0.1575 - val_prec_4: 0.1599 - val_recall_4: 0.0665 - val_prec_5: 0.1684 - val_recall_5: 0.1584 - val_f1_m: 0.8440 - val_recall_m: 0.8373 - val_precision_m: 0.8509 - val_precision: 0.8509 - val_recall_6: 0.8373 - val_f1score: 0.8440\n",
      "Epoch 222/300\n",
      "epoch:  221\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2749 - acc: 0.8781 - prec: 0.9724 - recall: 0.9773 - prec_1: 0.9388 - recall_1: 0.9655 - prec_2: 0.9762 - recall_2: 0.9623 - prec_3: 0.7048 - recall_3: 0.7110 - prec_4: 0.7016 - recall_4: 0.7159 - prec_5: 0.9703 - recall_5: 0.9666 - f1_m: 0.8787 - recall_m: 0.8700 - precision_m: 0.8877 - precision: 0.8877 - recall_6: 0.8700 - f1score: 0.8787 - val_loss: 0.5994 - val_acc: 0.8101 - val_prec: 0.1759 - val_recall: 0.1694 - val_prec_1: 0.1753 - val_recall_1: 0.1407 - val_prec_2: 0.1887 - val_recall_2: 0.1845 - val_prec_3: 0.1279 - val_recall_3: 0.0357 - val_prec_4: 0.1809 - val_recall_4: 0.1837 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.8073 - val_recall_m: 0.7981 - val_precision_m: 0.8172 - val_precision: 0.8172 - val_recall_6: 0.7981 - val_f1score: 0.8073\n",
      "Epoch 223/300\n",
      "epoch:  222\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2711 - acc: 0.8823 - prec: 0.9672 - recall: 0.9817 - prec_1: 0.9508 - recall_1: 0.9583 - prec_2: 0.9815 - recall_2: 0.9647 - prec_3: 0.7085 - recall_3: 0.7216 - prec_4: 0.7103 - recall_4: 0.7102 - prec_5: 0.9696 - recall_5: 0.9751 - f1_m: 0.8818 - recall_m: 0.8741 - precision_m: 0.8897 - precision: 0.8897 - recall_6: 0.8741 - f1score: 0.8818 - val_loss: 0.3585 - val_acc: 0.8341 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1749 - val_recall_1: 0.1641 - val_prec_2: 0.1901 - val_recall_2: 0.1768 - val_prec_3: 0.1439 - val_recall_3: 0.0604 - val_prec_4: 0.1822 - val_recall_4: 0.1610 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8344 - val_recall_m: 0.8281 - val_precision_m: 0.8410 - val_precision: 0.8410 - val_recall_6: 0.8281 - val_f1score: 0.8344\n",
      "Epoch 224/300\n",
      "epoch:  223\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2728 - acc: 0.8805 - prec: 0.9731 - recall: 0.9793 - prec_1: 0.9408 - recall_1: 0.9607 - prec_2: 0.9783 - recall_2: 0.9657 - prec_3: 0.7199 - recall_3: 0.6929 - prec_4: 0.7022 - recall_4: 0.7461 - prec_5: 0.9682 - recall_5: 0.9706 - f1_m: 0.8787 - recall_m: 0.8707 - precision_m: 0.8870 - precision: 0.8870 - recall_6: 0.8707 - f1score: 0.8787 - val_loss: 0.4573 - val_acc: 0.8166 - val_prec: 0.1759 - val_recall: 0.1701 - val_prec_1: 0.1748 - val_recall_1: 0.1513 - val_prec_2: 0.1899 - val_recall_2: 0.1754 - val_prec_3: 0.0960 - val_recall_3: 0.0331 - val_prec_4: 0.1809 - val_recall_4: 0.1899 - val_prec_5: 0.1684 - val_recall_5: 0.1638 - val_f1_m: 0.8152 - val_recall_m: 0.8093 - val_precision_m: 0.8214 - val_precision: 0.8214 - val_recall_6: 0.8093 - val_f1score: 0.8152\n",
      "Epoch 225/300\n",
      "epoch:  224\n",
      "Learning rate:  0.001\n",
      "7998/7998 [==============================] - 5s 678us/step - loss: 0.2743 - acc: 0.8811 - prec: 0.9685 - recall: 0.9807 - prec_1: 0.9433 - recall_1: 0.9488 - prec_2: 0.9683 - recall_2: 0.9630 - prec_3: 0.7257 - recall_3: 0.7138 - prec_4: 0.7127 - recall_4: 0.7387 - prec_5: 0.9636 - recall_5: 0.9717 - f1_m: 0.8806 - recall_m: 0.8733 - precision_m: 0.8881 - precision: 0.8881 - recall_6: 0.8733 - f1score: 0.8806 - val_loss: 0.4407 - val_acc: 0.8268 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1746 - val_recall_1: 0.1684 - val_prec_2: 0.1896 - val_recall_2: 0.1649 - val_prec_3: 0.1709 - val_recall_3: 0.1592 - val_prec_4: 0.1759 - val_recall_4: 0.0608 - val_prec_5: 0.1684 - val_recall_5: 0.1548 - val_f1_m: 0.8257 - val_recall_m: 0.8186 - val_precision_m: 0.8332 - val_precision: 0.8332 - val_recall_6: 0.8186 - val_f1score: 0.8257\n",
      "Epoch 226/300\n",
      "epoch:  225\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2708 - acc: 0.8798 - prec: 0.9674 - recall: 0.9789 - prec_1: 0.9427 - recall_1: 0.9598 - prec_2: 0.9744 - recall_2: 0.9662 - prec_3: 0.7195 - recall_3: 0.7044 - prec_4: 0.7019 - recall_4: 0.7240 - prec_5: 0.9701 - recall_5: 0.9689 - f1_m: 0.8793 - recall_m: 0.8720 - precision_m: 0.8870 - precision: 0.8870 - recall_6: 0.8720 - f1score: 0.8793 - val_loss: 0.3810 - val_acc: 0.8291 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1604 - val_prec_2: 0.1901 - val_recall_2: 0.1768 - val_prec_3: 0.1279 - val_recall_3: 0.0537 - val_prec_4: 0.1809 - val_recall_4: 0.1742 - val_prec_5: 0.1684 - val_recall_5: 0.1607 - val_f1_m: 0.8272 - val_recall_m: 0.8203 - val_precision_m: 0.8345 - val_precision: 0.8345 - val_recall_6: 0.8203 - val_f1score: 0.8272\n",
      "Epoch 227/300\n",
      "epoch:  226\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2739 - acc: 0.8807 - prec: 0.9619 - recall: 0.9813 - prec_1: 0.9509 - recall_1: 0.9529 - prec_2: 0.9684 - recall_2: 0.9605 - prec_3: 0.7052 - recall_3: 0.7471 - prec_4: 0.7316 - recall_4: 0.6936 - prec_5: 0.9685 - recall_5: 0.9738 - f1_m: 0.8792 - recall_m: 0.8716 - precision_m: 0.8872 - precision: 0.8872 - recall_6: 0.8716 - f1score: 0.8792 - val_loss: 0.3064 - val_acc: 0.8616 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1558 - val_prec_2: 0.1899 - val_recall_2: 0.1779 - val_prec_3: 0.1599 - val_recall_3: 0.0862 - val_prec_4: 0.1809 - val_recall_4: 0.1777 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.8605 - val_recall_m: 0.8531 - val_precision_m: 0.8684 - val_precision: 0.8684 - val_recall_6: 0.8531 - val_f1score: 0.8605\n",
      "Epoch 228/300\n",
      "epoch:  227\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2703 - acc: 0.8813 - prec: 0.9750 - recall: 0.9804 - prec_1: 0.9505 - recall_1: 0.9611 - prec_2: 0.9698 - recall_2: 0.9655 - prec_3: 0.7024 - recall_3: 0.7231 - prec_4: 0.7128 - recall_4: 0.7025 - prec_5: 0.9707 - recall_5: 0.9712 - f1_m: 0.8806 - recall_m: 0.8736 - precision_m: 0.8878 - precision: 0.8878 - recall_6: 0.8736 - f1score: 0.8806 - val_loss: 0.2777 - val_acc: 0.8796 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1578 - val_prec_2: 0.1899 - val_recall_2: 0.1752 - val_prec_3: 0.1754 - val_recall_3: 0.1362 - val_prec_4: 0.1854 - val_recall_4: 0.1423 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8792 - val_recall_m: 0.8713 - val_precision_m: 0.8875 - val_precision: 0.8875 - val_recall_6: 0.8713 - val_f1score: 0.8792\n",
      "Epoch 229/300\n",
      "epoch:  228\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2711 - acc: 0.8803 - prec: 0.9680 - recall: 0.9830 - prec_1: 0.9573 - recall_1: 0.9464 - prec_2: 0.9637 - recall_2: 0.9669 - prec_3: 0.7163 - recall_3: 0.7127 - prec_4: 0.7109 - recall_4: 0.7244 - prec_5: 0.9614 - recall_5: 0.9721 - f1_m: 0.8799 - recall_m: 0.8722 - precision_m: 0.8879 - precision: 0.8879 - recall_6: 0.8722 - f1score: 0.8799 - val_loss: 0.2780 - val_acc: 0.8826 - val_prec: 0.1759 - val_recall: 0.1721 - val_prec_1: 0.1749 - val_recall_1: 0.1607 - val_prec_2: 0.1899 - val_recall_2: 0.1735 - val_prec_3: 0.1755 - val_recall_3: 0.1340 - val_prec_4: 0.1864 - val_recall_4: 0.1498 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8807 - val_recall_m: 0.8728 - val_precision_m: 0.8889 - val_precision: 0.8889 - val_recall_6: 0.8728 - val_f1score: 0.8807\n",
      "Epoch 230/300\n",
      "epoch:  229\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2732 - acc: 0.8775 - prec: 0.9671 - recall: 0.9796 - prec_1: 0.9546 - recall_1: 0.9576 - prec_2: 0.9732 - recall_2: 0.9680 - prec_3: 0.6950 - recall_3: 0.7124 - prec_4: 0.6983 - recall_4: 0.6929 - prec_5: 0.9711 - recall_5: 0.9769 - f1_m: 0.8767 - recall_m: 0.8693 - precision_m: 0.8843 - precision: 0.8843 - recall_6: 0.8693 - f1score: 0.8767 - val_loss: 0.2808 - val_acc: 0.8748 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1568 - val_prec_2: 0.1899 - val_recall_2: 0.1774 - val_prec_3: 0.1759 - val_recall_3: 0.1174 - val_prec_4: 0.1843 - val_recall_4: 0.1619 - val_prec_5: 0.1684 - val_recall_5: 0.1628 - val_f1_m: 0.8733 - val_recall_m: 0.8656 - val_precision_m: 0.8813 - val_precision: 0.8813 - val_recall_6: 0.8656 - val_f1score: 0.8733\n",
      "Epoch 231/300\n",
      "epoch:  230\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.2738 - acc: 0.8793 - prec: 0.9631 - recall: 0.9803 - prec_1: 0.9426 - recall_1: 0.9596 - prec_2: 0.9715 - recall_2: 0.9638 - prec_3: 0.7135 - recall_3: 0.7124 - prec_4: 0.7032 - recall_4: 0.7180 - prec_5: 0.9769 - recall_5: 0.9720 - f1_m: 0.8778 - recall_m: 0.8707 - precision_m: 0.8851 - precision: 0.8851 - recall_6: 0.8707 - f1score: 0.8778 - val_loss: 0.2799 - val_acc: 0.8776 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1583 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1759 - val_recall_3: 0.1226 - val_prec_4: 0.1830 - val_recall_4: 0.1531 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8763 - val_recall_m: 0.8688 - val_precision_m: 0.8841 - val_precision: 0.8841 - val_recall_6: 0.8688 - val_f1score: 0.8763\n",
      "Epoch 232/300\n",
      "epoch:  231\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2748 - acc: 0.8770 - prec: 0.9723 - recall: 0.9776 - prec_1: 0.9393 - recall_1: 0.9556 - prec_2: 0.9693 - recall_2: 0.9582 - prec_3: 0.7010 - recall_3: 0.7052 - prec_4: 0.7049 - recall_4: 0.7146 - prec_5: 0.9674 - recall_5: 0.9758 - f1_m: 0.8766 - recall_m: 0.8692 - precision_m: 0.8843 - precision: 0.8843 - recall_6: 0.8692 - f1score: 0.8766 - val_loss: 0.2799 - val_acc: 0.8788 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1737 - val_prec_3: 0.1755 - val_recall_3: 0.1437 - val_prec_4: 0.1868 - val_recall_4: 0.1325 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8770 - val_recall_m: 0.8693 - val_precision_m: 0.8849 - val_precision: 0.8849 - val_recall_6: 0.8693 - val_f1score: 0.8770\n",
      "Epoch 233/300\n",
      "epoch:  232\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2699 - acc: 0.8822 - prec: 0.9657 - recall: 0.9780 - prec_1: 0.9496 - recall_1: 0.9578 - prec_2: 0.9798 - recall_2: 0.9661 - prec_3: 0.7190 - recall_3: 0.7119 - prec_4: 0.7077 - recall_4: 0.7251 - prec_5: 0.9669 - recall_5: 0.9708 - f1_m: 0.8818 - recall_m: 0.8748 - precision_m: 0.8890 - precision: 0.8890 - recall_6: 0.8748 - f1score: 0.8818 - val_loss: 0.2839 - val_acc: 0.8761 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1735 - val_prec_3: 0.1755 - val_recall_3: 0.1474 - val_prec_4: 0.1886 - val_recall_4: 0.1285 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8750 - val_recall_m: 0.8668 - val_precision_m: 0.8834 - val_precision: 0.8834 - val_recall_6: 0.8668 - val_f1score: 0.8750\n",
      "Epoch 234/300\n",
      "epoch:  233\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2714 - acc: 0.8780 - prec: 0.9649 - recall: 0.9772 - prec_1: 0.9428 - recall_1: 0.9582 - prec_2: 0.9768 - recall_2: 0.9639 - prec_3: 0.7112 - recall_3: 0.7035 - prec_4: 0.7001 - recall_4: 0.7224 - prec_5: 0.9702 - recall_5: 0.9734 - f1_m: 0.8770 - recall_m: 0.8696 - precision_m: 0.8846 - precision: 0.8846 - recall_6: 0.8696 - f1score: 0.8770 - val_loss: 0.2785 - val_acc: 0.8766 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1752 - val_prec_3: 0.1759 - val_recall_3: 0.1181 - val_prec_4: 0.1830 - val_recall_4: 0.1553 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8742 - val_recall_m: 0.8678 - val_precision_m: 0.8808 - val_precision: 0.8808 - val_recall_6: 0.8678 - val_f1score: 0.8742\n",
      "Epoch 235/300\n",
      "epoch:  234\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2699 - acc: 0.8785 - prec: 0.9660 - recall: 0.9786 - prec_1: 0.9475 - recall_1: 0.9568 - prec_2: 0.9692 - recall_2: 0.9628 - prec_3: 0.7069 - recall_3: 0.7215 - prec_4: 0.7146 - recall_4: 0.7057 - prec_5: 0.9695 - recall_5: 0.9723 - f1_m: 0.8779 - recall_m: 0.8702 - precision_m: 0.8858 - precision: 0.8858 - recall_6: 0.8702 - f1score: 0.8779 - val_loss: 0.2825 - val_acc: 0.8741 - val_prec: 0.1759 - val_recall: 0.1728 - val_prec_1: 0.1749 - val_recall_1: 0.1626 - val_prec_2: 0.1899 - val_recall_2: 0.1737 - val_prec_3: 0.1759 - val_recall_3: 0.1092 - val_prec_4: 0.1822 - val_recall_4: 0.1586 - val_prec_5: 0.1684 - val_recall_5: 0.1612 - val_f1_m: 0.8728 - val_recall_m: 0.8658 - val_precision_m: 0.8801 - val_precision: 0.8801 - val_recall_6: 0.8658 - val_f1score: 0.8728\n",
      "Epoch 236/300\n",
      "epoch:  235\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2749 - acc: 0.8800 - prec: 0.9679 - recall: 0.9791 - prec_1: 0.9414 - recall_1: 0.9509 - prec_2: 0.9693 - recall_2: 0.9667 - prec_3: 0.7217 - recall_3: 0.7197 - prec_4: 0.7117 - recall_4: 0.7302 - prec_5: 0.9663 - recall_5: 0.9645 - f1_m: 0.8792 - recall_m: 0.8712 - precision_m: 0.8874 - precision: 0.8874 - recall_6: 0.8712 - f1score: 0.8792 - val_loss: 0.2787 - val_acc: 0.8786 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1749 - val_recall_1: 0.1610 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1759 - val_recall_3: 0.1227 - val_prec_4: 0.1839 - val_recall_4: 0.1531 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8766 - val_recall_m: 0.8698 - val_precision_m: 0.8837 - val_precision: 0.8837 - val_recall_6: 0.8698 - val_f1score: 0.8766\n",
      "Epoch 237/300\n",
      "epoch:  236\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2639 - acc: 0.8856 - prec: 0.9735 - recall: 0.9816 - prec_1: 0.9425 - recall_1: 0.9624 - prec_2: 0.9772 - recall_2: 0.9665 - prec_3: 0.7281 - recall_3: 0.7197 - prec_4: 0.7173 - recall_4: 0.7311 - prec_5: 0.9779 - recall_5: 0.9726 - f1_m: 0.8854 - recall_m: 0.8785 - precision_m: 0.8926 - precision: 0.8926 - recall_6: 0.8785 - f1score: 0.8854 - val_loss: 0.2812 - val_acc: 0.8823 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1578 - val_prec_2: 0.1899 - val_recall_2: 0.1766 - val_prec_3: 0.1755 - val_recall_3: 0.1400 - val_prec_4: 0.1872 - val_recall_4: 0.1453 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8804 - val_recall_m: 0.8723 - val_precision_m: 0.8889 - val_precision: 0.8889 - val_recall_6: 0.8723 - val_f1score: 0.8804\n",
      "Epoch 238/300\n",
      "epoch:  237\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2751 - acc: 0.8760 - prec: 0.9693 - recall: 0.9813 - prec_1: 0.9430 - recall_1: 0.9518 - prec_2: 0.9742 - recall_2: 0.9631 - prec_3: 0.7071 - recall_3: 0.6934 - prec_4: 0.6906 - recall_4: 0.7185 - prec_5: 0.9675 - recall_5: 0.9747 - f1_m: 0.8755 - recall_m: 0.8676 - precision_m: 0.8836 - precision: 0.8836 - recall_6: 0.8676 - f1score: 0.8755 - val_loss: 0.2800 - val_acc: 0.8776 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1737 - val_prec_3: 0.1759 - val_recall_3: 0.1294 - val_prec_4: 0.1848 - val_recall_4: 0.1496 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8769 - val_recall_m: 0.8693 - val_precision_m: 0.8847 - val_precision: 0.8847 - val_recall_6: 0.8693 - val_f1score: 0.8769\n",
      "Epoch 239/300\n",
      "epoch:  238\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2743 - acc: 0.8796 - prec: 0.9732 - recall: 0.9836 - prec_1: 0.9369 - recall_1: 0.9605 - prec_2: 0.9729 - recall_2: 0.9588 - prec_3: 0.7201 - recall_3: 0.7140 - prec_4: 0.7070 - recall_4: 0.7280 - prec_5: 0.9645 - recall_5: 0.9697 - f1_m: 0.8789 - recall_m: 0.8711 - precision_m: 0.8869 - precision: 0.8869 - recall_6: 0.8711 - f1score: 0.8789 - val_loss: 0.2802 - val_acc: 0.8796 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1598 - val_prec_2: 0.1899 - val_recall_2: 0.1735 - val_prec_3: 0.1759 - val_recall_3: 0.1210 - val_prec_4: 0.1829 - val_recall_4: 0.1553 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8767 - val_recall_m: 0.8693 - val_precision_m: 0.8844 - val_precision: 0.8844 - val_recall_6: 0.8693 - val_f1score: 0.8767\n",
      "Epoch 240/300\n",
      "epoch:  239\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2720 - acc: 0.8783 - prec: 0.9707 - recall: 0.9813 - prec_1: 0.9464 - recall_1: 0.9517 - prec_2: 0.9687 - recall_2: 0.9609 - prec_3: 0.7127 - recall_3: 0.6993 - prec_4: 0.7007 - recall_4: 0.7230 - prec_5: 0.9677 - recall_5: 0.9736 - f1_m: 0.8785 - recall_m: 0.8723 - precision_m: 0.8848 - precision: 0.8848 - recall_6: 0.8723 - f1score: 0.8785 - val_loss: 0.2771 - val_acc: 0.8816 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1747 - val_prec_3: 0.1754 - val_recall_3: 0.1336 - val_prec_4: 0.1857 - val_recall_4: 0.1460 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8800 - val_recall_m: 0.8723 - val_precision_m: 0.8881 - val_precision: 0.8881 - val_recall_6: 0.8723 - val_f1score: 0.8800\n",
      "Epoch 241/300\n",
      "epoch:  240\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2768 - acc: 0.8788 - prec: 0.9662 - recall: 0.9785 - prec_1: 0.9396 - recall_1: 0.9596 - prec_2: 0.9766 - recall_2: 0.9621 - prec_3: 0.7115 - recall_3: 0.7051 - prec_4: 0.7036 - recall_4: 0.7226 - prec_5: 0.9717 - recall_5: 0.9727 - f1_m: 0.8777 - recall_m: 0.8695 - precision_m: 0.8863 - precision: 0.8863 - recall_6: 0.8695 - f1score: 0.8777 - val_loss: 0.2810 - val_acc: 0.8683 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1571 - val_prec_2: 0.1899 - val_recall_2: 0.1762 - val_prec_3: 0.1759 - val_recall_3: 0.1225 - val_prec_4: 0.1846 - val_recall_4: 0.1408 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8676 - val_recall_m: 0.8603 - val_precision_m: 0.8752 - val_precision: 0.8752 - val_recall_6: 0.8603 - val_f1score: 0.8676\n",
      "Epoch 242/300\n",
      "epoch:  241\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.2702 - acc: 0.8818 - prec: 0.9704 - recall: 0.9779 - prec_1: 0.9415 - recall_1: 0.9568 - prec_2: 0.9714 - recall_2: 0.9630 - prec_3: 0.7279 - recall_3: 0.7114 - prec_4: 0.7076 - recall_4: 0.7369 - prec_5: 0.9684 - recall_5: 0.9715 - f1_m: 0.8809 - recall_m: 0.8731 - precision_m: 0.8890 - precision: 0.8890 - recall_6: 0.8731 - f1score: 0.8809 - val_loss: 0.2792 - val_acc: 0.8801 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1737 - val_prec_3: 0.1754 - val_recall_3: 0.1367 - val_prec_4: 0.1848 - val_recall_4: 0.1423 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8787 - val_recall_m: 0.8713 - val_precision_m: 0.8864 - val_precision: 0.8864 - val_recall_6: 0.8713 - val_f1score: 0.8787\n",
      "Epoch 243/300\n",
      "epoch:  242\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2738 - acc: 0.8791 - prec: 0.9670 - recall: 0.9823 - prec_1: 0.9491 - recall_1: 0.9566 - prec_2: 0.9681 - recall_2: 0.9664 - prec_3: 0.7127 - recall_3: 0.6932 - prec_4: 0.7014 - recall_4: 0.7301 - prec_5: 0.9675 - recall_5: 0.9704 - f1_m: 0.8776 - recall_m: 0.8697 - precision_m: 0.8858 - precision: 0.8858 - recall_6: 0.8697 - f1score: 0.8776 - val_loss: 0.2786 - val_acc: 0.8806 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1755 - val_recall_3: 0.1398 - val_prec_4: 0.1864 - val_recall_4: 0.1408 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8792 - val_recall_m: 0.8723 - val_precision_m: 0.8864 - val_precision: 0.8864 - val_recall_6: 0.8723 - val_f1score: 0.8792\n",
      "Epoch 244/300\n",
      "epoch:  243\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2756 - acc: 0.8773 - prec: 0.9674 - recall: 0.9764 - prec_1: 0.9369 - recall_1: 0.9588 - prec_2: 0.9762 - recall_2: 0.9660 - prec_3: 0.7069 - recall_3: 0.7138 - prec_4: 0.6976 - recall_4: 0.7097 - prec_5: 0.9703 - recall_5: 0.9728 - f1_m: 0.8754 - recall_m: 0.8670 - precision_m: 0.8842 - precision: 0.8842 - recall_6: 0.8670 - f1score: 0.8754 - val_loss: 0.2806 - val_acc: 0.8801 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1744 - val_prec_3: 0.1755 - val_recall_3: 0.1398 - val_prec_4: 0.1864 - val_recall_4: 0.1390 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8786 - val_recall_m: 0.8711 - val_precision_m: 0.8866 - val_precision: 0.8866 - val_recall_6: 0.8711 - val_f1score: 0.8786\n",
      "Epoch 245/300\n",
      "epoch:  244\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2698 - acc: 0.8826 - prec: 0.9714 - recall: 0.9813 - prec_1: 0.9521 - recall_1: 0.9605 - prec_2: 0.9834 - recall_2: 0.9698 - prec_3: 0.7148 - recall_3: 0.7168 - prec_4: 0.7085 - recall_4: 0.7175 - prec_5: 0.9692 - recall_5: 0.9743 - f1_m: 0.8807 - recall_m: 0.8738 - precision_m: 0.8878 - precision: 0.8878 - recall_6: 0.8738 - f1score: 0.8807 - val_loss: 0.2808 - val_acc: 0.8771 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1596 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1759 - val_recall_3: 0.1186 - val_prec_4: 0.1824 - val_recall_4: 0.1569 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8753 - val_recall_m: 0.8678 - val_precision_m: 0.8831 - val_precision: 0.8831 - val_recall_6: 0.8678 - val_f1score: 0.8753\n",
      "Epoch 246/300\n",
      "epoch:  245\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2730 - acc: 0.8781 - prec: 0.9692 - recall: 0.9769 - prec_1: 0.9455 - recall_1: 0.9596 - prec_2: 0.9724 - recall_2: 0.9622 - prec_3: 0.7136 - recall_3: 0.7062 - prec_4: 0.6998 - recall_4: 0.7180 - prec_5: 0.9674 - recall_5: 0.9753 - f1_m: 0.8786 - recall_m: 0.8710 - precision_m: 0.8865 - precision: 0.8865 - recall_6: 0.8710 - f1score: 0.8786 - val_loss: 0.2806 - val_acc: 0.8728 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1573 - val_prec_2: 0.1899 - val_recall_2: 0.1764 - val_prec_3: 0.1759 - val_recall_3: 0.1147 - val_prec_4: 0.1829 - val_recall_4: 0.1596 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8719 - val_recall_m: 0.8636 - val_precision_m: 0.8806 - val_precision: 0.8806 - val_recall_6: 0.8636 - val_f1score: 0.8719\n",
      "Epoch 247/300\n",
      "epoch:  246\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2690 - acc: 0.8841 - prec: 0.9719 - recall: 0.9824 - prec_1: 0.9459 - recall_1: 0.9602 - prec_2: 0.9759 - recall_2: 0.9621 - prec_3: 0.7194 - recall_3: 0.7156 - prec_4: 0.7137 - recall_4: 0.7323 - prec_5: 0.9709 - recall_5: 0.9702 - f1_m: 0.8833 - recall_m: 0.8762 - precision_m: 0.8907 - precision: 0.8907 - recall_6: 0.8762 - f1score: 0.8833 - val_loss: 0.2801 - val_acc: 0.8801 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1591 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1755 - val_recall_3: 0.1469 - val_prec_4: 0.1886 - val_recall_4: 0.1310 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8767 - val_recall_m: 0.8691 - val_precision_m: 0.8846 - val_precision: 0.8846 - val_recall_6: 0.8691 - val_f1score: 0.8767\n",
      "Epoch 248/300\n",
      "epoch:  247\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2715 - acc: 0.8781 - prec: 0.9665 - recall: 0.9774 - prec_1: 0.9511 - recall_1: 0.9559 - prec_2: 0.9677 - recall_2: 0.9597 - prec_3: 0.7060 - recall_3: 0.6976 - prec_4: 0.7028 - recall_4: 0.7189 - prec_5: 0.9641 - recall_5: 0.9748 - f1_m: 0.8779 - recall_m: 0.8701 - precision_m: 0.8859 - precision: 0.8859 - recall_6: 0.8701 - f1score: 0.8779 - val_loss: 0.2826 - val_acc: 0.8793 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1561 - val_prec_2: 0.1899 - val_recall_2: 0.1774 - val_prec_3: 0.1755 - val_recall_3: 0.1471 - val_prec_4: 0.1886 - val_recall_4: 0.1325 - val_prec_5: 0.1684 - val_recall_5: 0.1623 - val_f1_m: 0.8784 - val_recall_m: 0.8706 - val_precision_m: 0.8867 - val_precision: 0.8867 - val_recall_6: 0.8706 - val_f1score: 0.8784\n",
      "Epoch 249/300\n",
      "epoch:  248\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2704 - acc: 0.8778 - prec: 0.9705 - recall: 0.9772 - prec_1: 0.9422 - recall_1: 0.9647 - prec_2: 0.9741 - recall_2: 0.9621 - prec_3: 0.7061 - recall_3: 0.6898 - prec_4: 0.6942 - recall_4: 0.7226 - prec_5: 0.9804 - recall_5: 0.9725 - f1_m: 0.8771 - recall_m: 0.8695 - precision_m: 0.8850 - precision: 0.8850 - recall_6: 0.8695 - f1score: 0.8771 - val_loss: 0.2786 - val_acc: 0.8813 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1583 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1759 - val_recall_3: 0.1334 - val_prec_4: 0.1848 - val_recall_4: 0.1468 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8802 - val_recall_m: 0.8731 - val_precision_m: 0.8875 - val_precision: 0.8875 - val_recall_6: 0.8731 - val_f1score: 0.8802\n",
      "Epoch 250/300\n",
      "epoch:  249\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2673 - acc: 0.8831 - prec: 0.9675 - recall: 0.9796 - prec_1: 0.9468 - recall_1: 0.9604 - prec_2: 0.9721 - recall_2: 0.9701 - prec_3: 0.7236 - recall_3: 0.7114 - prec_4: 0.7126 - recall_4: 0.7285 - prec_5: 0.9769 - recall_5: 0.9713 - f1_m: 0.8817 - recall_m: 0.8746 - precision_m: 0.8890 - precision: 0.8890 - recall_6: 0.8746 - f1score: 0.8817 - val_loss: 0.2780 - val_acc: 0.8808 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1578 - val_prec_2: 0.1899 - val_recall_2: 0.1774 - val_prec_3: 0.1754 - val_recall_3: 0.1350 - val_prec_4: 0.1848 - val_recall_4: 0.1435 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8797 - val_recall_m: 0.8711 - val_precision_m: 0.8886 - val_precision: 0.8886 - val_recall_6: 0.8711 - val_f1score: 0.8797\n",
      "Epoch 251/300\n",
      "epoch:  250\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2688 - acc: 0.8823 - prec: 0.9689 - recall: 0.9769 - prec_1: 0.9440 - recall_1: 0.9597 - prec_2: 0.9746 - recall_2: 0.9658 - prec_3: 0.7174 - recall_3: 0.7283 - prec_4: 0.7156 - recall_4: 0.7136 - prec_5: 0.9661 - recall_5: 0.9724 - f1_m: 0.8825 - recall_m: 0.8751 - precision_m: 0.8903 - precision: 0.8903 - recall_6: 0.8751 - f1score: 0.8825 - val_loss: 0.2775 - val_acc: 0.8828 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1573 - val_prec_2: 0.1899 - val_recall_2: 0.1774 - val_prec_3: 0.1755 - val_recall_3: 0.1370 - val_prec_4: 0.1864 - val_recall_4: 0.1445 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8812 - val_recall_m: 0.8731 - val_precision_m: 0.8896 - val_precision: 0.8896 - val_recall_6: 0.8731 - val_f1score: 0.8812\n",
      "Epoch 252/300\n",
      "epoch:  251\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2690 - acc: 0.8796 - prec: 0.9663 - recall: 0.9799 - prec_1: 0.9452 - recall_1: 0.9599 - prec_2: 0.9742 - recall_2: 0.9660 - prec_3: 0.7100 - recall_3: 0.7136 - prec_4: 0.7109 - recall_4: 0.7136 - prec_5: 0.9736 - recall_5: 0.9691 - f1_m: 0.8801 - recall_m: 0.8727 - precision_m: 0.8877 - precision: 0.8877 - recall_6: 0.8727 - f1score: 0.8801 - val_loss: 0.2780 - val_acc: 0.8816 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1591 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1755 - val_recall_3: 0.1398 - val_prec_4: 0.1864 - val_recall_4: 0.1410 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8808 - val_recall_m: 0.8743 - val_precision_m: 0.8875 - val_precision: 0.8875 - val_recall_6: 0.8743 - val_f1score: 0.8808\n",
      "Epoch 253/300\n",
      "epoch:  252\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2686 - acc: 0.8822 - prec: 0.9673 - recall: 0.9828 - prec_1: 0.9451 - recall_1: 0.9609 - prec_2: 0.9753 - recall_2: 0.9650 - prec_3: 0.7158 - recall_3: 0.7165 - prec_4: 0.7133 - recall_4: 0.7207 - prec_5: 0.9764 - recall_5: 0.9744 - f1_m: 0.8809 - recall_m: 0.8731 - precision_m: 0.8889 - precision: 0.8889 - recall_6: 0.8731 - f1score: 0.8809 - val_loss: 0.2812 - val_acc: 0.8701 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1759 - val_recall_3: 0.1155 - val_prec_4: 0.1829 - val_recall_4: 0.1533 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8687 - val_recall_m: 0.8623 - val_precision_m: 0.8754 - val_precision: 0.8754 - val_recall_6: 0.8623 - val_f1score: 0.8687\n",
      "Epoch 254/300\n",
      "epoch:  253\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.2685 - acc: 0.8840 - prec: 0.9680 - recall: 0.9824 - prec_1: 0.9446 - recall_1: 0.9578 - prec_2: 0.9778 - recall_2: 0.9616 - prec_3: 0.7287 - recall_3: 0.7224 - prec_4: 0.7194 - recall_4: 0.7427 - prec_5: 0.9669 - recall_5: 0.9685 - f1_m: 0.8828 - recall_m: 0.8751 - precision_m: 0.8908 - precision: 0.8908 - recall_6: 0.8751 - f1score: 0.8828 - val_loss: 0.2817 - val_acc: 0.8776 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1749 - val_recall_1: 0.1612 - val_prec_2: 0.1899 - val_recall_2: 0.1727 - val_prec_3: 0.1755 - val_recall_3: 0.1490 - val_prec_4: 0.1891 - val_recall_4: 0.1232 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8748 - val_recall_m: 0.8663 - val_precision_m: 0.8837 - val_precision: 0.8837 - val_recall_6: 0.8663 - val_f1score: 0.8748\n",
      "Epoch 255/300\n",
      "epoch:  254\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2724 - acc: 0.8831 - prec: 0.9714 - recall: 0.9816 - prec_1: 0.9392 - recall_1: 0.9573 - prec_2: 0.9732 - recall_2: 0.9628 - prec_3: 0.7259 - recall_3: 0.7139 - prec_4: 0.7164 - recall_4: 0.7412 - prec_5: 0.9687 - recall_5: 0.9746 - f1_m: 0.8816 - recall_m: 0.8740 - precision_m: 0.8894 - precision: 0.8894 - recall_6: 0.8740 - f1score: 0.8816 - val_loss: 0.2779 - val_acc: 0.8791 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1752 - val_prec_3: 0.1755 - val_recall_3: 0.1421 - val_prec_4: 0.1876 - val_recall_4: 0.1363 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8785 - val_recall_m: 0.8706 - val_precision_m: 0.8868 - val_precision: 0.8868 - val_recall_6: 0.8706 - val_f1score: 0.8785\n",
      "Epoch 256/300\n",
      "epoch:  255\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2735 - acc: 0.8803 - prec: 0.9679 - recall: 0.9776 - prec_1: 0.9441 - recall_1: 0.9571 - prec_2: 0.9695 - recall_2: 0.9599 - prec_3: 0.7147 - recall_3: 0.7140 - prec_4: 0.7139 - recall_4: 0.7188 - prec_5: 0.9702 - recall_5: 0.9691 - f1_m: 0.8781 - recall_m: 0.8697 - precision_m: 0.8867 - precision: 0.8867 - recall_6: 0.8697 - f1score: 0.8781 - val_loss: 0.2800 - val_acc: 0.8773 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1748 - val_recall_1: 0.1601 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1755 - val_recall_3: 0.1474 - val_prec_4: 0.1886 - val_recall_4: 0.1265 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.8765 - val_recall_m: 0.8691 - val_precision_m: 0.8843 - val_precision: 0.8843 - val_recall_6: 0.8691 - val_f1score: 0.8765\n",
      "Epoch 257/300\n",
      "epoch:  256\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2700 - acc: 0.8821 - prec: 0.9683 - recall: 0.9826 - prec_1: 0.9435 - recall_1: 0.9558 - prec_2: 0.9733 - recall_2: 0.9634 - prec_3: 0.7254 - recall_3: 0.7185 - prec_4: 0.7122 - recall_4: 0.7316 - prec_5: 0.9697 - recall_5: 0.9772 - f1_m: 0.8804 - recall_m: 0.8727 - precision_m: 0.8883 - precision: 0.8883 - recall_6: 0.8727 - f1score: 0.8804 - val_loss: 0.2797 - val_acc: 0.8763 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1598 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1759 - val_recall_3: 0.1167 - val_prec_4: 0.1829 - val_recall_4: 0.1609 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8748 - val_recall_m: 0.8671 - val_precision_m: 0.8830 - val_precision: 0.8830 - val_recall_6: 0.8671 - val_f1score: 0.8748\n",
      "Epoch 258/300\n",
      "epoch:  257\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2746 - acc: 0.8767 - prec: 0.9689 - recall: 0.9739 - prec_1: 0.9423 - recall_1: 0.9594 - prec_2: 0.9683 - recall_2: 0.9611 - prec_3: 0.7152 - recall_3: 0.7051 - prec_4: 0.6937 - recall_4: 0.7178 - prec_5: 0.9702 - recall_5: 0.9735 - f1_m: 0.8778 - recall_m: 0.8697 - precision_m: 0.8861 - precision: 0.8861 - recall_6: 0.8697 - f1score: 0.8778 - val_loss: 0.2798 - val_acc: 0.8748 - val_prec: 0.1759 - val_recall: 0.1726 - val_prec_1: 0.1749 - val_recall_1: 0.1617 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1759 - val_recall_3: 0.1218 - val_prec_4: 0.1830 - val_recall_4: 0.1445 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8731 - val_recall_m: 0.8663 - val_precision_m: 0.8803 - val_precision: 0.8803 - val_recall_6: 0.8663 - val_f1score: 0.8731\n",
      "Epoch 259/300\n",
      "epoch:  258\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2714 - acc: 0.8812 - prec: 0.9731 - recall: 0.9810 - prec_1: 0.9435 - recall_1: 0.9630 - prec_2: 0.9765 - recall_2: 0.9664 - prec_3: 0.7076 - recall_3: 0.7140 - prec_4: 0.7071 - recall_4: 0.7159 - prec_5: 0.9742 - recall_5: 0.9681 - f1_m: 0.8796 - recall_m: 0.8723 - precision_m: 0.8871 - precision: 0.8871 - recall_6: 0.8723 - f1score: 0.8796 - val_loss: 0.2799 - val_acc: 0.8808 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1598 - val_prec_2: 0.1899 - val_recall_2: 0.1737 - val_prec_3: 0.1755 - val_recall_3: 0.1479 - val_prec_4: 0.1886 - val_recall_4: 0.1272 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8792 - val_recall_m: 0.8721 - val_precision_m: 0.8866 - val_precision: 0.8866 - val_recall_6: 0.8721 - val_f1score: 0.8792\n",
      "Epoch 260/300\n",
      "epoch:  259\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.2753 - acc: 0.8782 - prec: 0.9644 - recall: 0.9803 - prec_1: 0.9502 - recall_1: 0.9573 - prec_2: 0.9737 - recall_2: 0.9668 - prec_3: 0.7094 - recall_3: 0.7121 - prec_4: 0.7008 - recall_4: 0.7072 - prec_5: 0.9707 - recall_5: 0.9731 - f1_m: 0.8758 - recall_m: 0.8686 - precision_m: 0.8833 - precision: 0.8833 - recall_6: 0.8686 - f1score: 0.8758 - val_loss: 0.2785 - val_acc: 0.8801 - val_prec: 0.1759 - val_recall: 0.1726 - val_prec_1: 0.1749 - val_recall_1: 0.1628 - val_prec_2: 0.1899 - val_recall_2: 0.1730 - val_prec_3: 0.1759 - val_recall_3: 0.1318 - val_prec_4: 0.1846 - val_recall_4: 0.1403 - val_prec_5: 0.1684 - val_recall_5: 0.1612 - val_f1_m: 0.8781 - val_recall_m: 0.8701 - val_precision_m: 0.8864 - val_precision: 0.8864 - val_recall_6: 0.8701 - val_f1score: 0.8781\n",
      "Epoch 261/300\n",
      "epoch:  260\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2683 - acc: 0.8782 - prec: 0.9674 - recall: 0.9813 - prec_1: 0.9580 - recall_1: 0.9567 - prec_2: 0.9766 - recall_2: 0.9681 - prec_3: 0.7025 - recall_3: 0.7048 - prec_4: 0.6967 - recall_4: 0.7030 - prec_5: 0.9671 - recall_5: 0.9766 - f1_m: 0.8783 - recall_m: 0.8711 - precision_m: 0.8857 - precision: 0.8857 - recall_6: 0.8711 - f1score: 0.8783 - val_loss: 0.2832 - val_acc: 0.8756 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1574 - val_prec_2: 0.1899 - val_recall_2: 0.1749 - val_prec_3: 0.1755 - val_recall_3: 0.1468 - val_prec_4: 0.1872 - val_recall_4: 0.1283 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8728 - val_recall_m: 0.8643 - val_precision_m: 0.8816 - val_precision: 0.8816 - val_recall_6: 0.8643 - val_f1score: 0.8728\n",
      "Epoch 262/300\n",
      "epoch:  261\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2750 - acc: 0.8802 - prec: 0.9684 - recall: 0.9820 - prec_1: 0.9451 - recall_1: 0.9590 - prec_2: 0.9758 - recall_2: 0.9626 - prec_3: 0.7101 - recall_3: 0.7218 - prec_4: 0.7104 - recall_4: 0.7096 - prec_5: 0.9658 - recall_5: 0.9730 - f1_m: 0.8784 - recall_m: 0.8702 - precision_m: 0.8870 - precision: 0.8870 - recall_6: 0.8702 - f1score: 0.8784 - val_loss: 0.2852 - val_acc: 0.8743 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1612 - val_prec_2: 0.1899 - val_recall_2: 0.1735 - val_prec_3: 0.1755 - val_recall_3: 0.1557 - val_prec_4: 0.1891 - val_recall_4: 0.1120 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8716 - val_recall_m: 0.8638 - val_precision_m: 0.8797 - val_precision: 0.8797 - val_recall_6: 0.8638 - val_f1score: 0.8716\n",
      "Epoch 263/300\n",
      "epoch:  262\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2711 - acc: 0.8802 - prec: 0.9774 - recall: 0.9788 - prec_1: 0.9439 - recall_1: 0.9601 - prec_2: 0.9705 - recall_2: 0.9585 - prec_3: 0.7091 - recall_3: 0.7127 - prec_4: 0.7096 - recall_4: 0.7196 - prec_5: 0.9706 - recall_5: 0.9730 - f1_m: 0.8785 - recall_m: 0.8715 - precision_m: 0.8857 - precision: 0.8857 - recall_6: 0.8715 - f1score: 0.8785 - val_loss: 0.2787 - val_acc: 0.8826 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1749 - val_recall_1: 0.1614 - val_prec_2: 0.1899 - val_recall_2: 0.1730 - val_prec_3: 0.1759 - val_recall_3: 0.1355 - val_prec_4: 0.1851 - val_recall_4: 0.1443 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8796 - val_recall_m: 0.8716 - val_precision_m: 0.8879 - val_precision: 0.8879 - val_recall_6: 0.8716 - val_f1score: 0.8796\n",
      "Epoch 264/300\n",
      "epoch:  263\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2753 - acc: 0.8787 - prec: 0.9680 - recall: 0.9829 - prec_1: 0.9450 - recall_1: 0.9578 - prec_2: 0.9711 - recall_2: 0.9608 - prec_3: 0.7110 - recall_3: 0.7129 - prec_4: 0.7099 - recall_4: 0.7186 - prec_5: 0.9705 - recall_5: 0.9708 - f1_m: 0.8782 - recall_m: 0.8710 - precision_m: 0.8856 - precision: 0.8856 - recall_6: 0.8710 - f1score: 0.8782 - val_loss: 0.2803 - val_acc: 0.8818 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1586 - val_prec_2: 0.1899 - val_recall_2: 0.1757 - val_prec_3: 0.1759 - val_recall_3: 0.1203 - val_prec_4: 0.1830 - val_recall_4: 0.1644 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8797 - val_recall_m: 0.8728 - val_precision_m: 0.8868 - val_precision: 0.8868 - val_recall_6: 0.8728 - val_f1score: 0.8797\n",
      "Epoch 265/300\n",
      "epoch:  264\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2712 - acc: 0.8811 - prec: 0.9643 - recall: 0.9804 - prec_1: 0.9477 - recall_1: 0.9550 - prec_2: 0.9683 - recall_2: 0.9689 - prec_3: 0.7183 - recall_3: 0.7210 - prec_4: 0.7146 - recall_4: 0.7212 - prec_5: 0.9638 - recall_5: 0.9710 - f1_m: 0.8795 - recall_m: 0.8722 - precision_m: 0.8870 - precision: 0.8870 - recall_6: 0.8722 - f1score: 0.8795 - val_loss: 0.2833 - val_acc: 0.8766 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1584 - val_prec_2: 0.1899 - val_recall_2: 0.1747 - val_prec_3: 0.1755 - val_recall_3: 0.1479 - val_prec_4: 0.1886 - val_recall_4: 0.1278 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8764 - val_recall_m: 0.8678 - val_precision_m: 0.8853 - val_precision: 0.8853 - val_recall_6: 0.8678 - val_f1score: 0.8764\n",
      "Epoch 266/300\n",
      "epoch:  265\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2752 - acc: 0.8741 - prec: 0.9675 - recall: 0.9765 - prec_1: 0.9382 - recall_1: 0.9556 - prec_2: 0.9609 - recall_2: 0.9586 - prec_3: 0.6961 - recall_3: 0.7025 - prec_4: 0.6972 - recall_4: 0.7019 - prec_5: 0.9775 - recall_5: 0.9690 - f1_m: 0.8742 - recall_m: 0.8668 - precision_m: 0.8819 - precision: 0.8819 - recall_6: 0.8668 - f1score: 0.8742 - val_loss: 0.2804 - val_acc: 0.8786 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1730 - val_prec_3: 0.1759 - val_recall_3: 0.1280 - val_prec_4: 0.1837 - val_recall_4: 0.1496 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8759 - val_recall_m: 0.8691 - val_precision_m: 0.8831 - val_precision: 0.8831 - val_recall_6: 0.8691 - val_f1score: 0.8759\n",
      "Epoch 267/300\n",
      "epoch:  266\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2703 - acc: 0.8810 - prec: 0.9671 - recall: 0.9797 - prec_1: 0.9395 - recall_1: 0.9551 - prec_2: 0.9706 - recall_2: 0.9581 - prec_3: 0.7163 - recall_3: 0.7123 - prec_4: 0.7114 - recall_4: 0.7203 - prec_5: 0.9681 - recall_5: 0.9727 - f1_m: 0.8798 - recall_m: 0.8725 - precision_m: 0.8874 - precision: 0.8874 - recall_6: 0.8725 - f1score: 0.8798 - val_loss: 0.2920 - val_acc: 0.8576 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1576 - val_prec_2: 0.1899 - val_recall_2: 0.1759 - val_prec_3: 0.1759 - val_recall_3: 0.0855 - val_prec_4: 0.1812 - val_recall_4: 0.1739 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8570 - val_recall_m: 0.8496 - val_precision_m: 0.8648 - val_precision: 0.8648 - val_recall_6: 0.8496 - val_f1score: 0.8570\n",
      "Epoch 268/300\n",
      "epoch:  267\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2718 - acc: 0.8817 - prec: 0.9658 - recall: 0.9821 - prec_1: 0.9488 - recall_1: 0.9567 - prec_2: 0.9741 - recall_2: 0.9680 - prec_3: 0.7158 - recall_3: 0.7167 - prec_4: 0.7090 - recall_4: 0.7193 - prec_5: 0.9669 - recall_5: 0.9725 - f1_m: 0.8799 - recall_m: 0.8721 - precision_m: 0.8880 - precision: 0.8880 - recall_6: 0.8721 - f1score: 0.8799 - val_loss: 0.2802 - val_acc: 0.8741 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1748 - val_recall_1: 0.1578 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1754 - val_recall_3: 0.1391 - val_prec_4: 0.1857 - val_recall_4: 0.1297 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8724 - val_recall_m: 0.8643 - val_precision_m: 0.8809 - val_precision: 0.8809 - val_recall_6: 0.8643 - val_f1score: 0.8724\n",
      "Epoch 269/300\n",
      "epoch:  268\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 660us/step - loss: 0.2735 - acc: 0.8808 - prec: 0.9677 - recall: 0.9802 - prec_1: 0.9438 - recall_1: 0.9593 - prec_2: 0.9741 - recall_2: 0.9617 - prec_3: 0.7233 - recall_3: 0.7080 - prec_4: 0.7061 - recall_4: 0.7266 - prec_5: 0.9692 - recall_5: 0.9701 - f1_m: 0.8800 - recall_m: 0.8715 - precision_m: 0.8889 - precision: 0.8889 - recall_6: 0.8715 - f1score: 0.8800 - val_loss: 0.2810 - val_acc: 0.8788 - val_prec: 0.1759 - val_recall: 0.1708 - val_prec_1: 0.1748 - val_recall_1: 0.1564 - val_prec_2: 0.1899 - val_recall_2: 0.1759 - val_prec_3: 0.1753 - val_recall_3: 0.1285 - val_prec_4: 0.1839 - val_recall_4: 0.1533 - val_prec_5: 0.1684 - val_recall_5: 0.1628 - val_f1_m: 0.8786 - val_recall_m: 0.8708 - val_precision_m: 0.8868 - val_precision: 0.8868 - val_recall_6: 0.8708 - val_f1score: 0.8786\n",
      "Epoch 270/300\n",
      "epoch:  269\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 661us/step - loss: 0.2688 - acc: 0.8827 - prec: 0.9670 - recall: 0.9776 - prec_1: 0.9453 - recall_1: 0.9666 - prec_2: 0.9735 - recall_2: 0.9619 - prec_3: 0.7250 - recall_3: 0.7157 - prec_4: 0.7163 - recall_4: 0.7326 - prec_5: 0.9703 - recall_5: 0.9701 - f1_m: 0.8814 - recall_m: 0.8736 - precision_m: 0.8894 - precision: 0.8894 - recall_6: 0.8736 - f1score: 0.8814 - val_loss: 0.2808 - val_acc: 0.8813 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1571 - val_prec_2: 0.1899 - val_recall_2: 0.1759 - val_prec_3: 0.1754 - val_recall_3: 0.1320 - val_prec_4: 0.1848 - val_recall_4: 0.1498 - val_prec_5: 0.1684 - val_recall_5: 0.1628 - val_f1_m: 0.8798 - val_recall_m: 0.8721 - val_precision_m: 0.8878 - val_precision: 0.8878 - val_recall_6: 0.8721 - val_f1score: 0.8798\n",
      "Epoch 271/300\n",
      "epoch:  270\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 674us/step - loss: 0.2729 - acc: 0.8781 - prec: 0.9578 - recall: 0.9802 - prec_1: 0.9446 - recall_1: 0.9562 - prec_2: 0.9763 - recall_2: 0.9645 - prec_3: 0.7080 - recall_3: 0.6884 - prec_4: 0.6966 - recall_4: 0.7319 - prec_5: 0.9732 - recall_5: 0.9702 - f1_m: 0.8774 - recall_m: 0.8695 - precision_m: 0.8856 - precision: 0.8856 - recall_6: 0.8695 - f1score: 0.8774 - val_loss: 0.2789 - val_acc: 0.8818 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1749 - val_prec_3: 0.1754 - val_recall_3: 0.1353 - val_prec_4: 0.1851 - val_recall_4: 0.1488 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8798 - val_recall_m: 0.8726 - val_precision_m: 0.8873 - val_precision: 0.8873 - val_recall_6: 0.8726 - val_f1score: 0.8798\n",
      "Epoch 272/300\n",
      "epoch:  271\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2740 - acc: 0.8790 - prec: 0.9707 - recall: 0.9777 - prec_1: 0.9407 - recall_1: 0.9500 - prec_2: 0.9680 - recall_2: 0.9564 - prec_3: 0.7215 - recall_3: 0.7137 - prec_4: 0.7100 - recall_4: 0.7248 - prec_5: 0.9632 - recall_5: 0.9720 - f1_m: 0.8796 - recall_m: 0.8712 - precision_m: 0.8882 - precision: 0.8882 - recall_6: 0.8712 - f1score: 0.8796 - val_loss: 0.2793 - val_acc: 0.8781 - val_prec: 0.1759 - val_recall: 0.1721 - val_prec_1: 0.1749 - val_recall_1: 0.1628 - val_prec_2: 0.1899 - val_recall_2: 0.1737 - val_prec_3: 0.1759 - val_recall_3: 0.1193 - val_prec_4: 0.1830 - val_recall_4: 0.1492 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8759 - val_recall_m: 0.8693 - val_precision_m: 0.8829 - val_precision: 0.8829 - val_recall_6: 0.8693 - val_f1score: 0.8759\n",
      "Epoch 273/300\n",
      "epoch:  272\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2702 - acc: 0.8803 - prec: 0.9703 - recall: 0.9771 - prec_1: 0.9501 - recall_1: 0.9528 - prec_2: 0.9693 - recall_2: 0.9661 - prec_3: 0.7157 - recall_3: 0.7141 - prec_4: 0.7067 - recall_4: 0.7220 - prec_5: 0.9651 - recall_5: 0.9789 - f1_m: 0.8796 - recall_m: 0.8723 - precision_m: 0.8872 - precision: 0.8872 - recall_6: 0.8723 - f1score: 0.8796 - val_loss: 0.2772 - val_acc: 0.8798 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1612 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1754 - val_recall_3: 0.1350 - val_prec_4: 0.1848 - val_recall_4: 0.1370 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8785 - val_recall_m: 0.8708 - val_precision_m: 0.8866 - val_precision: 0.8866 - val_recall_6: 0.8708 - val_f1score: 0.8785\n",
      "Epoch 274/300\n",
      "epoch:  273\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 670us/step - loss: 0.2754 - acc: 0.8781 - prec: 0.9692 - recall: 0.9772 - prec_1: 0.9375 - recall_1: 0.9595 - prec_2: 0.9673 - recall_2: 0.9625 - prec_3: 0.7086 - recall_3: 0.7079 - prec_4: 0.7010 - recall_4: 0.7205 - prec_5: 0.9749 - recall_5: 0.9687 - f1_m: 0.8758 - recall_m: 0.8680 - precision_m: 0.8839 - precision: 0.8839 - recall_6: 0.8680 - f1score: 0.8758 - val_loss: 0.2818 - val_acc: 0.8726 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1586 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1759 - val_recall_3: 0.1121 - val_prec_4: 0.1824 - val_recall_4: 0.1639 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8709 - val_recall_m: 0.8636 - val_precision_m: 0.8786 - val_precision: 0.8786 - val_recall_6: 0.8636 - val_f1score: 0.8709\n",
      "Epoch 275/300\n",
      "epoch:  274\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2729 - acc: 0.8792 - prec: 0.9675 - recall: 0.9776 - prec_1: 0.9477 - recall_1: 0.9651 - prec_2: 0.9754 - recall_2: 0.9594 - prec_3: 0.7174 - recall_3: 0.6986 - prec_4: 0.6992 - recall_4: 0.7292 - prec_5: 0.9652 - recall_5: 0.9747 - f1_m: 0.8781 - recall_m: 0.8710 - precision_m: 0.8855 - precision: 0.8855 - recall_6: 0.8710 - f1score: 0.8781 - val_loss: 0.2790 - val_acc: 0.8826 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1591 - val_prec_2: 0.1899 - val_recall_2: 0.1752 - val_prec_3: 0.1759 - val_recall_3: 0.1319 - val_prec_4: 0.1848 - val_recall_4: 0.1486 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8813 - val_recall_m: 0.8741 - val_precision_m: 0.8888 - val_precision: 0.8888 - val_recall_6: 0.8741 - val_f1score: 0.8813\n",
      "Epoch 276/300\n",
      "epoch:  275\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2680 - acc: 0.8817 - prec: 0.9681 - recall: 0.9810 - prec_1: 0.9423 - recall_1: 0.9562 - prec_2: 0.9820 - recall_2: 0.9626 - prec_3: 0.7174 - recall_3: 0.7173 - prec_4: 0.7096 - recall_4: 0.7232 - prec_5: 0.9683 - recall_5: 0.9729 - f1_m: 0.8808 - recall_m: 0.8736 - precision_m: 0.8883 - precision: 0.8883 - recall_6: 0.8736 - f1score: 0.8808 - val_loss: 0.2784 - val_acc: 0.8773 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1752 - val_prec_3: 0.1759 - val_recall_3: 0.1217 - val_prec_4: 0.1839 - val_recall_4: 0.1536 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8765 - val_recall_m: 0.8698 - val_precision_m: 0.8834 - val_precision: 0.8834 - val_recall_6: 0.8698 - val_f1score: 0.8765\n",
      "Epoch 277/300\n",
      "epoch:  276\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2737 - acc: 0.8796 - prec: 0.9616 - recall: 0.9817 - prec_1: 0.9391 - recall_1: 0.9573 - prec_2: 0.9712 - recall_2: 0.9564 - prec_3: 0.7126 - recall_3: 0.7146 - prec_4: 0.7131 - recall_4: 0.7199 - prec_5: 0.9653 - recall_5: 0.9680 - f1_m: 0.8786 - recall_m: 0.8710 - precision_m: 0.8865 - precision: 0.8865 - recall_6: 0.8710 - f1score: 0.8786 - val_loss: 0.2800 - val_acc: 0.8703 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1571 - val_prec_2: 0.1899 - val_recall_2: 0.1754 - val_prec_3: 0.1759 - val_recall_3: 0.1217 - val_prec_4: 0.1839 - val_recall_4: 0.1521 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8691 - val_recall_m: 0.8611 - val_precision_m: 0.8774 - val_precision: 0.8774 - val_recall_6: 0.8611 - val_f1score: 0.8691\n",
      "Epoch 278/300\n",
      "epoch:  277\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2714 - acc: 0.8806 - prec: 0.9708 - recall: 0.9789 - prec_1: 0.9368 - recall_1: 0.9590 - prec_2: 0.9766 - recall_2: 0.9616 - prec_3: 0.7163 - recall_3: 0.7028 - prec_4: 0.7055 - recall_4: 0.7293 - prec_5: 0.9648 - recall_5: 0.9710 - f1_m: 0.8795 - recall_m: 0.8716 - precision_m: 0.8876 - precision: 0.8876 - recall_6: 0.8716 - f1score: 0.8795 - val_loss: 0.2813 - val_acc: 0.8696 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1571 - val_prec_2: 0.1899 - val_recall_2: 0.1771 - val_prec_3: 0.1759 - val_recall_3: 0.1077 - val_prec_4: 0.1829 - val_recall_4: 0.1657 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8686 - val_recall_m: 0.8616 - val_precision_m: 0.8760 - val_precision: 0.8760 - val_recall_6: 0.8616 - val_f1score: 0.8686\n",
      "Epoch 279/300\n",
      "epoch:  278\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2696 - acc: 0.8816 - prec: 0.9640 - recall: 0.9859 - prec_1: 0.9419 - recall_1: 0.9572 - prec_2: 0.9692 - recall_2: 0.9646 - prec_3: 0.7202 - recall_3: 0.7081 - prec_4: 0.7123 - recall_4: 0.7296 - prec_5: 0.9727 - recall_5: 0.9677 - f1_m: 0.8807 - recall_m: 0.8738 - precision_m: 0.8877 - precision: 0.8877 - recall_6: 0.8738 - f1score: 0.8807 - val_loss: 0.2883 - val_acc: 0.8718 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1593 - val_prec_2: 0.1899 - val_recall_2: 0.1727 - val_prec_3: 0.1755 - val_recall_3: 0.1531 - val_prec_4: 0.1897 - val_recall_4: 0.1145 - val_prec_5: 0.1684 - val_recall_5: 0.1627 - val_f1_m: 0.8714 - val_recall_m: 0.8641 - val_precision_m: 0.8790 - val_precision: 0.8790 - val_recall_6: 0.8641 - val_f1score: 0.8714\n",
      "Epoch 280/300\n",
      "epoch:  279\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 671us/step - loss: 0.2668 - acc: 0.8845 - prec: 0.9676 - recall: 0.9833 - prec_1: 0.9507 - recall_1: 0.9551 - prec_2: 0.9734 - recall_2: 0.9644 - prec_3: 0.7283 - recall_3: 0.7173 - prec_4: 0.7157 - recall_4: 0.7290 - prec_5: 0.9695 - recall_5: 0.9739 - f1_m: 0.8850 - recall_m: 0.8767 - precision_m: 0.8935 - precision: 0.8935 - recall_6: 0.8767 - f1score: 0.8850 - val_loss: 0.2797 - val_acc: 0.8781 - val_prec: 0.1759 - val_recall: 0.1721 - val_prec_1: 0.1748 - val_recall_1: 0.1601 - val_prec_2: 0.1899 - val_recall_2: 0.1730 - val_prec_3: 0.1755 - val_recall_3: 0.1446 - val_prec_4: 0.1886 - val_recall_4: 0.1328 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8773 - val_recall_m: 0.8701 - val_precision_m: 0.8848 - val_precision: 0.8848 - val_recall_6: 0.8701 - val_f1score: 0.8773\n",
      "Epoch 281/300\n",
      "epoch:  280\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 668us/step - loss: 0.2687 - acc: 0.8806 - prec: 0.9711 - recall: 0.9818 - prec_1: 0.9383 - recall_1: 0.9660 - prec_2: 0.9801 - recall_2: 0.9571 - prec_3: 0.7145 - recall_3: 0.7064 - prec_4: 0.7067 - recall_4: 0.7299 - prec_5: 0.9733 - recall_5: 0.9705 - f1_m: 0.8800 - recall_m: 0.8726 - precision_m: 0.8876 - precision: 0.8876 - recall_6: 0.8726 - f1score: 0.8800 - val_loss: 0.2867 - val_acc: 0.8631 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1593 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1759 - val_recall_3: 0.0948 - val_prec_4: 0.1812 - val_recall_4: 0.1702 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8630 - val_recall_m: 0.8553 - val_precision_m: 0.8711 - val_precision: 0.8711 - val_recall_6: 0.8553 - val_f1score: 0.8630\n",
      "Epoch 282/300\n",
      "epoch:  281\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 673us/step - loss: 0.2676 - acc: 0.8800 - prec: 0.9681 - recall: 0.9819 - prec_1: 0.9431 - recall_1: 0.9604 - prec_2: 0.9754 - recall_2: 0.9641 - prec_3: 0.7078 - recall_3: 0.7073 - prec_4: 0.7062 - recall_4: 0.7208 - prec_5: 0.9767 - recall_5: 0.9699 - f1_m: 0.8790 - recall_m: 0.8711 - precision_m: 0.8871 - precision: 0.8871 - recall_6: 0.8711 - f1score: 0.8790 - val_loss: 0.2830 - val_acc: 0.8671 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1757 - val_prec_3: 0.1759 - val_recall_3: 0.1047 - val_prec_4: 0.1824 - val_recall_4: 0.1654 - val_prec_5: 0.1684 - val_recall_5: 0.1628 - val_f1_m: 0.8653 - val_recall_m: 0.8581 - val_precision_m: 0.8728 - val_precision: 0.8728 - val_recall_6: 0.8581 - val_f1score: 0.8653\n",
      "Epoch 283/300\n",
      "epoch:  282\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 664us/step - loss: 0.2715 - acc: 0.8837 - prec: 0.9731 - recall: 0.9804 - prec_1: 0.9415 - recall_1: 0.9563 - prec_2: 0.9793 - recall_2: 0.9634 - prec_3: 0.7265 - recall_3: 0.7161 - prec_4: 0.7166 - recall_4: 0.7389 - prec_5: 0.9632 - recall_5: 0.9690 - f1_m: 0.8821 - recall_m: 0.8738 - precision_m: 0.8907 - precision: 0.8907 - recall_6: 0.8738 - f1score: 0.8821 - val_loss: 0.2863 - val_acc: 0.8748 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1617 - val_prec_2: 0.1899 - val_recall_2: 0.1747 - val_prec_3: 0.1755 - val_recall_3: 0.1562 - val_prec_4: 0.1904 - val_recall_4: 0.1120 - val_prec_5: 0.1684 - val_recall_5: 0.1612 - val_f1_m: 0.8716 - val_recall_m: 0.8633 - val_precision_m: 0.8804 - val_precision: 0.8804 - val_recall_6: 0.8633 - val_f1score: 0.8716\n",
      "Epoch 284/300\n",
      "epoch:  283\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2678 - acc: 0.8835 - prec: 0.9711 - recall: 0.9787 - prec_1: 0.9442 - recall_1: 0.9566 - prec_2: 0.9738 - recall_2: 0.9651 - prec_3: 0.7273 - recall_3: 0.7166 - prec_4: 0.7170 - recall_4: 0.7346 - prec_5: 0.9643 - recall_5: 0.9692 - f1_m: 0.8833 - recall_m: 0.8763 - precision_m: 0.8906 - precision: 0.8906 - recall_6: 0.8763 - f1score: 0.8833 - val_loss: 0.2845 - val_acc: 0.8763 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1641 - val_prec_2: 0.1899 - val_recall_2: 0.1737 - val_prec_3: 0.1755 - val_recall_3: 0.1530 - val_prec_4: 0.1891 - val_recall_4: 0.1165 - val_prec_5: 0.1684 - val_recall_5: 0.1592 - val_f1_m: 0.8749 - val_recall_m: 0.8676 - val_precision_m: 0.8826 - val_precision: 0.8826 - val_recall_6: 0.8676 - val_f1score: 0.8749\n",
      "Epoch 285/300\n",
      "epoch:  284\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 659us/step - loss: 0.2721 - acc: 0.8806 - prec: 0.9613 - recall: 0.9819 - prec_1: 0.9447 - recall_1: 0.9554 - prec_2: 0.9692 - recall_2: 0.9618 - prec_3: 0.7125 - recall_3: 0.7285 - prec_4: 0.7180 - recall_4: 0.7143 - prec_5: 0.9694 - recall_5: 0.9742 - f1_m: 0.8799 - recall_m: 0.8720 - precision_m: 0.8880 - precision: 0.8880 - recall_6: 0.8720 - f1score: 0.8799 - val_loss: 0.2794 - val_acc: 0.8843 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1583 - val_prec_2: 0.1899 - val_recall_2: 0.1757 - val_prec_3: 0.1755 - val_recall_3: 0.1466 - val_prec_4: 0.1886 - val_recall_4: 0.1365 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8821 - val_recall_m: 0.8746 - val_precision_m: 0.8901 - val_precision: 0.8901 - val_recall_6: 0.8746 - val_f1score: 0.8821\n",
      "Epoch 286/300\n",
      "epoch:  285\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2693 - acc: 0.8815 - prec: 0.9664 - recall: 0.9788 - prec_1: 0.9420 - recall_1: 0.9561 - prec_2: 0.9762 - recall_2: 0.9652 - prec_3: 0.7171 - recall_3: 0.7066 - prec_4: 0.7095 - recall_4: 0.7268 - prec_5: 0.9699 - recall_5: 0.9725 - f1_m: 0.8807 - recall_m: 0.8741 - precision_m: 0.8876 - precision: 0.8876 - recall_6: 0.8741 - f1score: 0.8807 - val_loss: 0.2774 - val_acc: 0.8823 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1749 - val_recall_1: 0.1587 - val_prec_2: 0.1899 - val_recall_2: 0.1742 - val_prec_3: 0.1755 - val_recall_3: 0.1433 - val_prec_4: 0.1872 - val_recall_4: 0.1340 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8811 - val_recall_m: 0.8733 - val_precision_m: 0.8892 - val_precision: 0.8892 - val_recall_6: 0.8733 - val_f1score: 0.8811\n",
      "Epoch 287/300\n",
      "epoch:  286\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2717 - acc: 0.8803 - prec: 0.9730 - recall: 0.9779 - prec_1: 0.9429 - recall_1: 0.9566 - prec_2: 0.9679 - recall_2: 0.9639 - prec_3: 0.7185 - recall_3: 0.7082 - prec_4: 0.7078 - recall_4: 0.7275 - prec_5: 0.9686 - recall_5: 0.9739 - f1_m: 0.8795 - recall_m: 0.8722 - precision_m: 0.8870 - precision: 0.8870 - recall_6: 0.8722 - f1score: 0.8795 - val_loss: 0.2774 - val_acc: 0.8781 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1602 - val_prec_2: 0.1899 - val_recall_2: 0.1730 - val_prec_3: 0.1755 - val_recall_3: 0.1395 - val_prec_4: 0.1864 - val_recall_4: 0.1330 - val_prec_5: 0.1684 - val_recall_5: 0.1615 - val_f1_m: 0.8771 - val_recall_m: 0.8691 - val_precision_m: 0.8856 - val_precision: 0.8856 - val_recall_6: 0.8691 - val_f1score: 0.8771\n",
      "Epoch 288/300\n",
      "epoch:  287\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 679us/step - loss: 0.2705 - acc: 0.8803 - prec: 0.9727 - recall: 0.9842 - prec_1: 0.9476 - recall_1: 0.9652 - prec_2: 0.9797 - recall_2: 0.9665 - prec_3: 0.7088 - recall_3: 0.7115 - prec_4: 0.7025 - recall_4: 0.7120 - prec_5: 0.9729 - recall_5: 0.9776 - f1_m: 0.8793 - recall_m: 0.8717 - precision_m: 0.8872 - precision: 0.8872 - recall_6: 0.8717 - f1score: 0.8793 - val_loss: 0.2797 - val_acc: 0.8791 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1581 - val_prec_2: 0.1899 - val_recall_2: 0.1727 - val_prec_3: 0.1754 - val_recall_3: 0.1365 - val_prec_4: 0.1848 - val_recall_4: 0.1420 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8780 - val_recall_m: 0.8706 - val_precision_m: 0.8858 - val_precision: 0.8858 - val_recall_6: 0.8706 - val_f1score: 0.8780\n",
      "Epoch 289/300\n",
      "epoch:  288\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 665us/step - loss: 0.2696 - acc: 0.8805 - prec: 0.9712 - recall: 0.9786 - prec_1: 0.9450 - recall_1: 0.9594 - prec_2: 0.9688 - recall_2: 0.9630 - prec_3: 0.7101 - recall_3: 0.7097 - prec_4: 0.7058 - recall_4: 0.7165 - prec_5: 0.9730 - recall_5: 0.9696 - f1_m: 0.8808 - recall_m: 0.8733 - precision_m: 0.8885 - precision: 0.8885 - recall_6: 0.8733 - f1score: 0.8808 - val_loss: 0.3145 - val_acc: 0.8548 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1566 - val_prec_2: 0.1899 - val_recall_2: 0.1762 - val_prec_3: 0.1599 - val_recall_3: 0.0762 - val_prec_4: 0.1809 - val_recall_4: 0.1804 - val_prec_5: 0.1684 - val_recall_5: 0.1630 - val_f1_m: 0.8529 - val_recall_m: 0.8458 - val_precision_m: 0.8603 - val_precision: 0.8603 - val_recall_6: 0.8458 - val_f1score: 0.8529\n",
      "Epoch 290/300\n",
      "epoch:  289\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 662us/step - loss: 0.2681 - acc: 0.8815 - prec: 0.9696 - recall: 0.9766 - prec_1: 0.9335 - recall_1: 0.9601 - prec_2: 0.9770 - recall_2: 0.9622 - prec_3: 0.7209 - recall_3: 0.7074 - prec_4: 0.7120 - recall_4: 0.7371 - prec_5: 0.9686 - recall_5: 0.9705 - f1_m: 0.8805 - recall_m: 0.8723 - precision_m: 0.8890 - precision: 0.8890 - recall_6: 0.8723 - f1score: 0.8805 - val_loss: 0.2789 - val_acc: 0.8801 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1606 - val_prec_2: 0.1899 - val_recall_2: 0.1732 - val_prec_3: 0.1754 - val_recall_3: 0.1351 - val_prec_4: 0.1857 - val_recall_4: 0.1428 - val_prec_5: 0.1684 - val_recall_5: 0.1617 - val_f1_m: 0.8789 - val_recall_m: 0.8726 - val_precision_m: 0.8854 - val_precision: 0.8854 - val_recall_6: 0.8726 - val_f1score: 0.8789\n",
      "Epoch 291/300\n",
      "epoch:  290\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2697 - acc: 0.8807 - prec: 0.9667 - recall: 0.9813 - prec_1: 0.9470 - recall_1: 0.9534 - prec_2: 0.9768 - recall_2: 0.9617 - prec_3: 0.7184 - recall_3: 0.7046 - prec_4: 0.7105 - recall_4: 0.7325 - prec_5: 0.9645 - recall_5: 0.9747 - f1_m: 0.8798 - recall_m: 0.8722 - precision_m: 0.8877 - precision: 0.8877 - recall_6: 0.8722 - f1score: 0.8798 - val_loss: 0.2788 - val_acc: 0.8848 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1598 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1755 - val_recall_3: 0.1459 - val_prec_4: 0.1886 - val_recall_4: 0.1373 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8831 - val_recall_m: 0.8758 - val_precision_m: 0.8907 - val_precision: 0.8907 - val_recall_6: 0.8758 - val_f1score: 0.8831\n",
      "Epoch 292/300\n",
      "epoch:  291\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2682 - acc: 0.8802 - prec: 0.9711 - recall: 0.9779 - prec_1: 0.9504 - recall_1: 0.9583 - prec_2: 0.9780 - recall_2: 0.9665 - prec_3: 0.7179 - recall_3: 0.7027 - prec_4: 0.7032 - recall_4: 0.7335 - prec_5: 0.9643 - recall_5: 0.9724 - f1_m: 0.8791 - recall_m: 0.8717 - precision_m: 0.8868 - precision: 0.8868 - recall_6: 0.8717 - f1score: 0.8791 - val_loss: 0.2794 - val_acc: 0.8821 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1588 - val_prec_2: 0.1899 - val_recall_2: 0.1742 - val_prec_3: 0.1755 - val_recall_3: 0.1482 - val_prec_4: 0.1891 - val_recall_4: 0.1330 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8801 - val_recall_m: 0.8723 - val_precision_m: 0.8882 - val_precision: 0.8882 - val_recall_6: 0.8723 - val_f1score: 0.8801\n",
      "Epoch 293/300\n",
      "epoch:  292\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 667us/step - loss: 0.2720 - acc: 0.8778 - prec: 0.9700 - recall: 0.9822 - prec_1: 0.9373 - recall_1: 0.9548 - prec_2: 0.9717 - recall_2: 0.9637 - prec_3: 0.7091 - recall_3: 0.7049 - prec_4: 0.6990 - recall_4: 0.7210 - prec_5: 0.9674 - recall_5: 0.9699 - f1_m: 0.8774 - recall_m: 0.8702 - precision_m: 0.8848 - precision: 0.8848 - recall_6: 0.8702 - f1score: 0.8774 - val_loss: 0.2798 - val_acc: 0.8786 - val_prec: 0.1759 - val_recall: 0.1713 - val_prec_1: 0.1748 - val_recall_1: 0.1571 - val_prec_2: 0.1899 - val_recall_2: 0.1747 - val_prec_3: 0.1755 - val_recall_3: 0.1415 - val_prec_4: 0.1864 - val_recall_4: 0.1368 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8771 - val_recall_m: 0.8691 - val_precision_m: 0.8854 - val_precision: 0.8854 - val_recall_6: 0.8691 - val_f1score: 0.8771\n",
      "Epoch 294/300\n",
      "epoch:  293\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 677us/step - loss: 0.2744 - acc: 0.8778 - prec: 0.9693 - recall: 0.9789 - prec_1: 0.9392 - recall_1: 0.9532 - prec_2: 0.9672 - recall_2: 0.9620 - prec_3: 0.7105 - recall_3: 0.6959 - prec_4: 0.6990 - recall_4: 0.7316 - prec_5: 0.9711 - recall_5: 0.9746 - f1_m: 0.8764 - recall_m: 0.8697 - precision_m: 0.8833 - precision: 0.8833 - recall_6: 0.8697 - f1score: 0.8764 - val_loss: 0.2808 - val_acc: 0.8751 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1633 - val_prec_2: 0.1899 - val_recall_2: 0.1730 - val_prec_3: 0.1759 - val_recall_3: 0.1305 - val_prec_4: 0.1841 - val_recall_4: 0.1363 - val_prec_5: 0.1684 - val_recall_5: 0.1610 - val_f1_m: 0.8734 - val_recall_m: 0.8661 - val_precision_m: 0.8810 - val_precision: 0.8810 - val_recall_6: 0.8661 - val_f1score: 0.8734\n",
      "Epoch 295/300\n",
      "epoch:  294\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2715 - acc: 0.8790 - prec: 0.9629 - recall: 0.9797 - prec_1: 0.9435 - recall_1: 0.9549 - prec_2: 0.9809 - recall_2: 0.9637 - prec_3: 0.7207 - recall_3: 0.6931 - prec_4: 0.7021 - recall_4: 0.7351 - prec_5: 0.9655 - recall_5: 0.9709 - f1_m: 0.8782 - recall_m: 0.8705 - precision_m: 0.8862 - precision: 0.8862 - recall_6: 0.8705 - f1score: 0.8782 - val_loss: 0.2830 - val_acc: 0.8811 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1571 - val_prec_2: 0.1899 - val_recall_2: 0.1759 - val_prec_3: 0.1755 - val_recall_3: 0.1487 - val_prec_4: 0.1891 - val_recall_4: 0.1325 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8811 - val_recall_m: 0.8728 - val_precision_m: 0.8897 - val_precision: 0.8897 - val_recall_6: 0.8728 - val_f1score: 0.8811\n",
      "Epoch 296/300\n",
      "epoch:  295\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2685 - acc: 0.8828 - prec: 0.9669 - recall: 0.9793 - prec_1: 0.9448 - recall_1: 0.9624 - prec_2: 0.9769 - recall_2: 0.9595 - prec_3: 0.7259 - recall_3: 0.7203 - prec_4: 0.7178 - recall_4: 0.7317 - prec_5: 0.9767 - recall_5: 0.9746 - f1_m: 0.8822 - recall_m: 0.8750 - precision_m: 0.8897 - precision: 0.8897 - recall_6: 0.8750 - f1score: 0.8822 - val_loss: 0.2783 - val_acc: 0.8828 - val_prec: 0.1759 - val_recall: 0.1711 - val_prec_1: 0.1748 - val_recall_1: 0.1578 - val_prec_2: 0.1899 - val_recall_2: 0.1754 - val_prec_3: 0.1755 - val_recall_3: 0.1390 - val_prec_4: 0.1864 - val_recall_4: 0.1451 - val_prec_5: 0.1684 - val_recall_5: 0.1625 - val_f1_m: 0.8820 - val_recall_m: 0.8738 - val_precision_m: 0.8905 - val_precision: 0.8905 - val_recall_6: 0.8738 - val_f1score: 0.8820\n",
      "Epoch 297/300\n",
      "epoch:  296\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 663us/step - loss: 0.2763 - acc: 0.8776 - prec: 0.9650 - recall: 0.9780 - prec_1: 0.9415 - recall_1: 0.9572 - prec_2: 0.9696 - recall_2: 0.9607 - prec_3: 0.7192 - recall_3: 0.7035 - prec_4: 0.7046 - recall_4: 0.7366 - prec_5: 0.9674 - recall_5: 0.9678 - f1_m: 0.8767 - recall_m: 0.8691 - precision_m: 0.8845 - precision: 0.8845 - recall_6: 0.8691 - f1score: 0.8767 - val_loss: 0.2835 - val_acc: 0.8748 - val_prec: 0.1759 - val_recall: 0.1716 - val_prec_1: 0.1748 - val_recall_1: 0.1593 - val_prec_2: 0.1899 - val_recall_2: 0.1735 - val_prec_3: 0.1759 - val_recall_3: 0.1095 - val_prec_4: 0.1823 - val_recall_4: 0.1666 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8730 - val_recall_m: 0.8653 - val_precision_m: 0.8811 - val_precision: 0.8811 - val_recall_6: 0.8653 - val_f1score: 0.8730\n",
      "Epoch 298/300\n",
      "epoch:  297\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998/7998 [==============================] - 5s 669us/step - loss: 0.2706 - acc: 0.8828 - prec: 0.9673 - recall: 0.9803 - prec_1: 0.9431 - recall_1: 0.9546 - prec_2: 0.9685 - recall_2: 0.9636 - prec_3: 0.7230 - recall_3: 0.7260 - prec_4: 0.7215 - recall_4: 0.7305 - prec_5: 0.9699 - recall_5: 0.9699 - f1_m: 0.8825 - recall_m: 0.8745 - precision_m: 0.8909 - precision: 0.8909 - recall_6: 0.8745 - f1score: 0.8825 - val_loss: 0.2859 - val_acc: 0.8743 - val_prec: 0.1759 - val_recall: 0.1723 - val_prec_1: 0.1749 - val_recall_1: 0.1612 - val_prec_2: 0.1899 - val_recall_2: 0.1730 - val_prec_3: 0.1755 - val_recall_3: 0.1558 - val_prec_4: 0.1897 - val_recall_4: 0.1122 - val_prec_5: 0.1684 - val_recall_5: 0.1622 - val_f1_m: 0.8725 - val_recall_m: 0.8651 - val_precision_m: 0.8801 - val_precision: 0.8801 - val_recall_6: 0.8651 - val_f1score: 0.8725\n",
      "Epoch 299/300\n",
      "epoch:  298\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 666us/step - loss: 0.2703 - acc: 0.8826 - prec: 0.9702 - recall: 0.9837 - prec_1: 0.9458 - recall_1: 0.9568 - prec_2: 0.9764 - recall_2: 0.9618 - prec_3: 0.7201 - recall_3: 0.7222 - prec_4: 0.7197 - recall_4: 0.7298 - prec_5: 0.9654 - recall_5: 0.9736 - f1_m: 0.8825 - recall_m: 0.8752 - precision_m: 0.8901 - precision: 0.8901 - recall_6: 0.8752 - f1score: 0.8825 - val_loss: 0.2790 - val_acc: 0.8816 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1749 - val_recall_1: 0.1602 - val_prec_2: 0.1899 - val_recall_2: 0.1740 - val_prec_3: 0.1755 - val_recall_3: 0.1471 - val_prec_4: 0.1886 - val_recall_4: 0.1305 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8798 - val_recall_m: 0.8728 - val_precision_m: 0.8870 - val_precision: 0.8870 - val_recall_6: 0.8728 - val_f1score: 0.8798\n",
      "Epoch 300/300\n",
      "epoch:  299\n",
      "Learning rate:  0.0001\n",
      "7998/7998 [==============================] - 5s 672us/step - loss: 0.2725 - acc: 0.8787 - prec: 0.9698 - recall: 0.9782 - prec_1: 0.9373 - recall_1: 0.9546 - prec_2: 0.9663 - recall_2: 0.9590 - prec_3: 0.7064 - recall_3: 0.7200 - prec_4: 0.7136 - recall_4: 0.7105 - prec_5: 0.9693 - recall_5: 0.9749 - f1_m: 0.8778 - recall_m: 0.8701 - precision_m: 0.8857 - precision: 0.8857 - recall_6: 0.8701 - f1score: 0.8778 - val_loss: 0.2780 - val_acc: 0.8808 - val_prec: 0.1759 - val_recall: 0.1718 - val_prec_1: 0.1748 - val_recall_1: 0.1591 - val_prec_2: 0.1899 - val_recall_2: 0.1742 - val_prec_3: 0.1759 - val_recall_3: 0.1297 - val_prec_4: 0.1839 - val_recall_4: 0.1488 - val_prec_5: 0.1684 - val_recall_5: 0.1620 - val_f1_m: 0.8787 - val_recall_m: 0.8716 - val_precision_m: 0.8861 - val_precision: 0.8861 - val_recall_6: 0.8716 - val_f1score: 0.8787\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 500, 1)            0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_8 (ZeroPaddin (None, 506, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1/conv (Conv1D)          (None, 500, 24)           192       \n",
      "_________________________________________________________________\n",
      "1_conv (Conv1D)              (None, 500, 48)           1200      \n",
      "_________________________________________________________________\n",
      "1_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "1_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "2_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "2_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "2_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "3_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "3_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "3_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "4_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "4_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "4_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "5_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "5_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "5_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "6_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "6_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "6_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "7_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "7_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "7_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "8_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "8_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "8_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "9_conv (Conv1D)              (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "9_bn (BatchNormalization)    (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "9_relu (Activation)          (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "10_conv (Conv1D)             (None, 500, 48)           2352      \n",
      "_________________________________________________________________\n",
      "10_bn (BatchNormalization)   (None, 500, 48)           192       \n",
      "_________________________________________________________________\n",
      "10_relu (Activation)         (None, 500, 48)           0         \n",
      "_________________________________________________________________\n",
      "avg_pool (GlobalAveragePooli (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 6)                 294       \n",
      "=================================================================\n",
      "Total params: 24,774\n",
      "Trainable params: 23,814\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Train on 8004 samples, validate on 3996 samples\n",
      "Epoch 1/300\n",
      "epoch:  0\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 15s 2ms/step - loss: 1.4489 - acc: 0.4978 - prec: 0.7298 - recall: 0.6923 - prec_1: 0.3840 - recall_1: 0.5636 - prec_2: 0.4137 - recall_2: 0.3977 - prec_3: 0.5202 - recall_3: 0.1965 - prec_4: 0.4558 - recall_4: 0.8499 - prec_5: 0.4382 - recall_5: 0.2825 - f1_m: 0.0076 - recall_m: 0.0039 - precision_m: 0.2159 - precision: 0.2159 - recall_6: 0.0039 - f1score: 0.0076 - val_loss: 2.3074 - val_acc: 0.2025 - val_prec: 0.1441 - val_recall: 0.0376 - val_prec_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - val_prec_2: 0.1667 - val_recall_2: 0.1922 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0000e+00 - val_recall_5: 0.0000e+00 - val_f1_m: 0.0962 - val_recall_m: 0.0718 - val_precision_m: 0.1819 - val_precision: 0.1819 - val_recall_6: 0.0718 - val_f1score: 0.0962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "epoch:  1\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 1.0622 - acc: 0.6352 - prec: 0.8248 - recall: 0.9330 - prec_1: 0.5835 - recall_1: 0.2768 - prec_2: 0.5979 - recall_2: 0.8539 - prec_3: 0.5769 - recall_3: 0.5673 - prec_4: 0.5748 - recall_4: 0.5909 - prec_5: 0.6426 - recall_5: 0.6161 - f1_m: 0.1696 - recall_m: 0.0985 - precision_m: 0.8870 - precision: 0.8870 - recall_6: 0.0985 - f1score: 0.1696 - val_loss: 1.5745 - val_acc: 0.4287 - val_prec: 0.1762 - val_recall: 0.1313 - val_prec_1: 0.1602 - val_recall_1: 0.0223 - val_prec_2: 0.1809 - val_recall_2: 0.1733 - val_prec_3: 0.0801 - val_recall_3: 0.0172 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1162 - val_f1_m: 0.1481 - val_recall_m: 0.1071 - val_precision_m: 0.3418 - val_precision: 0.3418 - val_recall_6: 0.1071 - val_f1score: 0.1481\n",
      "Epoch 3/300\n",
      "epoch:  2\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.9407 - acc: 0.6550 - prec: 0.8152 - recall: 0.9230 - prec_1: 0.5876 - recall_1: 0.3077 - prec_2: 0.7196 - recall_2: 0.7927 - prec_3: 0.6144 - recall_3: 0.5456 - prec_4: 0.5816 - recall_4: 0.6431 - prec_5: 0.5881 - recall_5: 0.7470 - f1_m: 0.3781 - recall_m: 0.2500 - precision_m: 0.8091 - precision: 0.8091 - recall_6: 0.2500 - f1score: 0.3781 - val_loss: 1.7717 - val_acc: 0.4730 - val_prec: 0.1762 - val_recall: 0.1659 - val_prec_1: 0.1762 - val_recall_1: 0.0318 - val_prec_2: 0.1806 - val_recall_2: 0.1738 - val_prec_3: 0.0641 - val_recall_3: 0.0275 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1104 - val_f1_m: 0.2196 - val_recall_m: 0.1802 - val_precision_m: 0.3455 - val_precision: 0.3455 - val_recall_6: 0.1802 - val_f1score: 0.2196\n",
      "Epoch 4/300\n",
      "epoch:  3\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 680us/step - loss: 0.8765 - acc: 0.6800 - prec: 0.8271 - recall: 0.9282 - prec_1: 0.5987 - recall_1: 0.4956 - prec_2: 0.7644 - recall_2: 0.7776 - prec_3: 0.6262 - recall_3: 0.5351 - prec_4: 0.5852 - recall_4: 0.6709 - prec_5: 0.6664 - recall_5: 0.6961 - f1_m: 0.4600 - recall_m: 0.3295 - precision_m: 0.7727 - precision: 0.7727 - recall_6: 0.3295 - f1score: 0.4600 - val_loss: 1.6020 - val_acc: 0.5013 - val_prec: 0.1762 - val_recall: 0.1507 - val_prec_1: 0.1634 - val_recall_1: 0.0479 - val_prec_2: 0.1799 - val_recall_2: 0.1652 - val_prec_3: 0.0480 - val_recall_3: 0.0289 - val_prec_4: 0.0801 - val_recall_4: 0.0178 - val_prec_5: 0.1672 - val_recall_5: 0.1236 - val_f1_m: 0.2414 - val_recall_m: 0.1987 - val_precision_m: 0.3724 - val_precision: 0.3724 - val_recall_6: 0.1987 - val_f1score: 0.2414\n",
      "Epoch 5/300\n",
      "epoch:  4\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.8422 - acc: 0.6745 - prec: 0.8487 - recall: 0.9238 - prec_1: 0.6025 - recall_1: 0.4391 - prec_2: 0.7704 - recall_2: 0.7305 - prec_3: 0.6462 - recall_3: 0.5202 - prec_4: 0.5921 - recall_4: 0.7002 - prec_5: 0.6074 - recall_5: 0.7728 - f1_m: 0.5037 - recall_m: 0.3764 - precision_m: 0.7716 - precision: 0.7716 - recall_6: 0.3764 - f1score: 0.5037 - val_loss: 1.0259 - val_acc: 0.6006 - val_prec: 0.1756 - val_recall: 0.1686 - val_prec_1: 0.1762 - val_recall_1: 0.0606 - val_prec_2: 0.1804 - val_recall_2: 0.1206 - val_prec_3: 0.1697 - val_recall_3: 0.1424 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1408 - val_f1_m: 0.4187 - val_recall_m: 0.3619 - val_precision_m: 0.5907 - val_precision: 0.5907 - val_recall_6: 0.3619 - val_f1score: 0.4187\n",
      "Epoch 6/300\n",
      "epoch:  5\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.8062 - acc: 0.6865 - prec: 0.8617 - recall: 0.9192 - prec_1: 0.6021 - recall_1: 0.4874 - prec_2: 0.7797 - recall_2: 0.7455 - prec_3: 0.6642 - recall_3: 0.4917 - prec_4: 0.5844 - recall_4: 0.7398 - prec_5: 0.6406 - recall_5: 0.7611 - f1_m: 0.5438 - recall_m: 0.4222 - precision_m: 0.7719 - precision: 0.7719 - recall_6: 0.4222 - f1score: 0.5438 - val_loss: 1.9436 - val_acc: 0.4810 - val_prec: 0.1762 - val_recall: 0.1433 - val_prec_1: 0.1716 - val_recall_1: 0.1347 - val_prec_2: 0.1818 - val_recall_2: 0.1416 - val_prec_3: 0.0160 - val_recall_3: 0.0018 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0894 - val_f1_m: 0.2598 - val_recall_m: 0.2115 - val_precision_m: 0.4636 - val_precision: 0.4636 - val_recall_6: 0.2115 - val_f1score: 0.2598\n",
      "Epoch 7/300\n",
      "epoch:  6\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.7859 - acc: 0.6910 - prec: 0.8620 - recall: 0.9279 - prec_1: 0.6240 - recall_1: 0.4861 - prec_2: 0.7731 - recall_2: 0.7483 - prec_3: 0.6532 - recall_3: 0.5337 - prec_4: 0.5989 - recall_4: 0.7031 - prec_5: 0.6459 - recall_5: 0.7852 - f1_m: 0.5777 - recall_m: 0.4588 - precision_m: 0.7866 - precision: 0.7866 - recall_6: 0.4588 - f1score: 0.5777 - val_loss: 1.0353 - val_acc: 0.5773 - val_prec: 0.1755 - val_recall: 0.1657 - val_prec_1: 0.1738 - val_recall_1: 0.1324 - val_prec_2: 0.1818 - val_recall_2: 0.1510 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1662 - val_recall_4: 0.0878 - val_prec_5: 0.1672 - val_recall_5: 0.0842 - val_f1_m: 0.2997 - val_recall_m: 0.2593 - val_precision_m: 0.4557 - val_precision: 0.4557 - val_recall_6: 0.2593 - val_f1score: 0.2997\n",
      "Epoch 8/300\n",
      "epoch:  7\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.7647 - acc: 0.6957 - prec: 0.8844 - recall: 0.9190 - prec_1: 0.6065 - recall_1: 0.5095 - prec_2: 0.7877 - recall_2: 0.7388 - prec_3: 0.6617 - recall_3: 0.5517 - prec_4: 0.6103 - recall_4: 0.7013 - prec_5: 0.6512 - recall_5: 0.7897 - f1_m: 0.5932 - recall_m: 0.4786 - precision_m: 0.7858 - precision: 0.7858 - recall_6: 0.4786 - f1score: 0.5932 - val_loss: 1.5321 - val_acc: 0.5113 - val_prec: 0.1762 - val_recall: 0.1626 - val_prec_1: 0.1731 - val_recall_1: 0.1254 - val_prec_2: 0.1832 - val_recall_2: 0.1508 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1072 - val_f1_m: 0.3272 - val_recall_m: 0.2800 - val_precision_m: 0.4853 - val_precision: 0.4853 - val_recall_6: 0.2800 - val_f1score: 0.3272\n",
      "Epoch 9/300\n",
      "epoch:  8\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.7486 - acc: 0.6984 - prec: 0.8791 - recall: 0.9237 - prec_1: 0.6143 - recall_1: 0.5459 - prec_2: 0.7869 - recall_2: 0.7610 - prec_3: 0.6535 - recall_3: 0.5106 - prec_4: 0.5890 - recall_4: 0.7154 - prec_5: 0.6822 - recall_5: 0.7711 - f1_m: 0.6063 - recall_m: 0.4985 - precision_m: 0.7774 - precision: 0.7774 - recall_6: 0.4985 - f1score: 0.6063 - val_loss: 2.1639 - val_acc: 0.4965 - val_prec: 0.1756 - val_recall: 0.1663 - val_prec_1: 0.1742 - val_recall_1: 0.1191 - val_prec_2: 0.1819 - val_recall_2: 0.1330 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1075 - val_f1_m: 0.2881 - val_recall_m: 0.2510 - val_precision_m: 0.4241 - val_precision: 0.4241 - val_recall_6: 0.2510 - val_f1score: 0.2881\n",
      "Epoch 10/300\n",
      "epoch:  9\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.7332 - acc: 0.7048 - prec: 0.8813 - recall: 0.9275 - prec_1: 0.6239 - recall_1: 0.5796 - prec_2: 0.8039 - recall_2: 0.7518 - prec_3: 0.6424 - recall_3: 0.5303 - prec_4: 0.5931 - recall_4: 0.6885 - prec_5: 0.6939 - recall_5: 0.7717 - f1_m: 0.6154 - recall_m: 0.5097 - precision_m: 0.7811 - precision: 0.7811 - recall_6: 0.5097 - f1score: 0.6154 - val_loss: 2.4764 - val_acc: 0.4975 - val_prec: 0.1756 - val_recall: 0.1671 - val_prec_1: 0.1740 - val_recall_1: 0.0825 - val_prec_2: 0.1808 - val_recall_2: 0.1442 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1321 - val_f1_m: 0.3751 - val_recall_m: 0.3321 - val_precision_m: 0.4706 - val_precision: 0.4706 - val_recall_6: 0.3321 - val_f1score: 0.3751\n",
      "Epoch 11/300\n",
      "epoch:  10\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.7186 - acc: 0.7121 - prec: 0.8893 - recall: 0.9348 - prec_1: 0.6539 - recall_1: 0.5791 - prec_2: 0.7965 - recall_2: 0.7573 - prec_3: 0.6584 - recall_3: 0.5462 - prec_4: 0.6023 - recall_4: 0.6994 - prec_5: 0.6892 - recall_5: 0.8051 - f1_m: 0.6312 - recall_m: 0.5297 - precision_m: 0.7844 - precision: 0.7844 - recall_6: 0.5297 - f1score: 0.6312 - val_loss: 0.7998 - val_acc: 0.6321 - val_prec: 0.1762 - val_recall: 0.1635 - val_prec_1: 0.1745 - val_recall_1: 0.0763 - val_prec_2: 0.1830 - val_recall_2: 0.1050 - val_prec_3: 0.0320 - val_recall_3: 0.0068 - val_prec_4: 0.1822 - val_recall_4: 0.1737 - val_prec_5: 0.1672 - val_recall_5: 0.1483 - val_f1_m: 0.5149 - val_recall_m: 0.4585 - val_precision_m: 0.6341 - val_precision: 0.6341 - val_recall_6: 0.4585 - val_f1score: 0.5149\n",
      "Epoch 12/300\n",
      "epoch:  11\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.7082 - acc: 0.7145 - prec: 0.8849 - recall: 0.9292 - prec_1: 0.6311 - recall_1: 0.5455 - prec_2: 0.7866 - recall_2: 0.7618 - prec_3: 0.6662 - recall_3: 0.5938 - prec_4: 0.6222 - recall_4: 0.6936 - prec_5: 0.6961 - recall_5: 0.7954 - f1_m: 0.6420 - recall_m: 0.5420 - precision_m: 0.7919 - precision: 0.7919 - recall_6: 0.5420 - f1score: 0.6420 - val_loss: 1.8804 - val_acc: 0.4489 - val_prec: 0.1762 - val_recall: 0.1569 - val_prec_1: 0.1441 - val_recall_1: 0.0362 - val_prec_2: 0.1815 - val_recall_2: 0.1186 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1614 - val_f1_m: 0.3924 - val_recall_m: 0.3589 - val_precision_m: 0.4503 - val_precision: 0.4503 - val_recall_6: 0.3589 - val_f1score: 0.3924\n",
      "Epoch 13/300\n",
      "epoch:  12\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 666us/step - loss: 0.6937 - acc: 0.7200 - prec: 0.9098 - recall: 0.9255 - prec_1: 0.6426 - recall_1: 0.6361 - prec_2: 0.8099 - recall_2: 0.7668 - prec_3: 0.6658 - recall_3: 0.5493 - prec_4: 0.6048 - recall_4: 0.7169 - prec_5: 0.7319 - recall_5: 0.7798 - f1_m: 0.6480 - recall_m: 0.5520 - precision_m: 0.7873 - precision: 0.7873 - recall_6: 0.5520 - f1score: 0.6480 - val_loss: 0.8210 - val_acc: 0.6156 - val_prec: 0.1762 - val_recall: 0.1633 - val_prec_1: 0.1731 - val_recall_1: 0.1635 - val_prec_2: 0.1810 - val_recall_2: 0.1188 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1822 - val_recall_4: 0.1897 - val_prec_5: 0.1351 - val_recall_5: 0.0336 - val_f1_m: 0.5577 - val_recall_m: 0.5125 - val_precision_m: 0.6329 - val_precision: 0.6329 - val_recall_6: 0.5125 - val_f1score: 0.5577\n",
      "Epoch 14/300\n",
      "epoch:  13\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.6844 - acc: 0.7294 - prec: 0.8991 - recall: 0.9256 - prec_1: 0.6466 - recall_1: 0.6505 - prec_2: 0.8283 - recall_2: 0.7740 - prec_3: 0.6690 - recall_3: 0.5567 - prec_4: 0.6110 - recall_4: 0.7100 - prec_5: 0.7431 - recall_5: 0.7948 - f1_m: 0.6600 - recall_m: 0.5688 - precision_m: 0.7894 - precision: 0.7894 - recall_6: 0.5688 - f1score: 0.6600 - val_loss: 5.1078 - val_acc: 0.4107 - val_prec: 0.1701 - val_recall: 0.1724 - val_prec_1: 0.1740 - val_recall_1: 0.0908 - val_prec_2: 0.1815 - val_recall_2: 0.0709 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0970 - val_f1_m: 0.3141 - val_recall_m: 0.2768 - val_precision_m: 0.4100 - val_precision: 0.4100 - val_recall_6: 0.2768 - val_f1score: 0.3141\n",
      "Epoch 15/300\n",
      "epoch:  14\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 679us/step - loss: 0.6768 - acc: 0.7286 - prec: 0.8928 - recall: 0.9249 - prec_1: 0.6671 - recall_1: 0.6329 - prec_2: 0.8186 - recall_2: 0.7822 - prec_3: 0.6537 - recall_3: 0.5232 - prec_4: 0.6016 - recall_4: 0.7192 - prec_5: 0.7470 - recall_5: 0.8167 - f1_m: 0.6596 - recall_m: 0.5711 - precision_m: 0.7842 - precision: 0.7842 - recall_6: 0.5711 - f1score: 0.6596 - val_loss: 1.1460 - val_acc: 0.6214 - val_prec: 0.1762 - val_recall: 0.1647 - val_prec_1: 0.1727 - val_recall_1: 0.1530 - val_prec_2: 0.1802 - val_recall_2: 0.1183 - val_prec_3: 0.1683 - val_recall_3: 0.1086 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1115 - val_f1_m: 0.5287 - val_recall_m: 0.4587 - val_precision_m: 0.6846 - val_precision: 0.6846 - val_recall_6: 0.4587 - val_f1score: 0.5287\n",
      "Epoch 16/300\n",
      "epoch:  15\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.6610 - acc: 0.7379 - prec: 0.9038 - recall: 0.9256 - prec_1: 0.6767 - recall_1: 0.6295 - prec_2: 0.8268 - recall_2: 0.7880 - prec_3: 0.6768 - recall_3: 0.5761 - prec_4: 0.6167 - recall_4: 0.7052 - prec_5: 0.7513 - recall_5: 0.8469 - f1_m: 0.6821 - recall_m: 0.5975 - precision_m: 0.7970 - precision: 0.7970 - recall_6: 0.5975 - f1score: 0.6821 - val_loss: 2.0118 - val_acc: 0.4775 - val_prec: 0.1762 - val_recall: 0.1691 - val_prec_1: 0.1602 - val_recall_1: 0.0539 - val_prec_2: 0.1794 - val_recall_2: 0.1159 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1596 - val_f1_m: 0.4176 - val_recall_m: 0.3911 - val_precision_m: 0.4598 - val_precision: 0.4598 - val_recall_6: 0.3911 - val_f1score: 0.4176\n",
      "Epoch 17/300\n",
      "epoch:  16\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.6528 - acc: 0.7383 - prec: 0.9132 - recall: 0.9255 - prec_1: 0.6808 - recall_1: 0.6709 - prec_2: 0.8273 - recall_2: 0.7962 - prec_3: 0.6591 - recall_3: 0.5596 - prec_4: 0.6034 - recall_4: 0.6967 - prec_5: 0.7657 - recall_5: 0.8304 - f1_m: 0.6829 - recall_m: 0.6026 - precision_m: 0.7904 - precision: 0.7904 - recall_6: 0.6026 - f1score: 0.6829 - val_loss: 2.1891 - val_acc: 0.4957 - val_prec: 0.1740 - val_recall: 0.1708 - val_prec_1: 0.1682 - val_recall_1: 0.0618 - val_prec_2: 0.1802 - val_recall_2: 0.1360 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1531 - val_f1_m: 0.4187 - val_recall_m: 0.3849 - val_precision_m: 0.4746 - val_precision: 0.4746 - val_recall_6: 0.3849 - val_f1score: 0.4187\n",
      "Epoch 18/300\n",
      "epoch:  17\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.6414 - acc: 0.7400 - prec: 0.9013 - recall: 0.9305 - prec_1: 0.6811 - recall_1: 0.6463 - prec_2: 0.8274 - recall_2: 0.7963 - prec_3: 0.6485 - recall_3: 0.5449 - prec_4: 0.6018 - recall_4: 0.7019 - prec_5: 0.7733 - recall_5: 0.8470 - f1_m: 0.6927 - recall_m: 0.6166 - precision_m: 0.7926 - precision: 0.7926 - recall_6: 0.6166 - f1score: 0.6927 - val_loss: 4.0402 - val_acc: 0.5015 - val_prec: 0.1750 - val_recall: 0.1693 - val_prec_1: 0.1733 - val_recall_1: 0.1377 - val_prec_2: 0.1786 - val_recall_2: 0.1371 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0887 - val_f1_m: 0.4072 - val_recall_m: 0.3566 - val_precision_m: 0.5338 - val_precision: 0.5338 - val_recall_6: 0.3566 - val_f1score: 0.4072\n",
      "Epoch 19/300\n",
      "epoch:  18\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.6326 - acc: 0.7466 - prec: 0.9093 - recall: 0.9328 - prec_1: 0.6799 - recall_1: 0.6939 - prec_2: 0.8392 - recall_2: 0.7901 - prec_3: 0.6510 - recall_3: 0.5863 - prec_4: 0.6197 - recall_4: 0.6785 - prec_5: 0.7898 - recall_5: 0.8370 - f1_m: 0.7054 - recall_m: 0.6331 - precision_m: 0.7985 - precision: 0.7985 - recall_6: 0.6331 - f1score: 0.7054 - val_loss: 1.7668 - val_acc: 0.5253 - val_prec: 0.1756 - val_recall: 0.1712 - val_prec_1: 0.1762 - val_recall_1: 0.0873 - val_prec_2: 0.1837 - val_recall_2: 0.1494 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1498 - val_f1_m: 0.4607 - val_recall_m: 0.4194 - val_precision_m: 0.5383 - val_precision: 0.5383 - val_recall_6: 0.4194 - val_f1score: 0.4607\n",
      "Epoch 20/300\n",
      "epoch:  19\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 686us/step - loss: 0.6255 - acc: 0.7495 - prec: 0.9060 - recall: 0.9267 - prec_1: 0.6998 - recall_1: 0.6792 - prec_2: 0.8473 - recall_2: 0.8118 - prec_3: 0.6674 - recall_3: 0.5553 - prec_4: 0.6118 - recall_4: 0.7114 - prec_5: 0.7895 - recall_5: 0.8557 - f1_m: 0.7068 - recall_m: 0.6374 - precision_m: 0.7949 - precision: 0.7949 - recall_6: 0.6374 - f1score: 0.7068 - val_loss: 3.1504 - val_acc: 0.5248 - val_prec: 0.1762 - val_recall: 0.1658 - val_prec_1: 0.1682 - val_recall_1: 0.1107 - val_prec_2: 0.1815 - val_recall_2: 0.1564 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1226 - val_f1_m: 0.4761 - val_recall_m: 0.4324 - val_precision_m: 0.5527 - val_precision: 0.5527 - val_recall_6: 0.4324 - val_f1score: 0.4761\n",
      "Epoch 21/300\n",
      "epoch:  20\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.6140 - acc: 0.7587 - prec: 0.9123 - recall: 0.9250 - prec_1: 0.7085 - recall_1: 0.6924 - prec_2: 0.8370 - recall_2: 0.8222 - prec_3: 0.6724 - recall_3: 0.5865 - prec_4: 0.6260 - recall_4: 0.7015 - prec_5: 0.8064 - recall_5: 0.8515 - f1_m: 0.7212 - recall_m: 0.6543 - precision_m: 0.8051 - precision: 0.8051 - recall_6: 0.6543 - f1score: 0.7212 - val_loss: 1.9899 - val_acc: 0.5193 - val_prec: 0.1762 - val_recall: 0.1282 - val_prec_1: 0.1694 - val_recall_1: 0.0995 - val_prec_2: 0.1869 - val_recall_2: 0.1780 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1602 - val_recall_4: 0.0521 - val_prec_5: 0.1672 - val_recall_5: 0.0973 - val_f1_m: 0.4584 - val_recall_m: 0.4154 - val_precision_m: 0.5449 - val_precision: 0.5449 - val_recall_6: 0.4154 - val_f1score: 0.4584\n",
      "Epoch 22/300\n",
      "epoch:  21\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.6121 - acc: 0.7536 - prec: 0.9102 - recall: 0.9349 - prec_1: 0.7051 - recall_1: 0.6990 - prec_2: 0.8461 - recall_2: 0.7971 - prec_3: 0.6470 - recall_3: 0.5684 - prec_4: 0.6059 - recall_4: 0.6890 - prec_5: 0.8066 - recall_5: 0.8624 - f1_m: 0.7211 - recall_m: 0.6582 - precision_m: 0.7987 - precision: 0.7987 - recall_6: 0.6582 - f1score: 0.7211 - val_loss: 1.0678 - val_acc: 0.5533 - val_prec: 0.1762 - val_recall: 0.1373 - val_prec_1: 0.1704 - val_recall_1: 0.1080 - val_prec_2: 0.1869 - val_recall_2: 0.1437 - val_prec_3: 0.0480 - val_recall_3: 0.0183 - val_prec_4: 0.1441 - val_recall_4: 0.0348 - val_prec_5: 0.1672 - val_recall_5: 0.1401 - val_f1_m: 0.4480 - val_recall_m: 0.3921 - val_precision_m: 0.5492 - val_precision: 0.5492 - val_recall_6: 0.3921 - val_f1score: 0.4480\n",
      "Epoch 23/300\n",
      "epoch:  22\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.6080 - acc: 0.7500 - prec: 0.8994 - recall: 0.9247 - prec_1: 0.7064 - recall_1: 0.6920 - prec_2: 0.8294 - recall_2: 0.8036 - prec_3: 0.6464 - recall_3: 0.5870 - prec_4: 0.6109 - recall_4: 0.6732 - prec_5: 0.8028 - recall_5: 0.8470 - f1_m: 0.7198 - recall_m: 0.6569 - precision_m: 0.7979 - precision: 0.7979 - recall_6: 0.6569 - f1score: 0.7198 - val_loss: 3.1464 - val_acc: 0.4992 - val_prec: 0.1732 - val_recall: 0.1719 - val_prec_1: 0.1749 - val_recall_1: 0.1170 - val_prec_2: 0.1826 - val_recall_2: 0.1000 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1325 - val_f1_m: 0.4671 - val_recall_m: 0.4229 - val_precision_m: 0.5390 - val_precision: 0.5390 - val_recall_6: 0.4229 - val_f1score: 0.4671\n",
      "Epoch 24/300\n",
      "epoch:  23\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.5951 - acc: 0.7592 - prec: 0.9208 - recall: 0.9273 - prec_1: 0.7068 - recall_1: 0.6982 - prec_2: 0.8489 - recall_2: 0.8101 - prec_3: 0.6712 - recall_3: 0.5710 - prec_4: 0.6166 - recall_4: 0.7111 - prec_5: 0.8083 - recall_5: 0.8634 - f1_m: 0.7322 - recall_m: 0.6734 - precision_m: 0.8042 - precision: 0.8042 - recall_6: 0.6734 - f1score: 0.7322 - val_loss: 2.8372 - val_acc: 0.5053 - val_prec: 0.1762 - val_recall: 0.1679 - val_prec_1: 0.1651 - val_recall_1: 0.0831 - val_prec_2: 0.1828 - val_recall_2: 0.1347 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1456 - val_f1_m: 0.4539 - val_recall_m: 0.4154 - val_precision_m: 0.5190 - val_precision: 0.5190 - val_recall_6: 0.4154 - val_f1score: 0.4539\n",
      "Epoch 25/300\n",
      "epoch:  24\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.5966 - acc: 0.7575 - prec: 0.9085 - recall: 0.9323 - prec_1: 0.7246 - recall_1: 0.7219 - prec_2: 0.8433 - recall_2: 0.8163 - prec_3: 0.6477 - recall_3: 0.5418 - prec_4: 0.5937 - recall_4: 0.6935 - prec_5: 0.8367 - recall_5: 0.8731 - f1_m: 0.7275 - recall_m: 0.6723 - precision_m: 0.7944 - precision: 0.7944 - recall_6: 0.6723 - f1score: 0.7275 - val_loss: 2.0671 - val_acc: 0.5470 - val_prec: 0.1749 - val_recall: 0.1682 - val_prec_1: 0.1720 - val_recall_1: 0.1202 - val_prec_2: 0.1793 - val_recall_2: 0.1594 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0320 - val_recall_4: 0.0093 - val_prec_5: 0.1672 - val_recall_5: 0.1213 - val_f1_m: 0.4597 - val_recall_m: 0.4077 - val_precision_m: 0.5776 - val_precision: 0.5776 - val_recall_6: 0.4077 - val_f1score: 0.4597\n",
      "Epoch 26/300\n",
      "epoch:  25\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.5792 - acc: 0.7659 - prec: 0.9230 - recall: 0.9231 - prec_1: 0.7111 - recall_1: 0.7221 - prec_2: 0.8593 - recall_2: 0.8137 - prec_3: 0.6691 - recall_3: 0.5601 - prec_4: 0.6157 - recall_4: 0.7226 - prec_5: 0.8198 - recall_5: 0.8836 - f1_m: 0.7399 - recall_m: 0.6855 - precision_m: 0.8051 - precision: 0.8051 - recall_6: 0.6855 - f1score: 0.7399 - val_loss: 5.4719 - val_acc: 0.4785 - val_prec: 0.1701 - val_recall: 0.1727 - val_prec_1: 0.1739 - val_recall_1: 0.0874 - val_prec_2: 0.1808 - val_recall_2: 0.1088 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1313 - val_f1_m: 0.4308 - val_recall_m: 0.3909 - val_precision_m: 0.4974 - val_precision: 0.4974 - val_recall_6: 0.3909 - val_f1score: 0.4308\n",
      "Epoch 27/300\n",
      "epoch:  26\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.5796 - acc: 0.7696 - prec: 0.9038 - recall: 0.9288 - prec_1: 0.7344 - recall_1: 0.7265 - prec_2: 0.8661 - recall_2: 0.8176 - prec_3: 0.6713 - recall_3: 0.5758 - prec_4: 0.6210 - recall_4: 0.7165 - prec_5: 0.8209 - recall_5: 0.8878 - f1_m: 0.7415 - recall_m: 0.6887 - precision_m: 0.8047 - precision: 0.8047 - recall_6: 0.6887 - f1score: 0.7415 - val_loss: 5.0296 - val_acc: 0.4282 - val_prec: 0.1762 - val_recall: 0.1644 - val_prec_1: 0.1281 - val_recall_1: 0.0285 - val_prec_2: 0.1602 - val_recall_2: 0.0912 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1624 - val_f1_m: 0.4125 - val_recall_m: 0.3996 - val_precision_m: 0.4289 - val_precision: 0.4289 - val_recall_6: 0.3996 - val_f1score: 0.4125\n",
      "Epoch 28/300\n",
      "epoch:  27\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.5741 - acc: 0.7652 - prec: 0.9193 - recall: 0.9374 - prec_1: 0.7310 - recall_1: 0.7317 - prec_2: 0.8558 - recall_2: 0.8218 - prec_3: 0.6489 - recall_3: 0.5543 - prec_4: 0.6023 - recall_4: 0.6958 - prec_5: 0.8340 - recall_5: 0.8890 - f1_m: 0.7418 - recall_m: 0.6925 - precision_m: 0.7997 - precision: 0.7997 - recall_6: 0.6925 - f1score: 0.7418 - val_loss: 3.7453 - val_acc: 0.3936 - val_prec: 0.1745 - val_recall: 0.1727 - val_prec_1: 0.1735 - val_recall_1: 0.1372 - val_prec_2: 0.1858 - val_recall_2: 0.0889 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1121 - val_recall_5: 0.0216 - val_f1_m: 0.3826 - val_recall_m: 0.3709 - val_precision_m: 0.3970 - val_precision: 0.3970 - val_recall_6: 0.3709 - val_f1score: 0.3826\n",
      "Epoch 29/300\n",
      "epoch:  28\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.5684 - acc: 0.7665 - prec: 0.9092 - recall: 0.9327 - prec_1: 0.7309 - recall_1: 0.7189 - prec_2: 0.8482 - recall_2: 0.8327 - prec_3: 0.6632 - recall_3: 0.5691 - prec_4: 0.6179 - recall_4: 0.7134 - prec_5: 0.8249 - recall_5: 0.8773 - f1_m: 0.7456 - recall_m: 0.6968 - precision_m: 0.8031 - precision: 0.8031 - recall_6: 0.6968 - f1score: 0.7456 - val_loss: 0.9822 - val_acc: 0.5978 - val_prec: 0.1762 - val_recall: 0.1664 - val_prec_1: 0.1732 - val_recall_1: 0.1707 - val_prec_2: 0.1830 - val_recall_2: 0.1046 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1805 - val_recall_4: 0.1739 - val_prec_5: 0.1031 - val_recall_5: 0.0315 - val_f1_m: 0.5159 - val_recall_m: 0.4942 - val_precision_m: 0.5627 - val_precision: 0.5627 - val_recall_6: 0.4942 - val_f1score: 0.5159\n",
      "Epoch 30/300\n",
      "epoch:  29\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.5590 - acc: 0.7656 - prec: 0.9221 - recall: 0.9327 - prec_1: 0.7355 - recall_1: 0.7453 - prec_2: 0.8638 - recall_2: 0.8239 - prec_3: 0.6521 - recall_3: 0.5428 - prec_4: 0.6021 - recall_4: 0.7062 - prec_5: 0.8366 - recall_5: 0.8884 - f1_m: 0.7430 - recall_m: 0.6960 - precision_m: 0.7982 - precision: 0.7982 - recall_6: 0.6960 - f1score: 0.7430 - val_loss: 0.7842 - val_acc: 0.6994 - val_prec: 0.1762 - val_recall: 0.1647 - val_prec_1: 0.1748 - val_recall_1: 0.1444 - val_prec_2: 0.1879 - val_recall_2: 0.1586 - val_prec_3: 0.1281 - val_recall_3: 0.0669 - val_prec_4: 0.1602 - val_recall_4: 0.0906 - val_prec_5: 0.1672 - val_recall_5: 0.1113 - val_f1_m: 0.6687 - val_recall_m: 0.6139 - val_precision_m: 0.7574 - val_precision: 0.7574 - val_recall_6: 0.6139 - val_f1score: 0.6687\n",
      "Epoch 31/300\n",
      "epoch:  30\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 6s 689us/step - loss: 0.5560 - acc: 0.7707 - prec: 0.9209 - recall: 0.9308 - prec_1: 0.7464 - recall_1: 0.7403 - prec_2: 0.8611 - recall_2: 0.8355 - prec_3: 0.6530 - recall_3: 0.5610 - prec_4: 0.6118 - recall_4: 0.7022 - prec_5: 0.8359 - recall_5: 0.8949 - f1_m: 0.7476 - recall_m: 0.7013 - precision_m: 0.8014 - precision: 0.8014 - recall_6: 0.7013 - f1score: 0.7476 - val_loss: 3.7295 - val_acc: 0.4627 - val_prec: 0.1750 - val_recall: 0.1722 - val_prec_1: 0.1602 - val_recall_1: 0.0740 - val_prec_2: 0.1776 - val_recall_2: 0.1069 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1291 - val_f1_m: 0.4381 - val_recall_m: 0.4109 - val_precision_m: 0.4772 - val_precision: 0.4772 - val_recall_6: 0.4109 - val_f1score: 0.4381\n",
      "Epoch 32/300\n",
      "epoch:  31\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.5542 - acc: 0.7726 - prec: 0.9222 - recall: 0.9280 - prec_1: 0.7585 - recall_1: 0.7393 - prec_2: 0.8642 - recall_2: 0.8302 - prec_3: 0.6549 - recall_3: 0.5929 - prec_4: 0.6144 - recall_4: 0.6782 - prec_5: 0.8307 - recall_5: 0.8998 - f1_m: 0.7521 - recall_m: 0.7063 - precision_m: 0.8052 - precision: 0.8052 - recall_6: 0.7063 - f1score: 0.7521 - val_loss: 1.0493 - val_acc: 0.5430 - val_prec: 0.1762 - val_recall: 0.1221 - val_prec_1: 0.1716 - val_recall_1: 0.1714 - val_prec_2: 0.1762 - val_recall_2: 0.0818 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1822 - val_recall_4: 0.1642 - val_prec_5: 0.1672 - val_recall_5: 0.0520 - val_f1_m: 0.5103 - val_recall_m: 0.4755 - val_precision_m: 0.5710 - val_precision: 0.5710 - val_recall_6: 0.4755 - val_f1score: 0.5103\n",
      "Epoch 33/300\n",
      "epoch:  32\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.5507 - acc: 0.7736 - prec: 0.9250 - recall: 0.9289 - prec_1: 0.7396 - recall_1: 0.7561 - prec_2: 0.8713 - recall_2: 0.8240 - prec_3: 0.6673 - recall_3: 0.5434 - prec_4: 0.6102 - recall_4: 0.7228 - prec_5: 0.8439 - recall_5: 0.8901 - f1_m: 0.7566 - recall_m: 0.7141 - precision_m: 0.8056 - precision: 0.8056 - recall_6: 0.7141 - f1score: 0.7566 - val_loss: 1.5470 - val_acc: 0.3196 - val_prec: 0.0961 - val_recall: 0.0053 - val_prec_1: 0.1680 - val_recall_1: 0.0848 - val_prec_2: 0.1800 - val_recall_2: 0.1588 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1922 - val_recall_4: 0.0871 - val_prec_5: 0.1351 - val_recall_5: 0.0315 - val_f1_m: 0.2171 - val_recall_m: 0.1959 - val_precision_m: 0.2547 - val_precision: 0.2547 - val_recall_6: 0.1959 - val_f1score: 0.2171\n",
      "Epoch 34/300\n",
      "epoch:  33\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.5499 - acc: 0.7740 - prec: 0.9180 - recall: 0.9337 - prec_1: 0.7395 - recall_1: 0.7507 - prec_2: 0.8648 - recall_2: 0.8186 - prec_3: 0.6587 - recall_3: 0.5682 - prec_4: 0.6120 - recall_4: 0.6986 - prec_5: 0.8549 - recall_5: 0.8895 - f1_m: 0.7539 - recall_m: 0.7095 - precision_m: 0.8053 - precision: 0.8053 - recall_6: 0.7095 - f1score: 0.7539 - val_loss: 0.7366 - val_acc: 0.6789 - val_prec: 0.1762 - val_recall: 0.1686 - val_prec_1: 0.1762 - val_recall_1: 0.0909 - val_prec_2: 0.1820 - val_recall_2: 0.1825 - val_prec_3: 0.1702 - val_recall_3: 0.1461 - val_prec_4: 0.1121 - val_recall_4: 0.0423 - val_prec_5: 0.1672 - val_recall_5: 0.1030 - val_f1_m: 0.5904 - val_recall_m: 0.5455 - val_precision_m: 0.6783 - val_precision: 0.6783 - val_recall_6: 0.5455 - val_f1score: 0.5904\n",
      "Epoch 35/300\n",
      "epoch:  34\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.5380 - acc: 0.7770 - prec: 0.9235 - recall: 0.9292 - prec_1: 0.7579 - recall_1: 0.7694 - prec_2: 0.8711 - recall_2: 0.8354 - prec_3: 0.6597 - recall_3: 0.5587 - prec_4: 0.6047 - recall_4: 0.6991 - prec_5: 0.8572 - recall_5: 0.9002 - f1_m: 0.7608 - recall_m: 0.7213 - precision_m: 0.8063 - precision: 0.8063 - recall_6: 0.7213 - f1score: 0.7608 - val_loss: 0.7126 - val_acc: 0.7032 - val_prec: 0.1762 - val_recall: 0.1616 - val_prec_1: 0.1680 - val_recall_1: 0.1357 - val_prec_2: 0.1862 - val_recall_2: 0.1169 - val_prec_3: 0.1702 - val_recall_3: 0.1294 - val_prec_4: 0.1762 - val_recall_4: 0.0616 - val_prec_5: 0.1672 - val_recall_5: 0.1391 - val_f1_m: 0.6823 - val_recall_m: 0.6419 - val_precision_m: 0.7393 - val_precision: 0.7393 - val_recall_6: 0.6419 - val_f1score: 0.6823\n",
      "Epoch 36/300\n",
      "epoch:  35\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.5326 - acc: 0.7860 - prec: 0.9245 - recall: 0.9312 - prec_1: 0.7634 - recall_1: 0.7701 - prec_2: 0.8791 - recall_2: 0.8460 - prec_3: 0.6869 - recall_3: 0.5851 - prec_4: 0.6252 - recall_4: 0.7201 - prec_5: 0.8483 - recall_5: 0.9011 - f1_m: 0.7678 - recall_m: 0.7266 - precision_m: 0.8148 - precision: 0.8148 - recall_6: 0.7266 - f1score: 0.7678 - val_loss: 2.6036 - val_acc: 0.4447 - val_prec: 0.1750 - val_recall: 0.1709 - val_prec_1: 0.1738 - val_recall_1: 0.1390 - val_prec_2: 0.1842 - val_recall_2: 0.1359 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0160 - val_recall_4: 0.0050 - val_prec_5: 0.0961 - val_recall_5: 0.0270 - val_f1_m: 0.4287 - val_recall_m: 0.4039 - val_precision_m: 0.4622 - val_precision: 0.4622 - val_recall_6: 0.4039 - val_f1score: 0.4287\n",
      "Epoch 37/300\n",
      "epoch:  36\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 685us/step - loss: 0.5332 - acc: 0.7805 - prec: 0.9249 - recall: 0.9291 - prec_1: 0.7600 - recall_1: 0.7776 - prec_2: 0.8825 - recall_2: 0.8295 - prec_3: 0.6739 - recall_3: 0.5530 - prec_4: 0.6107 - recall_4: 0.7168 - prec_5: 0.8502 - recall_5: 0.9051 - f1_m: 0.7629 - recall_m: 0.7230 - precision_m: 0.8083 - precision: 0.8083 - recall_6: 0.7230 - f1score: 0.7629 - val_loss: 2.3231 - val_acc: 0.4770 - val_prec: 0.1724 - val_recall: 0.1737 - val_prec_1: 0.1735 - val_recall_1: 0.1196 - val_prec_2: 0.1842 - val_recall_2: 0.1205 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0922 - val_f1_m: 0.4494 - val_recall_m: 0.4139 - val_precision_m: 0.5040 - val_precision: 0.5040 - val_recall_6: 0.4139 - val_f1score: 0.4494\n",
      "Epoch 38/300\n",
      "epoch:  37\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.5245 - acc: 0.7846 - prec: 0.9239 - recall: 0.9371 - prec_1: 0.7706 - recall_1: 0.7827 - prec_2: 0.8795 - recall_2: 0.8358 - prec_3: 0.6706 - recall_3: 0.5599 - prec_4: 0.6139 - recall_4: 0.7210 - prec_5: 0.8644 - recall_5: 0.9035 - f1_m: 0.7688 - recall_m: 0.7314 - precision_m: 0.8109 - precision: 0.8109 - recall_6: 0.7314 - f1score: 0.7688 - val_loss: 1.0383 - val_acc: 0.6089 - val_prec: 0.1762 - val_recall: 0.1699 - val_prec_1: 0.1739 - val_recall_1: 0.1385 - val_prec_2: 0.1869 - val_recall_2: 0.1151 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1664 - val_recall_4: 0.1031 - val_prec_5: 0.1672 - val_recall_5: 0.1188 - val_f1_m: 0.5851 - val_recall_m: 0.5493 - val_precision_m: 0.6386 - val_precision: 0.6386 - val_recall_6: 0.5493 - val_f1score: 0.5851\n",
      "Epoch 39/300\n",
      "epoch:  38\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.5219 - acc: 0.7880 - prec: 0.9214 - recall: 0.9291 - prec_1: 0.7884 - recall_1: 0.7828 - prec_2: 0.8865 - recall_2: 0.8560 - prec_3: 0.6673 - recall_3: 0.5667 - prec_4: 0.6198 - recall_4: 0.7249 - prec_5: 0.8603 - recall_5: 0.9102 - f1_m: 0.7745 - recall_m: 0.7371 - precision_m: 0.8167 - precision: 0.8167 - recall_6: 0.7371 - f1score: 0.7745 - val_loss: 3.0873 - val_acc: 0.3654 - val_prec: 0.1667 - val_recall: 0.1749 - val_prec_1: 0.1602 - val_recall_1: 0.0380 - val_prec_2: 0.1860 - val_recall_2: 0.1484 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1191 - val_recall_5: 0.0360 - val_f1_m: 0.3305 - val_recall_m: 0.3073 - val_precision_m: 0.3686 - val_precision: 0.3686 - val_recall_6: 0.3073 - val_f1score: 0.3305\n",
      "Epoch 40/300\n",
      "epoch:  39\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.5334 - acc: 0.7825 - prec: 0.9202 - recall: 0.9359 - prec_1: 0.7692 - recall_1: 0.7627 - prec_2: 0.8763 - recall_2: 0.8404 - prec_3: 0.6680 - recall_3: 0.5950 - prec_4: 0.6292 - recall_4: 0.6948 - prec_5: 0.8561 - recall_5: 0.9105 - f1_m: 0.7654 - recall_m: 0.7243 - precision_m: 0.8125 - precision: 0.8125 - recall_6: 0.7243 - f1score: 0.7654 - val_loss: 2.4583 - val_acc: 0.4722 - val_prec: 0.1762 - val_recall: 0.1506 - val_prec_1: 0.1682 - val_recall_1: 0.0597 - val_prec_2: 0.1692 - val_recall_2: 0.1839 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1108 - val_f1_m: 0.4413 - val_recall_m: 0.4114 - val_precision_m: 0.4849 - val_precision: 0.4849 - val_recall_6: 0.4114 - val_f1score: 0.4413\n",
      "Epoch 41/300\n",
      "epoch:  40\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.5112 - acc: 0.7879 - prec: 0.9247 - recall: 0.9359 - prec_1: 0.7636 - recall_1: 0.7910 - prec_2: 0.8839 - recall_2: 0.8397 - prec_3: 0.6796 - recall_3: 0.5779 - prec_4: 0.6279 - recall_4: 0.7161 - prec_5: 0.8680 - recall_5: 0.8989 - f1_m: 0.7761 - recall_m: 0.7400 - precision_m: 0.8168 - precision: 0.8168 - recall_6: 0.7400 - f1score: 0.7761 - val_loss: 2.4840 - val_acc: 0.3514 - val_prec: 0.1762 - val_recall: 0.1362 - val_prec_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - val_prec_2: 0.1602 - val_recall_2: 0.0631 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1669 - val_f1_m: 0.3483 - val_recall_m: 0.3431 - val_precision_m: 0.3543 - val_precision: 0.3543 - val_recall_6: 0.3431 - val_f1score: 0.3483\n",
      "Epoch 42/300\n",
      "epoch:  41\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.5112 - acc: 0.7914 - prec: 0.9248 - recall: 0.9370 - prec_1: 0.7718 - recall_1: 0.7844 - prec_2: 0.8869 - recall_2: 0.8525 - prec_3: 0.6819 - recall_3: 0.5883 - prec_4: 0.6357 - recall_4: 0.7253 - prec_5: 0.8546 - recall_5: 0.9064 - f1_m: 0.7788 - recall_m: 0.7451 - precision_m: 0.8164 - precision: 0.8164 - recall_6: 0.7451 - f1score: 0.7788 - val_loss: 4.4887 - val_acc: 0.4219 - val_prec: 0.1676 - val_recall: 0.1737 - val_prec_1: 0.1597 - val_recall_1: 0.0376 - val_prec_2: 0.1842 - val_recall_2: 0.1221 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1120 - val_f1_m: 0.3879 - val_recall_m: 0.3629 - val_precision_m: 0.4262 - val_precision: 0.4262 - val_recall_6: 0.3629 - val_f1score: 0.3879\n",
      "Epoch 43/300\n",
      "epoch:  42\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 683us/step - loss: 0.5112 - acc: 0.7829 - prec: 0.9246 - recall: 0.9370 - prec_1: 0.7709 - recall_1: 0.7775 - prec_2: 0.8831 - recall_2: 0.8511 - prec_3: 0.6387 - recall_3: 0.5616 - prec_4: 0.6045 - recall_4: 0.6831 - prec_5: 0.8629 - recall_5: 0.9130 - f1_m: 0.7702 - recall_m: 0.7368 - precision_m: 0.8078 - precision: 0.8078 - recall_6: 0.7368 - f1score: 0.7702 - val_loss: 2.3413 - val_acc: 0.4617 - val_prec: 0.1738 - val_recall: 0.1691 - val_prec_1: 0.1602 - val_recall_1: 0.0287 - val_prec_2: 0.1797 - val_recall_2: 0.1274 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1593 - val_f1_m: 0.4441 - val_recall_m: 0.4287 - val_precision_m: 0.4645 - val_precision: 0.4645 - val_recall_6: 0.4287 - val_f1score: 0.4441\n",
      "Epoch 44/300\n",
      "epoch:  43\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.5086 - acc: 0.7896 - prec: 0.9280 - recall: 0.9383 - prec_1: 0.7822 - recall_1: 0.8011 - prec_2: 0.8880 - recall_2: 0.8411 - prec_3: 0.6689 - recall_3: 0.5825 - prec_4: 0.6239 - recall_4: 0.7052 - prec_5: 0.8668 - recall_5: 0.9120 - f1_m: 0.7768 - recall_m: 0.7431 - precision_m: 0.8144 - precision: 0.8144 - recall_6: 0.7431 - f1score: 0.7768 - val_loss: 4.9621 - val_acc: 0.4469 - val_prec: 0.1679 - val_recall: 0.1729 - val_prec_1: 0.1751 - val_recall_1: 0.0707 - val_prec_2: 0.1813 - val_recall_2: 0.1559 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0809 - val_f1_m: 0.4094 - val_recall_m: 0.3831 - val_precision_m: 0.4504 - val_precision: 0.4504 - val_recall_6: 0.3831 - val_f1score: 0.4094\n",
      "Epoch 45/300\n",
      "epoch:  44\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.5088 - acc: 0.7884 - prec: 0.9213 - recall: 0.9353 - prec_1: 0.7779 - recall_1: 0.7982 - prec_2: 0.8951 - recall_2: 0.8467 - prec_3: 0.6770 - recall_3: 0.5606 - prec_4: 0.6172 - recall_4: 0.7253 - prec_5: 0.8670 - recall_5: 0.9110 - f1_m: 0.7784 - recall_m: 0.7430 - precision_m: 0.8181 - precision: 0.8181 - recall_6: 0.7430 - f1score: 0.7784 - val_loss: 1.4224 - val_acc: 0.4119 - val_prec: 0.1762 - val_recall: 0.0435 - val_prec_1: 0.1679 - val_recall_1: 0.1581 - val_prec_2: 0.1865 - val_recall_2: 0.1565 - val_prec_3: 0.0801 - val_recall_3: 0.0153 - val_prec_4: 0.1602 - val_recall_4: 0.0473 - val_prec_5: 0.1351 - val_recall_5: 0.0210 - val_f1_m: 0.3406 - val_recall_m: 0.3368 - val_precision_m: 0.3451 - val_precision: 0.3451 - val_recall_6: 0.3368 - val_f1score: 0.3406\n",
      "Epoch 46/300\n",
      "epoch:  45\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.5013 - acc: 0.7934 - prec: 0.9138 - recall: 0.9383 - prec_1: 0.7866 - recall_1: 0.7946 - prec_2: 0.8903 - recall_2: 0.8545 - prec_3: 0.6624 - recall_3: 0.6078 - prec_4: 0.6317 - recall_4: 0.6900 - prec_5: 0.8748 - recall_5: 0.9114 - f1_m: 0.7834 - recall_m: 0.7500 - precision_m: 0.8207 - precision: 0.8207 - recall_6: 0.7500 - f1score: 0.7834 - val_loss: 1.1284 - val_acc: 0.5551 - val_prec: 0.1762 - val_recall: 0.1568 - val_prec_1: 0.1700 - val_recall_1: 0.0599 - val_prec_2: 0.1795 - val_recall_2: 0.1823 - val_prec_3: 0.1703 - val_recall_3: 0.0832 - val_prec_4: 0.1441 - val_recall_4: 0.0402 - val_prec_5: 0.1672 - val_recall_5: 0.0748 - val_f1_m: 0.4244 - val_recall_m: 0.3966 - val_precision_m: 0.5040 - val_precision: 0.5040 - val_recall_6: 0.3966 - val_f1score: 0.4244\n",
      "Epoch 47/300\n",
      "epoch:  46\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 667us/step - loss: 0.4966 - acc: 0.7925 - prec: 0.9226 - recall: 0.9425 - prec_1: 0.7807 - recall_1: 0.7798 - prec_2: 0.8959 - recall_2: 0.8486 - prec_3: 0.6844 - recall_3: 0.5758 - prec_4: 0.6309 - recall_4: 0.7298 - prec_5: 0.8628 - recall_5: 0.9152 - f1_m: 0.7832 - recall_m: 0.7511 - precision_m: 0.8189 - precision: 0.8189 - recall_6: 0.7511 - f1score: 0.7832 - val_loss: 1.0909 - val_acc: 0.6096 - val_prec: 0.1762 - val_recall: 0.1673 - val_prec_1: 0.1754 - val_recall_1: 0.0926 - val_prec_2: 0.1842 - val_recall_2: 0.1210 - val_prec_3: 0.1441 - val_recall_3: 0.0973 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1566 - val_f1_m: 0.5900 - val_recall_m: 0.5548 - val_precision_m: 0.6425 - val_precision: 0.6425 - val_recall_6: 0.5548 - val_f1score: 0.5900\n",
      "Epoch 48/300\n",
      "epoch:  47\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.4952 - acc: 0.7934 - prec: 0.9294 - recall: 0.9398 - prec_1: 0.7813 - recall_1: 0.7997 - prec_2: 0.9077 - recall_2: 0.8568 - prec_3: 0.6712 - recall_3: 0.5920 - prec_4: 0.6298 - recall_4: 0.7053 - prec_5: 0.8664 - recall_5: 0.9105 - f1_m: 0.7846 - recall_m: 0.7522 - precision_m: 0.8205 - precision: 0.8205 - recall_6: 0.7522 - f1score: 0.7846 - val_loss: 0.7524 - val_acc: 0.6914 - val_prec: 0.1755 - val_recall: 0.1668 - val_prec_1: 0.1738 - val_recall_1: 0.0947 - val_prec_2: 0.1806 - val_recall_2: 0.1752 - val_prec_3: 0.1702 - val_recall_3: 0.1469 - val_prec_4: 0.1121 - val_recall_4: 0.0410 - val_prec_5: 0.1672 - val_recall_5: 0.1175 - val_f1_m: 0.6421 - val_recall_m: 0.5998 - val_precision_m: 0.7096 - val_precision: 0.7096 - val_recall_6: 0.5998 - val_f1score: 0.6421\n",
      "Epoch 49/300\n",
      "epoch:  48\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.4914 - acc: 0.7916 - prec: 0.9168 - recall: 0.9358 - prec_1: 0.7906 - recall_1: 0.7926 - prec_2: 0.8909 - recall_2: 0.8630 - prec_3: 0.6741 - recall_3: 0.5808 - prec_4: 0.6231 - recall_4: 0.7063 - prec_5: 0.8600 - recall_5: 0.9084 - f1_m: 0.7854 - recall_m: 0.7544 - precision_m: 0.8198 - precision: 0.8198 - recall_6: 0.7544 - f1score: 0.7854 - val_loss: 3.0880 - val_acc: 0.4807 - val_prec: 0.1732 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.0572 - val_prec_2: 0.1899 - val_recall_2: 0.1271 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1528 - val_f1_m: 0.4589 - val_recall_m: 0.4342 - val_precision_m: 0.4927 - val_precision: 0.4927 - val_recall_6: 0.4342 - val_f1score: 0.4589\n",
      "Epoch 50/300\n",
      "epoch:  49\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.4812 - acc: 0.7990 - prec: 0.9322 - recall: 0.9435 - prec_1: 0.8016 - recall_1: 0.8062 - prec_2: 0.8974 - recall_2: 0.8667 - prec_3: 0.6620 - recall_3: 0.6011 - prec_4: 0.6249 - recall_4: 0.6840 - prec_5: 0.8808 - recall_5: 0.9209 - f1_m: 0.7914 - recall_m: 0.7617 - precision_m: 0.8240 - precision: 0.8240 - recall_6: 0.7617 - f1score: 0.7914 - val_loss: 1.7174 - val_acc: 0.4927 - val_prec: 0.1762 - val_recall: 0.1428 - val_prec_1: 0.1720 - val_recall_1: 0.1579 - val_prec_2: 0.1842 - val_recall_2: 0.1473 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0778 - val_f1_m: 0.4903 - val_recall_m: 0.4800 - val_precision_m: 0.5023 - val_precision: 0.5023 - val_recall_6: 0.4800 - val_f1score: 0.4903\n",
      "Epoch 51/300\n",
      "epoch:  50\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.4773 - acc: 0.7994 - prec: 0.9358 - recall: 0.9367 - prec_1: 0.7937 - recall_1: 0.8128 - prec_2: 0.9079 - recall_2: 0.8707 - prec_3: 0.6634 - recall_3: 0.5989 - prec_4: 0.6260 - recall_4: 0.6919 - prec_5: 0.8752 - recall_5: 0.9186 - f1_m: 0.7924 - recall_m: 0.7631 - precision_m: 0.8245 - precision: 0.8245 - recall_6: 0.7631 - f1score: 0.7924 - val_loss: 3.0536 - val_acc: 0.5135 - val_prec: 0.1762 - val_recall: 0.1650 - val_prec_1: 0.1731 - val_recall_1: 0.1499 - val_prec_2: 0.1890 - val_recall_2: 0.1587 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0766 - val_f1_m: 0.5048 - val_recall_m: 0.4832 - val_precision_m: 0.5340 - val_precision: 0.5340 - val_recall_6: 0.4832 - val_f1score: 0.5048\n",
      "Epoch 52/300\n",
      "epoch:  51\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.4816 - acc: 0.7926 - prec: 0.9326 - recall: 0.9355 - prec_1: 0.7756 - recall_1: 0.8019 - prec_2: 0.8915 - recall_2: 0.8559 - prec_3: 0.6636 - recall_3: 0.6089 - prec_4: 0.6302 - recall_4: 0.6846 - prec_5: 0.8696 - recall_5: 0.9164 - f1_m: 0.7863 - recall_m: 0.7579 - precision_m: 0.8175 - precision: 0.8175 - recall_6: 0.7579 - f1score: 0.7863 - val_loss: 1.8555 - val_acc: 0.5028 - val_prec: 0.1762 - val_recall: 0.1624 - val_prec_1: 0.1655 - val_recall_1: 0.0623 - val_prec_2: 0.1833 - val_recall_2: 0.1444 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0480 - val_recall_4: 0.0098 - val_prec_5: 0.1672 - val_recall_5: 0.1506 - val_f1_m: 0.4680 - val_recall_m: 0.4457 - val_precision_m: 0.4969 - val_precision: 0.4969 - val_recall_6: 0.4457 - val_f1score: 0.4680\n",
      "Epoch 53/300\n",
      "epoch:  52\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.4800 - acc: 0.7990 - prec: 0.9320 - recall: 0.9421 - prec_1: 0.7927 - recall_1: 0.8105 - prec_2: 0.8939 - recall_2: 0.8733 - prec_3: 0.6696 - recall_3: 0.5965 - prec_4: 0.6191 - recall_4: 0.6888 - prec_5: 0.8791 - recall_5: 0.9225 - f1_m: 0.7891 - recall_m: 0.7609 - precision_m: 0.8200 - precision: 0.8200 - recall_6: 0.7609 - f1score: 0.7891 - val_loss: 2.3975 - val_acc: 0.3994 - val_prec: 0.1667 - val_recall: 0.1742 - val_prec_1: 0.1578 - val_recall_1: 0.0932 - val_prec_2: 0.1858 - val_recall_2: 0.1050 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0801 - val_recall_4: 0.0240 - val_prec_5: 0.1351 - val_recall_5: 0.0273 - val_f1_m: 0.3871 - val_recall_m: 0.3709 - val_precision_m: 0.4078 - val_precision: 0.4078 - val_recall_6: 0.3709 - val_f1score: 0.3871\n",
      "Epoch 54/300\n",
      "epoch:  53\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 6s 690us/step - loss: 0.4708 - acc: 0.8041 - prec: 0.9292 - recall: 0.9440 - prec_1: 0.8063 - recall_1: 0.8175 - prec_2: 0.8989 - recall_2: 0.8631 - prec_3: 0.6748 - recall_3: 0.5988 - prec_4: 0.6328 - recall_4: 0.7005 - prec_5: 0.8802 - recall_5: 0.9258 - f1_m: 0.7962 - recall_m: 0.7672 - precision_m: 0.8281 - precision: 0.8281 - recall_6: 0.7672 - f1score: 0.7962 - val_loss: 0.6406 - val_acc: 0.7290 - val_prec: 0.1762 - val_recall: 0.1608 - val_prec_1: 0.1729 - val_recall_1: 0.1407 - val_prec_2: 0.1828 - val_recall_2: 0.1651 - val_prec_3: 0.1281 - val_recall_3: 0.0682 - val_prec_4: 0.1762 - val_recall_4: 0.1126 - val_prec_5: 0.1672 - val_recall_5: 0.1268 - val_f1_m: 0.7087 - val_recall_m: 0.6727 - val_precision_m: 0.7607 - val_precision: 0.7607 - val_recall_6: 0.6727 - val_f1score: 0.7087\n",
      "Epoch 55/300\n",
      "epoch:  54\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.4628 - acc: 0.8052 - prec: 0.9430 - recall: 0.9417 - prec_1: 0.8108 - recall_1: 0.8322 - prec_2: 0.9091 - recall_2: 0.8765 - prec_3: 0.6690 - recall_3: 0.6116 - prec_4: 0.6271 - recall_4: 0.6810 - prec_5: 0.8856 - recall_5: 0.9267 - f1_m: 0.7957 - recall_m: 0.7702 - precision_m: 0.8234 - precision: 0.8234 - recall_6: 0.7702 - f1score: 0.7957 - val_loss: 0.7279 - val_acc: 0.6704 - val_prec: 0.1713 - val_recall: 0.1696 - val_prec_1: 0.1715 - val_recall_1: 0.0932 - val_prec_2: 0.1837 - val_recall_2: 0.1689 - val_prec_3: 0.1281 - val_recall_3: 0.0494 - val_prec_4: 0.1762 - val_recall_4: 0.1164 - val_prec_5: 0.1672 - val_recall_5: 0.1158 - val_f1_m: 0.6181 - val_recall_m: 0.5711 - val_precision_m: 0.6981 - val_precision: 0.6981 - val_recall_6: 0.5711 - val_f1score: 0.6181\n",
      "Epoch 56/300\n",
      "epoch:  55\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.4627 - acc: 0.8033 - prec: 0.9325 - recall: 0.9416 - prec_1: 0.7964 - recall_1: 0.8227 - prec_2: 0.9067 - recall_2: 0.8661 - prec_3: 0.6829 - recall_3: 0.6039 - prec_4: 0.6304 - recall_4: 0.7046 - prec_5: 0.8787 - recall_5: 0.9141 - f1_m: 0.7991 - recall_m: 0.7737 - precision_m: 0.8266 - precision: 0.8266 - recall_6: 0.7737 - f1score: 0.7991 - val_loss: 1.6383 - val_acc: 0.5163 - val_prec: 0.1762 - val_recall: 0.1372 - val_prec_1: 0.1724 - val_recall_1: 0.1316 - val_prec_2: 0.1882 - val_recall_2: 0.1720 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1655 - val_recall_4: 0.1052 - val_prec_5: 0.0801 - val_recall_5: 0.0163 - val_f1_m: 0.5047 - val_recall_m: 0.4890 - val_precision_m: 0.5274 - val_precision: 0.5274 - val_recall_6: 0.4890 - val_f1score: 0.5047\n",
      "Epoch 57/300\n",
      "epoch:  56\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.4624 - acc: 0.8058 - prec: 0.9369 - recall: 0.9419 - prec_1: 0.8092 - recall_1: 0.8205 - prec_2: 0.9042 - recall_2: 0.8628 - prec_3: 0.6775 - recall_3: 0.5974 - prec_4: 0.6307 - recall_4: 0.7111 - prec_5: 0.8827 - recall_5: 0.9328 - f1_m: 0.7991 - recall_m: 0.7736 - precision_m: 0.8270 - precision: 0.8270 - recall_6: 0.7736 - f1score: 0.7991 - val_loss: 0.9409 - val_acc: 0.6547 - val_prec: 0.1762 - val_recall: 0.1696 - val_prec_1: 0.1705 - val_recall_1: 0.0758 - val_prec_2: 0.1837 - val_recall_2: 0.1769 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1762 - val_recall_4: 0.1224 - val_prec_5: 0.1672 - val_recall_5: 0.1571 - val_f1_m: 0.6358 - val_recall_m: 0.6099 - val_precision_m: 0.6703 - val_precision: 0.6703 - val_recall_6: 0.6099 - val_f1score: 0.6358\n",
      "Epoch 58/300\n",
      "epoch:  57\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.4585 - acc: 0.8091 - prec: 0.9267 - recall: 0.9428 - prec_1: 0.8100 - recall_1: 0.8256 - prec_2: 0.9140 - recall_2: 0.8690 - prec_3: 0.6753 - recall_3: 0.6030 - prec_4: 0.6394 - recall_4: 0.7081 - prec_5: 0.8852 - recall_5: 0.9234 - f1_m: 0.8011 - recall_m: 0.7757 - precision_m: 0.8287 - precision: 0.8287 - recall_6: 0.7757 - f1score: 0.8011 - val_loss: 0.9334 - val_acc: 0.5736 - val_prec: 0.1762 - val_recall: 0.1676 - val_prec_1: 0.1725 - val_recall_1: 0.1667 - val_prec_2: 0.1762 - val_recall_2: 0.0823 - val_prec_3: 0.1281 - val_recall_3: 0.0586 - val_prec_4: 0.1602 - val_recall_4: 0.0728 - val_prec_5: 0.1512 - val_recall_5: 0.0501 - val_f1_m: 0.5690 - val_recall_m: 0.5613 - val_precision_m: 0.5774 - val_precision: 0.5774 - val_recall_6: 0.5613 - val_f1score: 0.5690\n",
      "Epoch 59/300\n",
      "epoch:  58\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 679us/step - loss: 0.4578 - acc: 0.8108 - prec: 0.9403 - recall: 0.9438 - prec_1: 0.8129 - recall_1: 0.8196 - prec_2: 0.9145 - recall_2: 0.8782 - prec_3: 0.6899 - recall_3: 0.6118 - prec_4: 0.6477 - recall_4: 0.7259 - prec_5: 0.8763 - recall_5: 0.9265 - f1_m: 0.8042 - recall_m: 0.7789 - precision_m: 0.8318 - precision: 0.8318 - recall_6: 0.7789 - f1score: 0.8042 - val_loss: 5.5481 - val_acc: 0.4037 - val_prec: 0.1745 - val_recall: 0.1719 - val_prec_1: 0.1441 - val_recall_1: 0.0253 - val_prec_2: 0.1602 - val_recall_2: 0.0653 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1571 - val_f1_m: 0.3935 - val_recall_m: 0.3871 - val_precision_m: 0.4013 - val_precision: 0.4013 - val_recall_6: 0.3871 - val_f1score: 0.3935\n",
      "Epoch 60/300\n",
      "epoch:  59\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 679us/step - loss: 0.4523 - acc: 0.8133 - prec: 0.9372 - recall: 0.9425 - prec_1: 0.8139 - recall_1: 0.8293 - prec_2: 0.9181 - recall_2: 0.8755 - prec_3: 0.6984 - recall_3: 0.6002 - prec_4: 0.6485 - recall_4: 0.7372 - prec_5: 0.8850 - recall_5: 0.9264 - f1_m: 0.8089 - recall_m: 0.7835 - precision_m: 0.8366 - precision: 0.8366 - recall_6: 0.7835 - f1score: 0.8089 - val_loss: 1.8022 - val_acc: 0.4755 - val_prec: 0.1762 - val_recall: 0.1469 - val_prec_1: 0.1647 - val_recall_1: 0.0265 - val_prec_2: 0.1842 - val_recall_2: 0.1313 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0961 - val_recall_4: 0.0363 - val_prec_5: 0.1672 - val_recall_5: 0.1631 - val_f1_m: 0.4579 - val_recall_m: 0.4402 - val_precision_m: 0.4792 - val_precision: 0.4792 - val_recall_6: 0.4402 - val_f1score: 0.4579\n",
      "Epoch 61/300\n",
      "epoch:  60\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.4593 - acc: 0.8020 - prec: 0.9349 - recall: 0.9442 - prec_1: 0.8098 - recall_1: 0.8150 - prec_2: 0.8981 - recall_2: 0.8789 - prec_3: 0.6725 - recall_3: 0.5745 - prec_4: 0.6198 - recall_4: 0.7070 - prec_5: 0.8880 - recall_5: 0.9183 - f1_m: 0.7963 - recall_m: 0.7707 - precision_m: 0.8242 - precision: 0.8242 - recall_6: 0.7707 - f1score: 0.7963 - val_loss: 4.6295 - val_acc: 0.4017 - val_prec: 0.1669 - val_recall: 0.1752 - val_prec_1: 0.1757 - val_recall_1: 0.0467 - val_prec_2: 0.1842 - val_recall_2: 0.1534 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0586 - val_f1_m: 0.3800 - val_recall_m: 0.3599 - val_precision_m: 0.4104 - val_precision: 0.4104 - val_recall_6: 0.3599 - val_f1score: 0.3800\n",
      "Epoch 62/300\n",
      "epoch:  61\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.4541 - acc: 0.8078 - prec: 0.9389 - recall: 0.9438 - prec_1: 0.8123 - recall_1: 0.8268 - prec_2: 0.9130 - recall_2: 0.8744 - prec_3: 0.6799 - recall_3: 0.5971 - prec_4: 0.6305 - recall_4: 0.7090 - prec_5: 0.8839 - recall_5: 0.9330 - f1_m: 0.8004 - recall_m: 0.7762 - precision_m: 0.8270 - precision: 0.8270 - recall_6: 0.7762 - f1score: 0.8004 - val_loss: 1.1112 - val_acc: 0.6441 - val_prec: 0.1762 - val_recall: 0.1430 - val_prec_1: 0.1602 - val_recall_1: 0.0948 - val_prec_2: 0.1782 - val_recall_2: 0.1212 - val_prec_3: 0.1441 - val_recall_3: 0.0359 - val_prec_4: 0.1762 - val_recall_4: 0.1269 - val_prec_5: 0.1672 - val_recall_5: 0.1609 - val_f1_m: 0.6377 - val_recall_m: 0.6219 - val_precision_m: 0.6558 - val_precision: 0.6558 - val_recall_6: 0.6219 - val_f1score: 0.6377\n",
      "Epoch 63/300\n",
      "epoch:  62\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.4500 - acc: 0.8113 - prec: 0.9349 - recall: 0.9454 - prec_1: 0.8067 - recall_1: 0.8383 - prec_2: 0.9167 - recall_2: 0.8723 - prec_3: 0.6876 - recall_3: 0.5995 - prec_4: 0.6389 - recall_4: 0.7156 - prec_5: 0.8964 - recall_5: 0.9238 - f1_m: 0.8056 - recall_m: 0.7820 - precision_m: 0.8313 - precision: 0.8313 - recall_6: 0.7820 - f1score: 0.8056 - val_loss: 4.4945 - val_acc: 0.3351 - val_prec: 0.1281 - val_recall: 0.0114 - val_prec_1: 0.1656 - val_recall_1: 0.1327 - val_prec_2: 0.1922 - val_recall_2: 0.1110 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1025 - val_f1_m: 0.3326 - val_recall_m: 0.3266 - val_precision_m: 0.3391 - val_precision: 0.3391 - val_recall_6: 0.3266 - val_f1score: 0.3326\n",
      "Epoch 64/300\n",
      "epoch:  63\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.4535 - acc: 0.8037 - prec: 0.9301 - recall: 0.9441 - prec_1: 0.8032 - recall_1: 0.8275 - prec_2: 0.9074 - recall_2: 0.8739 - prec_3: 0.6574 - recall_3: 0.6108 - prec_4: 0.6264 - recall_4: 0.6748 - prec_5: 0.8952 - recall_5: 0.9212 - f1_m: 0.7999 - recall_m: 0.7752 - precision_m: 0.8266 - precision: 0.8266 - recall_6: 0.7752 - f1score: 0.7999 - val_loss: 2.1806 - val_acc: 0.5260 - val_prec: 0.1762 - val_recall: 0.1571 - val_prec_1: 0.1602 - val_recall_1: 0.0483 - val_prec_2: 0.1802 - val_recall_2: 0.1355 - val_prec_3: 0.1281 - val_recall_3: 0.0473 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1639 - val_f1_m: 0.5129 - val_recall_m: 0.5023 - val_precision_m: 0.5250 - val_precision: 0.5250 - val_recall_6: 0.5023 - val_f1score: 0.5129\n",
      "Epoch 65/300\n",
      "epoch:  64\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 680us/step - loss: 0.4482 - acc: 0.8092 - prec: 0.9313 - recall: 0.9456 - prec_1: 0.8251 - recall_1: 0.8244 - prec_2: 0.9061 - recall_2: 0.8862 - prec_3: 0.6650 - recall_3: 0.5898 - prec_4: 0.6264 - recall_4: 0.7013 - prec_5: 0.8978 - recall_5: 0.9312 - f1_m: 0.8026 - recall_m: 0.7792 - precision_m: 0.8280 - precision: 0.8280 - recall_6: 0.7792 - f1score: 0.8026 - val_loss: 4.0543 - val_acc: 0.4229 - val_prec: 0.1762 - val_recall: 0.1684 - val_prec_1: 0.1724 - val_recall_1: 0.1742 - val_prec_2: 0.1762 - val_recall_2: 0.0739 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1031 - val_recall_5: 0.0300 - val_f1_m: 0.4224 - val_recall_m: 0.4214 - val_precision_m: 0.4235 - val_precision: 0.4235 - val_recall_6: 0.4214 - val_f1score: 0.4224\n",
      "Epoch 66/300\n",
      "epoch:  65\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.4431 - acc: 0.8115 - prec: 0.9378 - recall: 0.9423 - prec_1: 0.8294 - recall_1: 0.8394 - prec_2: 0.9129 - recall_2: 0.8925 - prec_3: 0.6687 - recall_3: 0.6176 - prec_4: 0.6368 - recall_4: 0.6953 - prec_5: 0.8868 - recall_5: 0.9329 - f1_m: 0.8039 - recall_m: 0.7792 - precision_m: 0.8307 - precision: 0.8307 - recall_6: 0.7792 - f1score: 0.8039 - val_loss: 2.3006 - val_acc: 0.5240 - val_prec: 0.1762 - val_recall: 0.1677 - val_prec_1: 0.1598 - val_recall_1: 0.1081 - val_prec_2: 0.1780 - val_recall_2: 0.1188 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1529 - val_f1_m: 0.5225 - val_recall_m: 0.5090 - val_precision_m: 0.5379 - val_precision: 0.5379 - val_recall_6: 0.5090 - val_f1score: 0.5225\n",
      "Epoch 67/300\n",
      "epoch:  66\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 6s 687us/step - loss: 0.4384 - acc: 0.8135 - prec: 0.9408 - recall: 0.9462 - prec_1: 0.8306 - recall_1: 0.8485 - prec_2: 0.9110 - recall_2: 0.8896 - prec_3: 0.6803 - recall_3: 0.5961 - prec_4: 0.6305 - recall_4: 0.7125 - prec_5: 0.9044 - recall_5: 0.9293 - f1_m: 0.8095 - recall_m: 0.7879 - precision_m: 0.8328 - precision: 0.8328 - recall_6: 0.7879 - f1score: 0.8095 - val_loss: 2.5442 - val_acc: 0.5493 - val_prec: 0.1740 - val_recall: 0.1729 - val_prec_1: 0.1744 - val_recall_1: 0.1085 - val_prec_2: 0.1842 - val_recall_2: 0.1098 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1602 - val_recall_4: 0.0751 - val_prec_5: 0.1672 - val_recall_5: 0.1062 - val_f1_m: 0.5454 - val_recall_m: 0.5268 - val_precision_m: 0.5682 - val_precision: 0.5682 - val_recall_6: 0.5268 - val_f1score: 0.5454\n",
      "Epoch 68/300\n",
      "epoch:  67\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.4368 - acc: 0.8093 - prec: 0.9372 - recall: 0.9502 - prec_1: 0.8331 - recall_1: 0.8344 - prec_2: 0.9140 - recall_2: 0.8851 - prec_3: 0.6715 - recall_3: 0.6071 - prec_4: 0.6301 - recall_4: 0.6846 - prec_5: 0.8939 - recall_5: 0.9310 - f1_m: 0.8061 - recall_m: 0.7844 - precision_m: 0.8296 - precision: 0.8296 - recall_6: 0.7844 - f1score: 0.8061 - val_loss: 1.8919 - val_acc: 0.4129 - val_prec: 0.1762 - val_recall: 0.1255 - val_prec_1: 0.0641 - val_recall_1: 0.0046 - val_prec_2: 0.1762 - val_recall_2: 0.0911 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1281 - val_recall_4: 0.0526 - val_prec_5: 0.1672 - val_recall_5: 0.1667 - val_f1_m: 0.3639 - val_recall_m: 0.3524 - val_precision_m: 0.3845 - val_precision: 0.3845 - val_recall_6: 0.3524 - val_f1score: 0.3639\n",
      "Epoch 69/300\n",
      "epoch:  68\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.4303 - acc: 0.8213 - prec: 0.9427 - recall: 0.9440 - prec_1: 0.8215 - recall_1: 0.8394 - prec_2: 0.9185 - recall_2: 0.8895 - prec_3: 0.6971 - recall_3: 0.6204 - prec_4: 0.6501 - recall_4: 0.7243 - prec_5: 0.8990 - recall_5: 0.9240 - f1_m: 0.8174 - recall_m: 0.7949 - precision_m: 0.8416 - precision: 0.8416 - recall_6: 0.7949 - f1score: 0.8174 - val_loss: 0.6551 - val_acc: 0.7155 - val_prec: 0.1762 - val_recall: 0.1701 - val_prec_1: 0.1753 - val_recall_1: 0.1250 - val_prec_2: 0.1856 - val_recall_2: 0.1736 - val_prec_3: 0.0320 - val_recall_3: 0.0144 - val_prec_4: 0.1662 - val_recall_4: 0.1349 - val_prec_5: 0.1672 - val_recall_5: 0.1491 - val_f1_m: 0.6798 - val_recall_m: 0.6341 - val_precision_m: 0.7452 - val_precision: 0.7452 - val_recall_6: 0.6341 - val_f1score: 0.6798\n",
      "Epoch 70/300\n",
      "epoch:  69\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.4325 - acc: 0.8107 - prec: 0.9415 - recall: 0.9488 - prec_1: 0.8211 - recall_1: 0.8364 - prec_2: 0.9157 - recall_2: 0.8826 - prec_3: 0.6595 - recall_3: 0.6165 - prec_4: 0.6298 - recall_4: 0.6770 - prec_5: 0.8902 - recall_5: 0.9266 - f1_m: 0.8060 - recall_m: 0.7856 - precision_m: 0.8279 - precision: 0.8279 - recall_6: 0.7856 - f1score: 0.8060 - val_loss: 2.8020 - val_acc: 0.4132 - val_prec: 0.1698 - val_recall: 0.1727 - val_prec_1: 0.1121 - val_recall_1: 0.0113 - val_prec_2: 0.1809 - val_recall_2: 0.1775 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0868 - val_f1_m: 0.4080 - val_recall_m: 0.3926 - val_precision_m: 0.4279 - val_precision: 0.4279 - val_recall_6: 0.3926 - val_f1score: 0.4080\n",
      "Epoch 71/300\n",
      "epoch:  70\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 6s 690us/step - loss: 0.4341 - acc: 0.8155 - prec: 0.9359 - recall: 0.9459 - prec_1: 0.8363 - recall_1: 0.8411 - prec_2: 0.9205 - recall_2: 0.8874 - prec_3: 0.6781 - recall_3: 0.6019 - prec_4: 0.6333 - recall_4: 0.7120 - prec_5: 0.8884 - recall_5: 0.9284 - f1_m: 0.8103 - recall_m: 0.7884 - precision_m: 0.8339 - precision: 0.8339 - recall_6: 0.7884 - f1score: 0.8103 - val_loss: 1.1568 - val_acc: 0.5928 - val_prec: 0.1762 - val_recall: 0.1423 - val_prec_1: 0.1754 - val_recall_1: 0.1636 - val_prec_2: 0.1909 - val_recall_2: 0.1475 - val_prec_3: 0.1121 - val_recall_3: 0.0534 - val_prec_4: 0.1602 - val_recall_4: 0.0363 - val_prec_5: 0.1672 - val_recall_5: 0.0929 - val_f1_m: 0.5719 - val_recall_m: 0.5485 - val_precision_m: 0.6005 - val_precision: 0.6005 - val_recall_6: 0.5485 - val_f1score: 0.5719\n",
      "Epoch 72/300\n",
      "epoch:  71\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.4254 - acc: 0.8198 - prec: 0.9410 - recall: 0.9449 - prec_1: 0.8311 - recall_1: 0.8517 - prec_2: 0.9277 - recall_2: 0.8928 - prec_3: 0.6872 - recall_3: 0.6140 - prec_4: 0.6414 - recall_4: 0.7188 - prec_5: 0.8974 - recall_5: 0.9245 - f1_m: 0.8159 - recall_m: 0.7955 - precision_m: 0.8378 - precision: 0.8378 - recall_6: 0.7955 - f1score: 0.8159 - val_loss: 1.5395 - val_acc: 0.4762 - val_prec: 0.1762 - val_recall: 0.0858 - val_prec_1: 0.1690 - val_recall_1: 0.0901 - val_prec_2: 0.1922 - val_recall_2: 0.1246 - val_prec_3: 0.1121 - val_recall_3: 0.0509 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1536 - val_f1_m: 0.4702 - val_recall_m: 0.4542 - val_precision_m: 0.4927 - val_precision: 0.4927 - val_recall_6: 0.4542 - val_f1score: 0.4702\n",
      "Epoch 73/300\n",
      "epoch:  72\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.4260 - acc: 0.8192 - prec: 0.9424 - recall: 0.9496 - prec_1: 0.8206 - recall_1: 0.8523 - prec_2: 0.9233 - recall_2: 0.8855 - prec_3: 0.6869 - recall_3: 0.6111 - prec_4: 0.6435 - recall_4: 0.7168 - prec_5: 0.9074 - recall_5: 0.9337 - f1_m: 0.8141 - recall_m: 0.7922 - precision_m: 0.8376 - precision: 0.8376 - recall_6: 0.7922 - f1score: 0.8141 - val_loss: 4.3214 - val_acc: 0.3323 - val_prec: 0.1701 - val_recall: 0.1749 - val_prec_1: 0.1441 - val_recall_1: 0.0307 - val_prec_2: 0.1879 - val_recall_2: 0.1457 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1031 - val_recall_5: 0.0133 - val_f1_m: 0.3238 - val_recall_m: 0.3166 - val_precision_m: 0.3324 - val_precision: 0.3324 - val_recall_6: 0.3166 - val_f1score: 0.3238\n",
      "Epoch 74/300\n",
      "epoch:  73\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.4211 - acc: 0.8192 - prec: 0.9461 - recall: 0.9496 - prec_1: 0.8326 - recall_1: 0.8412 - prec_2: 0.9135 - recall_2: 0.8950 - prec_3: 0.6832 - recall_3: 0.6192 - prec_4: 0.6505 - recall_4: 0.7153 - prec_5: 0.8996 - recall_5: 0.9277 - f1_m: 0.8177 - recall_m: 0.7980 - precision_m: 0.8388 - precision: 0.8388 - recall_6: 0.7980 - f1score: 0.8177 - val_loss: 2.0399 - val_acc: 0.4612 - val_prec: 0.1762 - val_recall: 0.1676 - val_prec_1: 0.0961 - val_recall_1: 0.0121 - val_prec_2: 0.1774 - val_recall_2: 0.1594 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1481 - val_f1_m: 0.4579 - val_recall_m: 0.4497 - val_precision_m: 0.4669 - val_precision: 0.4669 - val_recall_6: 0.4497 - val_f1score: 0.4579\n",
      "Epoch 75/300\n",
      "epoch:  74\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.4248 - acc: 0.8161 - prec: 0.9446 - recall: 0.9495 - prec_1: 0.8241 - recall_1: 0.8377 - prec_2: 0.9179 - recall_2: 0.8879 - prec_3: 0.6774 - recall_3: 0.6263 - prec_4: 0.6462 - recall_4: 0.6947 - prec_5: 0.8973 - recall_5: 0.9297 - f1_m: 0.8120 - recall_m: 0.7912 - precision_m: 0.8342 - precision: 0.8342 - recall_6: 0.7912 - f1score: 0.8120 - val_loss: 2.7522 - val_acc: 0.5453 - val_prec: 0.1762 - val_recall: 0.1660 - val_prec_1: 0.1726 - val_recall_1: 0.1697 - val_prec_2: 0.1851 - val_recall_2: 0.1434 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0991 - val_f1_m: 0.5423 - val_recall_m: 0.5333 - val_precision_m: 0.5527 - val_precision: 0.5527 - val_recall_6: 0.5333 - val_f1score: 0.5423\n",
      "Epoch 76/300\n",
      "epoch:  75\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.4227 - acc: 0.8180 - prec: 0.9404 - recall: 0.9498 - prec_1: 0.8410 - recall_1: 0.8414 - prec_2: 0.9054 - recall_2: 0.8911 - prec_3: 0.6848 - recall_3: 0.6098 - prec_4: 0.6367 - recall_4: 0.7047 - prec_5: 0.8983 - recall_5: 0.9404 - f1_m: 0.8150 - recall_m: 0.7950 - precision_m: 0.8366 - precision: 0.8366 - recall_6: 0.7950 - f1score: 0.8150 - val_loss: 1.4949 - val_acc: 0.5435 - val_prec: 0.1762 - val_recall: 0.1696 - val_prec_1: 0.1727 - val_recall_1: 0.1595 - val_prec_2: 0.1762 - val_recall_2: 0.1010 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1602 - val_recall_4: 0.0583 - val_prec_5: 0.1672 - val_recall_5: 0.0829 - val_f1_m: 0.5352 - val_recall_m: 0.5240 - val_precision_m: 0.5550 - val_precision: 0.5550 - val_recall_6: 0.5240 - val_f1score: 0.5352\n",
      "Epoch 77/300\n",
      "epoch:  76\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.4238 - acc: 0.8168 - prec: 0.9376 - recall: 0.9498 - prec_1: 0.8217 - recall_1: 0.8570 - prec_2: 0.9203 - recall_2: 0.8830 - prec_3: 0.6841 - recall_3: 0.6289 - prec_4: 0.6460 - recall_4: 0.7052 - prec_5: 0.9059 - recall_5: 0.9266 - f1_m: 0.8134 - recall_m: 0.7930 - precision_m: 0.8353 - precision: 0.8353 - recall_6: 0.7930 - f1score: 0.8134 - val_loss: 2.1812 - val_acc: 0.4997 - val_prec: 0.1762 - val_recall: 0.1417 - val_prec_1: 0.1730 - val_recall_1: 0.1576 - val_prec_2: 0.1842 - val_recall_2: 0.1292 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1023 - val_f1_m: 0.4993 - val_recall_m: 0.4905 - val_precision_m: 0.5089 - val_precision: 0.5089 - val_recall_6: 0.4905 - val_f1score: 0.4993\n",
      "Epoch 78/300\n",
      "epoch:  77\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.4146 - acc: 0.8202 - prec: 0.9435 - recall: 0.9511 - prec_1: 0.8307 - recall_1: 0.8532 - prec_2: 0.9263 - recall_2: 0.8957 - prec_3: 0.6696 - recall_3: 0.6365 - prec_4: 0.6455 - recall_4: 0.6728 - prec_5: 0.9023 - recall_5: 0.9293 - f1_m: 0.8166 - recall_m: 0.7969 - precision_m: 0.8377 - precision: 0.8377 - recall_6: 0.7969 - f1score: 0.8166 - val_loss: 1.8008 - val_acc: 0.5478 - val_prec: 0.1762 - val_recall: 0.1689 - val_prec_1: 0.1738 - val_recall_1: 0.1296 - val_prec_2: 0.1842 - val_recall_2: 0.1063 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1602 - val_recall_4: 0.0604 - val_prec_5: 0.1672 - val_recall_5: 0.1164 - val_f1_m: 0.5247 - val_recall_m: 0.5103 - val_precision_m: 0.5412 - val_precision: 0.5412 - val_recall_6: 0.5103 - val_f1score: 0.5247\n",
      "Epoch 79/300\n",
      "epoch:  78\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.4148 - acc: 0.8232 - prec: 0.9520 - recall: 0.9552 - prec_1: 0.8352 - recall_1: 0.8590 - prec_2: 0.9138 - recall_2: 0.8887 - prec_3: 0.6751 - recall_3: 0.6433 - prec_4: 0.6599 - recall_4: 0.6911 - prec_5: 0.9108 - recall_5: 0.9296 - f1_m: 0.8186 - recall_m: 0.7994 - precision_m: 0.8391 - precision: 0.8391 - recall_6: 0.7994 - f1score: 0.8186 - val_loss: 1.6043 - val_acc: 0.5611 - val_prec: 0.1762 - val_recall: 0.1646 - val_prec_1: 0.1712 - val_recall_1: 0.1237 - val_prec_2: 0.1815 - val_recall_2: 0.1064 - val_prec_3: 0.0961 - val_recall_3: 0.0279 - val_prec_4: 0.1126 - val_recall_4: 0.0257 - val_prec_5: 0.1672 - val_recall_5: 0.1390 - val_f1_m: 0.5361 - val_recall_m: 0.5200 - val_precision_m: 0.5624 - val_precision: 0.5624 - val_recall_6: 0.5200 - val_f1score: 0.5361\n",
      "Epoch 80/300\n",
      "epoch:  79\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.4146 - acc: 0.8191 - prec: 0.9450 - recall: 0.9517 - prec_1: 0.8286 - recall_1: 0.8573 - prec_2: 0.9235 - recall_2: 0.8875 - prec_3: 0.6686 - recall_3: 0.6090 - prec_4: 0.6447 - recall_4: 0.7116 - prec_5: 0.9048 - recall_5: 0.9299 - f1_m: 0.8152 - recall_m: 0.7974 - precision_m: 0.8342 - precision: 0.8342 - recall_6: 0.7974 - f1score: 0.8152 - val_loss: 1.1796 - val_acc: 0.5348 - val_prec: 0.1762 - val_recall: 0.1517 - val_prec_1: 0.1655 - val_recall_1: 0.0254 - val_prec_2: 0.1763 - val_recall_2: 0.1644 - val_prec_3: 0.0801 - val_recall_3: 0.0246 - val_prec_4: 0.1121 - val_recall_4: 0.0571 - val_prec_5: 0.1672 - val_recall_5: 0.1588 - val_f1_m: 0.5162 - val_recall_m: 0.4947 - val_precision_m: 0.5437 - val_precision: 0.5437 - val_recall_6: 0.4947 - val_f1score: 0.5162\n",
      "Epoch 81/300\n",
      "epoch:  80\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.4134 - acc: 0.8167 - prec: 0.9390 - recall: 0.9546 - prec_1: 0.8304 - recall_1: 0.8426 - prec_2: 0.9178 - recall_2: 0.8980 - prec_3: 0.6698 - recall_3: 0.5905 - prec_4: 0.6365 - recall_4: 0.7116 - prec_5: 0.9123 - recall_5: 0.9274 - f1_m: 0.8144 - recall_m: 0.7960 - precision_m: 0.8340 - precision: 0.8340 - recall_6: 0.7960 - f1score: 0.8144 - val_loss: 2.5340 - val_acc: 0.5248 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1742 - val_recall_1: 0.1614 - val_prec_2: 0.1922 - val_recall_2: 0.1065 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1163 - val_f1_m: 0.5204 - val_recall_m: 0.5128 - val_precision_m: 0.5287 - val_precision: 0.5287 - val_recall_6: 0.5128 - val_f1score: 0.5204\n",
      "Epoch 82/300\n",
      "epoch:  81\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 684us/step - loss: 0.4071 - acc: 0.8237 - prec: 0.9462 - recall: 0.9544 - prec_1: 0.8415 - recall_1: 0.8500 - prec_2: 0.9203 - recall_2: 0.8959 - prec_3: 0.6792 - recall_3: 0.6383 - prec_4: 0.6535 - recall_4: 0.7030 - prec_5: 0.8992 - recall_5: 0.9343 - f1_m: 0.8204 - recall_m: 0.8011 - precision_m: 0.8411 - precision: 0.8411 - recall_6: 0.8011 - f1score: 0.8204 - val_loss: 2.3540 - val_acc: 0.4570 - val_prec: 0.1724 - val_recall: 0.1732 - val_prec_1: 0.1762 - val_recall_1: 0.0316 - val_prec_2: 0.1824 - val_recall_2: 0.1865 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1019 - val_f1_m: 0.4449 - val_recall_m: 0.4354 - val_precision_m: 0.4570 - val_precision: 0.4570 - val_recall_6: 0.4354 - val_f1score: 0.4449\n",
      "Epoch 83/300\n",
      "epoch:  82\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.4100 - acc: 0.8202 - prec: 0.9450 - recall: 0.9543 - prec_1: 0.8315 - recall_1: 0.8549 - prec_2: 0.9269 - recall_2: 0.8877 - prec_3: 0.6847 - recall_3: 0.6103 - prec_4: 0.6405 - recall_4: 0.7107 - prec_5: 0.9031 - recall_5: 0.9391 - f1_m: 0.8170 - recall_m: 0.7981 - precision_m: 0.8373 - precision: 0.8373 - recall_6: 0.7981 - f1score: 0.8170 - val_loss: 1.3007 - val_acc: 0.5333 - val_prec: 0.1710 - val_recall: 0.1736 - val_prec_1: 0.0320 - val_recall_1: 0.0039 - val_prec_2: 0.1771 - val_recall_2: 0.1172 - val_prec_3: 0.1702 - val_recall_3: 0.0965 - val_prec_4: 0.1602 - val_recall_4: 0.0428 - val_prec_5: 0.1672 - val_recall_5: 0.1280 - val_f1_m: 0.5209 - val_recall_m: 0.5073 - val_precision_m: 0.5373 - val_precision: 0.5373 - val_recall_6: 0.5073 - val_f1score: 0.5209\n",
      "Epoch 84/300\n",
      "epoch:  83\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.4093 - acc: 0.8232 - prec: 0.9375 - recall: 0.9598 - prec_1: 0.8401 - recall_1: 0.8487 - prec_2: 0.9166 - recall_2: 0.9038 - prec_3: 0.6766 - recall_3: 0.6366 - prec_4: 0.6528 - recall_4: 0.6899 - prec_5: 0.9100 - recall_5: 0.9276 - f1_m: 0.8169 - recall_m: 0.7971 - precision_m: 0.8380 - precision: 0.8380 - recall_6: 0.7971 - f1score: 0.8169 - val_loss: 3.6014 - val_acc: 0.4860 - val_prec: 0.1704 - val_recall: 0.1666 - val_prec_1: 0.1700 - val_recall_1: 0.0617 - val_prec_2: 0.1809 - val_recall_2: 0.1346 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1499 - val_f1_m: 0.4732 - val_recall_m: 0.4540 - val_precision_m: 0.4978 - val_precision: 0.4978 - val_recall_6: 0.4540 - val_f1score: 0.4732\n",
      "Epoch 85/300\n",
      "epoch:  84\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.4062 - acc: 0.8205 - prec: 0.9377 - recall: 0.9532 - prec_1: 0.8376 - recall_1: 0.8449 - prec_2: 0.9220 - recall_2: 0.8970 - prec_3: 0.6744 - recall_3: 0.6577 - prec_4: 0.6596 - recall_4: 0.6809 - prec_5: 0.9028 - recall_5: 0.9319 - f1_m: 0.8184 - recall_m: 0.8002 - precision_m: 0.8378 - precision: 0.8378 - recall_6: 0.8002 - f1score: 0.8184 - val_loss: 5.5905 - val_acc: 0.4092 - val_prec: 0.1762 - val_recall: 0.1322 - val_prec_1: 0.1716 - val_recall_1: 0.1420 - val_prec_2: 0.0961 - val_recall_2: 0.0113 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1389 - val_f1_m: 0.4079 - val_recall_m: 0.4052 - val_precision_m: 0.4109 - val_precision: 0.4109 - val_recall_6: 0.4052 - val_f1score: 0.4079\n",
      "Epoch 86/300\n",
      "epoch:  85\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.4249 - acc: 0.8115 - prec: 0.9352 - recall: 0.9460 - prec_1: 0.8258 - recall_1: 0.8189 - prec_2: 0.9064 - recall_2: 0.8924 - prec_3: 0.6792 - recall_3: 0.5918 - prec_4: 0.6383 - recall_4: 0.7075 - prec_5: 0.8942 - recall_5: 0.9325 - f1_m: 0.8083 - recall_m: 0.7887 - precision_m: 0.8293 - precision: 0.8293 - recall_6: 0.7887 - f1score: 0.8083 - val_loss: 2.3186 - val_acc: 0.5103 - val_prec: 0.1756 - val_recall: 0.1721 - val_prec_1: 0.1762 - val_recall_1: 0.0719 - val_prec_2: 0.1724 - val_recall_2: 0.1608 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1368 - val_f1_m: 0.5063 - val_recall_m: 0.4877 - val_precision_m: 0.5288 - val_precision: 0.5288 - val_recall_6: 0.4877 - val_f1score: 0.5063\n",
      "Epoch 87/300\n",
      "epoch:  86\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.4075 - acc: 0.8217 - prec: 0.9323 - recall: 0.9563 - prec_1: 0.8450 - recall_1: 0.8464 - prec_2: 0.9241 - recall_2: 0.8988 - prec_3: 0.6709 - recall_3: 0.6184 - prec_4: 0.6443 - recall_4: 0.6978 - prec_5: 0.9035 - recall_5: 0.9336 - f1_m: 0.8191 - recall_m: 0.8007 - precision_m: 0.8386 - precision: 0.8386 - recall_6: 0.8007 - f1score: 0.8191 - val_loss: 1.0393 - val_acc: 0.5633 - val_prec: 0.1762 - val_recall: 0.1702 - val_prec_1: 0.1281 - val_recall_1: 0.0148 - val_prec_2: 0.1839 - val_recall_2: 0.1868 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1822 - val_recall_4: 0.1529 - val_prec_5: 0.1672 - val_recall_5: 0.0923 - val_f1_m: 0.5395 - val_recall_m: 0.5208 - val_precision_m: 0.5679 - val_precision: 0.5679 - val_recall_6: 0.5208 - val_f1score: 0.5395\n",
      "Epoch 88/300\n",
      "epoch:  87\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.4019 - acc: 0.8182 - prec: 0.9341 - recall: 0.9546 - prec_1: 0.8396 - recall_1: 0.8461 - prec_2: 0.9221 - recall_2: 0.9034 - prec_3: 0.6616 - recall_3: 0.6325 - prec_4: 0.6477 - recall_4: 0.6802 - prec_5: 0.9025 - recall_5: 0.9318 - f1_m: 0.8165 - recall_m: 0.7989 - precision_m: 0.8352 - precision: 0.8352 - recall_6: 0.7989 - f1score: 0.8165 - val_loss: 3.5208 - val_acc: 0.3969 - val_prec: 0.1680 - val_recall: 0.1749 - val_prec_1: 0.1754 - val_recall_1: 0.0621 - val_prec_2: 0.1858 - val_recall_2: 0.1147 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1657 - val_recall_5: 0.0729 - val_f1_m: 0.3779 - val_recall_m: 0.3564 - val_precision_m: 0.4077 - val_precision: 0.4077 - val_recall_6: 0.3564 - val_f1score: 0.3779\n",
      "Epoch 89/300\n",
      "epoch:  88\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 667us/step - loss: 0.3987 - acc: 0.8256 - prec: 0.9504 - recall: 0.9562 - prec_1: 0.8428 - recall_1: 0.8573 - prec_2: 0.9227 - recall_2: 0.9014 - prec_3: 0.6799 - recall_3: 0.6283 - prec_4: 0.6546 - recall_4: 0.7117 - prec_5: 0.9110 - recall_5: 0.9350 - f1_m: 0.8234 - recall_m: 0.8067 - precision_m: 0.8409 - precision: 0.8409 - recall_6: 0.8067 - f1score: 0.8234 - val_loss: 2.6458 - val_acc: 0.5490 - val_prec: 0.1738 - val_recall: 0.1696 - val_prec_1: 0.1602 - val_recall_1: 0.0840 - val_prec_2: 0.1762 - val_recall_2: 0.0977 - val_prec_3: 0.1688 - val_recall_3: 0.0622 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1573 - val_f1_m: 0.5425 - val_recall_m: 0.5250 - val_precision_m: 0.5642 - val_precision: 0.5642 - val_recall_6: 0.5250 - val_f1score: 0.5425\n",
      "Epoch 90/300\n",
      "epoch:  89\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3962 - acc: 0.8285 - prec: 0.9452 - recall: 0.9557 - prec_1: 0.8441 - recall_1: 0.8503 - prec_2: 0.9267 - recall_2: 0.9034 - prec_3: 0.6959 - recall_3: 0.6539 - prec_4: 0.6628 - recall_4: 0.7042 - prec_5: 0.9050 - recall_5: 0.9397 - f1_m: 0.8253 - recall_m: 0.8073 - precision_m: 0.8446 - precision: 0.8446 - recall_6: 0.8073 - f1score: 0.8253 - val_loss: 1.0465 - val_acc: 0.5746 - val_prec: 0.1750 - val_recall: 0.1713 - val_prec_1: 0.0801 - val_recall_1: 0.0041 - val_prec_2: 0.1771 - val_recall_2: 0.1393 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1822 - val_recall_4: 0.1461 - val_prec_5: 0.1672 - val_recall_5: 0.1576 - val_f1_m: 0.5561 - val_recall_m: 0.5353 - val_precision_m: 0.5857 - val_precision: 0.5857 - val_recall_6: 0.5353 - val_f1score: 0.5561\n",
      "Epoch 91/300\n",
      "epoch:  90\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.4013 - acc: 0.8210 - prec: 0.9363 - recall: 0.9549 - prec_1: 0.8360 - recall_1: 0.8548 - prec_2: 0.9203 - recall_2: 0.9061 - prec_3: 0.6788 - recall_3: 0.6373 - prec_4: 0.6490 - recall_4: 0.6878 - prec_5: 0.9098 - recall_5: 0.9285 - f1_m: 0.8194 - recall_m: 0.8028 - precision_m: 0.8369 - precision: 0.8369 - recall_6: 0.8028 - f1score: 0.8194 - val_loss: 0.7695 - val_acc: 0.6924 - val_prec: 0.1762 - val_recall: 0.1726 - val_prec_1: 0.1741 - val_recall_1: 0.1423 - val_prec_2: 0.1882 - val_recall_2: 0.1287 - val_prec_3: 0.1121 - val_recall_3: 0.0549 - val_prec_4: 0.1762 - val_recall_4: 0.1049 - val_prec_5: 0.1672 - val_recall_5: 0.1286 - val_f1_m: 0.6869 - val_recall_m: 0.6662 - val_precision_m: 0.7113 - val_precision: 0.7113 - val_recall_6: 0.6662 - val_f1score: 0.6869\n",
      "Epoch 92/300\n",
      "epoch:  91\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.4006 - acc: 0.8227 - prec: 0.9425 - recall: 0.9585 - prec_1: 0.8436 - recall_1: 0.8557 - prec_2: 0.9270 - recall_2: 0.9009 - prec_3: 0.6742 - recall_3: 0.6253 - prec_4: 0.6386 - recall_4: 0.6896 - prec_5: 0.9078 - recall_5: 0.9304 - f1_m: 0.8196 - recall_m: 0.8030 - precision_m: 0.8372 - precision: 0.8372 - recall_6: 0.8030 - f1score: 0.8196 - val_loss: 6.3687 - val_acc: 0.3599 - val_prec: 0.1710 - val_recall: 0.1676 - val_prec_1: 0.1726 - val_recall_1: 0.0904 - val_prec_2: 0.1855 - val_recall_2: 0.1334 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0000e+00 - val_recall_5: 0.0000e+00 - val_f1_m: 0.3561 - val_recall_m: 0.3514 - val_precision_m: 0.3613 - val_precision: 0.3613 - val_recall_6: 0.3514 - val_f1score: 0.3561\n",
      "Epoch 93/300\n",
      "epoch:  92\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 683us/step - loss: 0.3994 - acc: 0.8237 - prec: 0.9466 - recall: 0.9483 - prec_1: 0.8393 - recall_1: 0.8485 - prec_2: 0.9208 - recall_2: 0.9054 - prec_3: 0.6897 - recall_3: 0.6501 - prec_4: 0.6612 - recall_4: 0.7001 - prec_5: 0.8952 - recall_5: 0.9332 - f1_m: 0.8215 - recall_m: 0.8031 - precision_m: 0.8412 - precision: 0.8412 - recall_6: 0.8031 - f1score: 0.8215 - val_loss: 3.2377 - val_acc: 0.3841 - val_prec: 0.1682 - val_recall: 0.1727 - val_prec_1: 0.1722 - val_recall_1: 0.1225 - val_prec_2: 0.1922 - val_recall_2: 0.0948 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0871 - val_recall_5: 0.0185 - val_f1_m: 0.3841 - val_recall_m: 0.3809 - val_precision_m: 0.3877 - val_precision: 0.3877 - val_recall_6: 0.3809 - val_f1score: 0.3841\n",
      "Epoch 94/300\n",
      "epoch:  93\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 6s 702us/step - loss: 0.3937 - acc: 0.8285 - prec: 0.9525 - recall: 0.9532 - prec_1: 0.8455 - recall_1: 0.8631 - prec_2: 0.9228 - recall_2: 0.9071 - prec_3: 0.6870 - recall_3: 0.6402 - prec_4: 0.6616 - recall_4: 0.7077 - prec_5: 0.9040 - recall_5: 0.9315 - f1_m: 0.8265 - recall_m: 0.8093 - precision_m: 0.8448 - precision: 0.8448 - recall_6: 0.8093 - f1score: 0.8265 - val_loss: 2.0426 - val_acc: 0.5380 - val_prec: 0.1750 - val_recall: 0.1721 - val_prec_1: 0.1749 - val_recall_1: 0.0755 - val_prec_2: 0.1827 - val_recall_2: 0.1748 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0160 - val_recall_4: 5.0050e-04 - val_prec_5: 0.1672 - val_recall_5: 0.1503 - val_f1_m: 0.5305 - val_recall_m: 0.5148 - val_precision_m: 0.5501 - val_precision: 0.5501 - val_recall_6: 0.5148 - val_f1score: 0.5305\n",
      "Epoch 95/300\n",
      "epoch:  94\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3900 - acc: 0.8316 - prec: 0.9587 - recall: 0.9579 - prec_1: 0.8502 - recall_1: 0.8739 - prec_2: 0.9351 - recall_2: 0.9036 - prec_3: 0.6896 - recall_3: 0.6248 - prec_4: 0.6564 - recall_4: 0.7167 - prec_5: 0.9086 - recall_5: 0.9358 - f1_m: 0.8299 - recall_m: 0.8145 - precision_m: 0.8462 - precision: 0.8462 - recall_6: 0.8145 - f1score: 0.8299 - val_loss: 2.0721 - val_acc: 0.4717 - val_prec: 0.1762 - val_recall: 0.1693 - val_prec_1: 0.1441 - val_recall_1: 0.0204 - val_prec_2: 0.1732 - val_recall_2: 0.1225 - val_prec_3: 0.0801 - val_recall_3: 0.0080 - val_prec_4: 0.0801 - val_recall_4: 0.0264 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.4399 - val_recall_m: 0.4304 - val_precision_m: 0.4509 - val_precision: 0.4509 - val_recall_6: 0.4304 - val_f1score: 0.4399\n",
      "Epoch 96/300\n",
      "epoch:  95\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3907 - acc: 0.8305 - prec: 0.9458 - recall: 0.9573 - prec_1: 0.8637 - recall_1: 0.8594 - prec_2: 0.9230 - recall_2: 0.9129 - prec_3: 0.6851 - recall_3: 0.6412 - prec_4: 0.6700 - recall_4: 0.7015 - prec_5: 0.9051 - recall_5: 0.9414 - f1_m: 0.8275 - recall_m: 0.8098 - precision_m: 0.8463 - precision: 0.8463 - recall_6: 0.8098 - f1score: 0.8275 - val_loss: 4.4592 - val_acc: 0.4357 - val_prec: 0.1762 - val_recall: 0.1096 - val_prec_1: 0.1704 - val_recall_1: 0.1642 - val_prec_2: 0.1762 - val_recall_2: 0.0536 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1265 - val_f1_m: 0.4335 - val_recall_m: 0.4324 - val_precision_m: 0.4346 - val_precision: 0.4346 - val_recall_6: 0.4324 - val_f1score: 0.4335\n",
      "Epoch 97/300\n",
      "epoch:  96\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3850 - acc: 0.8345 - prec: 0.9552 - recall: 0.9610 - prec_1: 0.8596 - recall_1: 0.8620 - prec_2: 0.9296 - recall_2: 0.9207 - prec_3: 0.6817 - recall_3: 0.6699 - prec_4: 0.6672 - recall_4: 0.6831 - prec_5: 0.9041 - recall_5: 0.9432 - f1_m: 0.8311 - recall_m: 0.8148 - precision_m: 0.8483 - precision: 0.8483 - recall_6: 0.8148 - f1score: 0.8311 - val_loss: 0.9388 - val_acc: 0.6409 - val_prec: 0.1704 - val_recall: 0.1703 - val_prec_1: 0.1728 - val_recall_1: 0.1213 - val_prec_2: 0.1842 - val_recall_2: 0.1068 - val_prec_3: 0.1602 - val_recall_3: 0.0772 - val_prec_4: 0.1762 - val_recall_4: 0.0738 - val_prec_5: 0.1672 - val_recall_5: 0.1415 - val_f1_m: 0.6382 - val_recall_m: 0.6166 - val_precision_m: 0.6665 - val_precision: 0.6665 - val_recall_6: 0.6166 - val_f1score: 0.6382\n",
      "Epoch 98/300\n",
      "epoch:  97\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3849 - acc: 0.8293 - prec: 0.9460 - recall: 0.9553 - prec_1: 0.8546 - recall_1: 0.8665 - prec_2: 0.9318 - recall_2: 0.9089 - prec_3: 0.6732 - recall_3: 0.6546 - prec_4: 0.6498 - recall_4: 0.6777 - prec_5: 0.9112 - recall_5: 0.9415 - f1_m: 0.8278 - recall_m: 0.8122 - precision_m: 0.8442 - precision: 0.8442 - recall_6: 0.8122 - f1score: 0.8278 - val_loss: 5.1371 - val_acc: 0.4530 - val_prec: 0.1683 - val_recall: 0.1757 - val_prec_1: 0.1762 - val_recall_1: 0.0499 - val_prec_2: 0.1895 - val_recall_2: 0.1580 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1039 - val_f1_m: 0.4382 - val_recall_m: 0.4214 - val_precision_m: 0.4604 - val_precision: 0.4604 - val_recall_6: 0.4214 - val_f1score: 0.4382\n",
      "Epoch 99/300\n",
      "epoch:  98\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 684us/step - loss: 0.3871 - acc: 0.8290 - prec: 0.9504 - recall: 0.9627 - prec_1: 0.8534 - recall_1: 0.8701 - prec_2: 0.9263 - recall_2: 0.9102 - prec_3: 0.6865 - recall_3: 0.6197 - prec_4: 0.6496 - recall_4: 0.7079 - prec_5: 0.9217 - recall_5: 0.9319 - f1_m: 0.8258 - recall_m: 0.8100 - precision_m: 0.8425 - precision: 0.8425 - recall_6: 0.8100 - f1score: 0.8258 - val_loss: 2.4593 - val_acc: 0.5258 - val_prec: 0.1678 - val_recall: 0.1711 - val_prec_1: 0.1642 - val_recall_1: 0.0875 - val_prec_2: 0.1869 - val_recall_2: 0.1599 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1392 - val_f1_m: 0.5240 - val_recall_m: 0.5088 - val_precision_m: 0.5416 - val_precision: 0.5416 - val_recall_6: 0.5088 - val_f1score: 0.5240\n",
      "Epoch 100/300\n",
      "epoch:  99\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3872 - acc: 0.8288 - prec: 0.9493 - recall: 0.9588 - prec_1: 0.8494 - recall_1: 0.8780 - prec_2: 0.9249 - recall_2: 0.9000 - prec_3: 0.6883 - recall_3: 0.6347 - prec_4: 0.6578 - recall_4: 0.7142 - prec_5: 0.9149 - recall_5: 0.9341 - f1_m: 0.8270 - recall_m: 0.8116 - precision_m: 0.8433 - precision: 0.8433 - recall_6: 0.8116 - f1score: 0.8270 - val_loss: 6.2706 - val_acc: 0.3436 - val_prec: 0.1762 - val_recall: 0.0951 - val_prec_1: 0.1702 - val_recall_1: 0.1748 - val_prec_2: 0.1815 - val_recall_2: 0.0573 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1512 - val_recall_5: 0.0358 - val_f1_m: 0.3435 - val_recall_m: 0.3433 - val_precision_m: 0.3436 - val_precision: 0.3436 - val_recall_6: 0.3433 - val_f1score: 0.3435\n",
      "Epoch 101/300\n",
      "epoch:  100\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3923 - acc: 0.8247 - prec: 0.9441 - recall: 0.9574 - prec_1: 0.8490 - recall_1: 0.8622 - prec_2: 0.9279 - recall_2: 0.9136 - prec_3: 0.6762 - recall_3: 0.6250 - prec_4: 0.6471 - recall_4: 0.6915 - prec_5: 0.9235 - recall_5: 0.9290 - f1_m: 0.8217 - recall_m: 0.8053 - precision_m: 0.8389 - precision: 0.8389 - recall_6: 0.8053 - f1score: 0.8217 - val_loss: 2.8214 - val_acc: 0.5010 - val_prec: 0.1732 - val_recall: 0.1729 - val_prec_1: 0.1754 - val_recall_1: 0.0556 - val_prec_2: 0.1860 - val_recall_2: 0.1602 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1669 - val_recall_5: 0.1460 - val_f1_m: 0.4937 - val_recall_m: 0.4787 - val_precision_m: 0.5113 - val_precision: 0.5113 - val_recall_6: 0.4787 - val_f1score: 0.4937\n",
      "Epoch 102/300\n",
      "epoch:  101\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3851 - acc: 0.8345 - prec: 0.9444 - recall: 0.9644 - prec_1: 0.8581 - recall_1: 0.8707 - prec_2: 0.9318 - recall_2: 0.9120 - prec_3: 0.6859 - recall_3: 0.6478 - prec_4: 0.6656 - recall_4: 0.7065 - prec_5: 0.9177 - recall_5: 0.9395 - f1_m: 0.8304 - recall_m: 0.8137 - precision_m: 0.8480 - precision: 0.8480 - recall_6: 0.8137 - f1score: 0.8304 - val_loss: 3.3197 - val_acc: 0.5383 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1743 - val_recall_1: 0.1396 - val_prec_2: 0.1842 - val_recall_2: 0.1042 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1496 - val_f1_m: 0.5325 - val_recall_m: 0.5205 - val_precision_m: 0.5462 - val_precision: 0.5462 - val_recall_6: 0.5205 - val_f1score: 0.5325\n",
      "Epoch 103/300\n",
      "epoch:  102\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3780 - acc: 0.8376 - prec: 0.9469 - recall: 0.9584 - prec_1: 0.8694 - recall_1: 0.8527 - prec_2: 0.9168 - recall_2: 0.9141 - prec_3: 0.7043 - recall_3: 0.6631 - prec_4: 0.6751 - recall_4: 0.7180 - prec_5: 0.9133 - recall_5: 0.9450 - f1_m: 0.8337 - recall_m: 0.8186 - precision_m: 0.8498 - precision: 0.8498 - recall_6: 0.8186 - f1score: 0.8337 - val_loss: 2.9126 - val_acc: 0.5313 - val_prec: 0.1762 - val_recall: 0.1464 - val_prec_1: 0.1682 - val_recall_1: 0.1228 - val_prec_2: 0.1890 - val_recall_2: 0.1351 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1541 - val_f1_m: 0.5292 - val_recall_m: 0.5188 - val_precision_m: 0.5407 - val_precision: 0.5407 - val_recall_6: 0.5188 - val_f1score: 0.5292\n",
      "Epoch 104/300\n",
      "epoch:  103\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3763 - acc: 0.8366 - prec: 0.9488 - recall: 0.9658 - prec_1: 0.8634 - recall_1: 0.8735 - prec_2: 0.9331 - recall_2: 0.9091 - prec_3: 0.6848 - recall_3: 0.6525 - prec_4: 0.6754 - recall_4: 0.7117 - prec_5: 0.9195 - recall_5: 0.9405 - f1_m: 0.8330 - recall_m: 0.8175 - precision_m: 0.8494 - precision: 0.8494 - recall_6: 0.8175 - f1score: 0.8330 - val_loss: 2.1272 - val_acc: 0.5561 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1602 - val_recall_1: 0.0435 - val_prec_2: 0.1810 - val_recall_2: 0.1830 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0801 - val_recall_4: 0.0380 - val_prec_5: 0.1672 - val_recall_5: 0.1563 - val_f1_m: 0.5500 - val_recall_m: 0.5373 - val_precision_m: 0.5655 - val_precision: 0.5655 - val_recall_6: 0.5373 - val_f1score: 0.5500\n",
      "Epoch 105/300\n",
      "epoch:  104\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.3732 - acc: 0.8381 - prec: 0.9567 - recall: 0.9551 - prec_1: 0.8561 - recall_1: 0.8774 - prec_2: 0.9333 - recall_2: 0.9166 - prec_3: 0.6924 - recall_3: 0.6609 - prec_4: 0.6670 - recall_4: 0.6986 - prec_5: 0.9214 - recall_5: 0.9343 - f1_m: 0.8353 - recall_m: 0.8207 - precision_m: 0.8507 - precision: 0.8507 - recall_6: 0.8207 - f1score: 0.8353 - val_loss: 0.8518 - val_acc: 0.7257 - val_prec: 0.1762 - val_recall: 0.1695 - val_prec_1: 0.1742 - val_recall_1: 0.1388 - val_prec_2: 0.1906 - val_recall_2: 0.1486 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1822 - val_recall_4: 0.1802 - val_prec_5: 0.1672 - val_recall_5: 0.1430 - val_f1_m: 0.7152 - val_recall_m: 0.6927 - val_precision_m: 0.7422 - val_precision: 0.7422 - val_recall_6: 0.6927 - val_f1score: 0.7152\n",
      "Epoch 106/300\n",
      "epoch:  105\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3794 - acc: 0.8341 - prec: 0.9546 - recall: 0.9607 - prec_1: 0.8545 - recall_1: 0.8818 - prec_2: 0.9239 - recall_2: 0.9103 - prec_3: 0.6917 - recall_3: 0.6483 - prec_4: 0.6621 - recall_4: 0.7074 - prec_5: 0.9251 - recall_5: 0.9395 - f1_m: 0.8306 - recall_m: 0.8168 - precision_m: 0.8451 - precision: 0.8451 - recall_6: 0.8168 - f1score: 0.8306 - val_loss: 3.7822 - val_acc: 0.4707 - val_prec: 0.1714 - val_recall: 0.1752 - val_prec_1: 0.1741 - val_recall_1: 0.1161 - val_prec_2: 0.1851 - val_recall_2: 0.1459 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1592 - val_recall_5: 0.0671 - val_f1_m: 0.4644 - val_recall_m: 0.4510 - val_precision_m: 0.4805 - val_precision: 0.4805 - val_recall_6: 0.4510 - val_f1score: 0.4644\n",
      "Epoch 107/300\n",
      "epoch:  106\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3717 - acc: 0.8410 - prec: 0.9582 - recall: 0.9634 - prec_1: 0.8566 - recall_1: 0.8842 - prec_2: 0.9305 - recall_2: 0.9127 - prec_3: 0.7050 - recall_3: 0.6649 - prec_4: 0.6779 - recall_4: 0.7099 - prec_5: 0.9287 - recall_5: 0.9389 - f1_m: 0.8374 - recall_m: 0.8235 - precision_m: 0.8520 - precision: 0.8520 - recall_6: 0.8235 - f1score: 0.8374 - val_loss: 1.1970 - val_acc: 0.5841 - val_prec: 0.1762 - val_recall: 0.1621 - val_prec_1: 0.1709 - val_recall_1: 0.1686 - val_prec_2: 0.1762 - val_recall_2: 0.0900 - val_prec_3: 0.0961 - val_recall_3: 0.0391 - val_prec_4: 0.1281 - val_recall_4: 0.0408 - val_prec_5: 0.1672 - val_recall_5: 0.1211 - val_f1_m: 0.5492 - val_recall_m: 0.5435 - val_precision_m: 0.5573 - val_precision: 0.5573 - val_recall_6: 0.5435 - val_f1score: 0.5492\n",
      "Epoch 108/300\n",
      "epoch:  107\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3787 - acc: 0.8330 - prec: 0.9429 - recall: 0.9634 - prec_1: 0.8614 - recall_1: 0.8695 - prec_2: 0.9237 - recall_2: 0.9120 - prec_3: 0.6865 - recall_3: 0.6345 - prec_4: 0.6560 - recall_4: 0.7182 - prec_5: 0.9253 - recall_5: 0.9377 - f1_m: 0.8314 - recall_m: 0.8171 - precision_m: 0.8464 - precision: 0.8464 - recall_6: 0.8171 - f1score: 0.8314 - val_loss: 1.3394 - val_acc: 0.5728 - val_prec: 0.1756 - val_recall: 0.1696 - val_prec_1: 0.1441 - val_recall_1: 0.0246 - val_prec_2: 0.1796 - val_recall_2: 0.1711 - val_prec_3: 0.1448 - val_recall_3: 0.0915 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1498 - val_f1_m: 0.5696 - val_recall_m: 0.5623 - val_precision_m: 0.5777 - val_precision: 0.5777 - val_recall_6: 0.5623 - val_f1score: 0.5696\n",
      "Epoch 109/300\n",
      "epoch:  108\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3709 - acc: 0.8350 - prec: 0.9519 - recall: 0.9662 - prec_1: 0.8576 - recall_1: 0.8899 - prec_2: 0.9377 - recall_2: 0.9081 - prec_3: 0.6765 - recall_3: 0.6487 - prec_4: 0.6549 - recall_4: 0.6883 - prec_5: 0.9313 - recall_5: 0.9392 - f1_m: 0.8330 - recall_m: 0.8186 - precision_m: 0.8482 - precision: 0.8482 - recall_6: 0.8186 - f1score: 0.8330 - val_loss: 0.8525 - val_acc: 0.6396 - val_prec: 0.1762 - val_recall: 0.1637 - val_prec_1: 0.1740 - val_recall_1: 0.1427 - val_prec_2: 0.1895 - val_recall_2: 0.1365 - val_prec_3: 0.1281 - val_recall_3: 0.0614 - val_prec_4: 0.1762 - val_recall_4: 0.1211 - val_prec_5: 0.1672 - val_recall_5: 0.0568 - val_f1_m: 0.6347 - val_recall_m: 0.6224 - val_precision_m: 0.6480 - val_precision: 0.6480 - val_recall_6: 0.6224 - val_f1score: 0.6347\n",
      "Epoch 110/300\n",
      "epoch:  109\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3737 - acc: 0.8400 - prec: 0.9489 - recall: 0.9630 - prec_1: 0.8655 - recall_1: 0.8886 - prec_2: 0.9367 - recall_2: 0.9208 - prec_3: 0.6903 - recall_3: 0.6719 - prec_4: 0.6725 - recall_4: 0.6962 - prec_5: 0.9204 - recall_5: 0.9392 - f1_m: 0.8372 - recall_m: 0.8235 - precision_m: 0.8516 - precision: 0.8516 - recall_6: 0.8235 - f1score: 0.8372 - val_loss: 5.2076 - val_acc: 0.3591 - val_prec: 0.1756 - val_recall: 0.1594 - val_prec_1: 0.1602 - val_recall_1: 0.0742 - val_prec_2: 0.1121 - val_recall_2: 0.0326 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1073 - val_f1_m: 0.3571 - val_recall_m: 0.3514 - val_precision_m: 0.3636 - val_precision: 0.3636 - val_recall_6: 0.3514 - val_f1score: 0.3571\n",
      "Epoch 111/300\n",
      "epoch:  110\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3829 - acc: 0.8312 - prec: 0.9529 - recall: 0.9630 - prec_1: 0.8568 - recall_1: 0.8579 - prec_2: 0.9223 - recall_2: 0.9170 - prec_3: 0.6944 - recall_3: 0.6393 - prec_4: 0.6531 - recall_4: 0.7115 - prec_5: 0.9098 - recall_5: 0.9314 - f1_m: 0.8288 - recall_m: 0.8135 - precision_m: 0.8450 - precision: 0.8450 - recall_6: 0.8135 - f1score: 0.8288 - val_loss: 1.3187 - val_acc: 0.5798 - val_prec: 0.1711 - val_recall: 0.1469 - val_prec_1: 0.1678 - val_recall_1: 0.1349 - val_prec_2: 0.1762 - val_recall_2: 0.0605 - val_prec_3: 0.1441 - val_recall_3: 0.0883 - val_prec_4: 0.1602 - val_recall_4: 0.0686 - val_prec_5: 0.1672 - val_recall_5: 0.0985 - val_f1_m: 0.5755 - val_recall_m: 0.5656 - val_precision_m: 0.5867 - val_precision: 0.5867 - val_recall_6: 0.5656 - val_f1score: 0.5755\n",
      "Epoch 112/300\n",
      "epoch:  111\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3725 - acc: 0.8340 - prec: 0.9492 - recall: 0.9595 - prec_1: 0.8537 - recall_1: 0.8814 - prec_2: 0.9299 - recall_2: 0.9082 - prec_3: 0.6848 - recall_3: 0.6448 - prec_4: 0.6617 - recall_4: 0.7004 - prec_5: 0.9263 - recall_5: 0.9315 - f1_m: 0.8329 - recall_m: 0.8190 - precision_m: 0.8477 - precision: 0.8477 - recall_6: 0.8190 - f1score: 0.8329 - val_loss: 1.6714 - val_acc: 0.5323 - val_prec: 0.1665 - val_recall: 0.1729 - val_prec_1: 0.1602 - val_recall_1: 0.0982 - val_prec_2: 0.1897 - val_recall_2: 0.1609 - val_prec_3: 0.1121 - val_recall_3: 0.0481 - val_prec_4: 0.1424 - val_recall_4: 0.0415 - val_prec_5: 0.1512 - val_recall_5: 0.0613 - val_f1_m: 0.5084 - val_recall_m: 0.4882 - val_precision_m: 0.5373 - val_precision: 0.5373 - val_recall_6: 0.4882 - val_f1score: 0.5084\n",
      "Epoch 113/300\n",
      "epoch:  112\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3691 - acc: 0.8406 - prec: 0.9491 - recall: 0.9607 - prec_1: 0.8612 - recall_1: 0.8844 - prec_2: 0.9359 - recall_2: 0.9111 - prec_3: 0.6898 - recall_3: 0.6682 - prec_4: 0.6794 - recall_4: 0.7019 - prec_5: 0.9321 - recall_5: 0.9419 - f1_m: 0.8377 - recall_m: 0.8241 - precision_m: 0.8520 - precision: 0.8520 - recall_6: 0.8241 - f1score: 0.8377 - val_loss: 1.0412 - val_acc: 0.6184 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1762 - val_recall_1: 0.1238 - val_prec_2: 0.1822 - val_recall_2: 0.1800 - val_prec_3: 0.0801 - val_recall_3: 0.0227 - val_prec_4: 0.1762 - val_recall_4: 0.0853 - val_prec_5: 0.1672 - val_recall_5: 0.0927 - val_f1_m: 0.5820 - val_recall_m: 0.5581 - val_precision_m: 0.6132 - val_precision: 0.6132 - val_recall_6: 0.5581 - val_f1score: 0.5820\n",
      "Epoch 114/300\n",
      "epoch:  113\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3650 - acc: 0.8396 - prec: 0.9653 - recall: 0.9606 - prec_1: 0.8696 - recall_1: 0.8899 - prec_2: 0.9288 - recall_2: 0.9204 - prec_3: 0.6898 - recall_3: 0.6599 - prec_4: 0.6682 - recall_4: 0.7074 - prec_5: 0.9264 - recall_5: 0.9356 - f1_m: 0.8379 - recall_m: 0.8253 - precision_m: 0.8510 - precision: 0.8510 - recall_6: 0.8253 - f1score: 0.8379 - val_loss: 3.8127 - val_acc: 0.3931 - val_prec: 0.1762 - val_recall: 0.0984 - val_prec_1: 0.1732 - val_recall_1: 0.1752 - val_prec_2: 0.1831 - val_recall_2: 0.1134 - val_prec_3: 0.0160 - val_recall_3: 0.0025 - val_prec_4: 0.0320 - val_recall_4: 9.1758e-04 - val_prec_5: 0.1351 - val_recall_5: 0.0348 - val_f1_m: 0.3897 - val_recall_m: 0.3889 - val_precision_m: 0.3906 - val_precision: 0.3906 - val_recall_6: 0.3889 - val_f1score: 0.3897\n",
      "Epoch 115/300\n",
      "epoch:  114\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3652 - acc: 0.8381 - prec: 0.9565 - recall: 0.9656 - prec_1: 0.8588 - recall_1: 0.8793 - prec_2: 0.9434 - recall_2: 0.9161 - prec_3: 0.6787 - recall_3: 0.6545 - prec_4: 0.6629 - recall_4: 0.6927 - prec_5: 0.9195 - recall_5: 0.9370 - f1_m: 0.8357 - recall_m: 0.8216 - precision_m: 0.8506 - precision: 0.8506 - recall_6: 0.8216 - f1score: 0.8357 - val_loss: 0.7596 - val_acc: 0.6401 - val_prec: 0.1762 - val_recall: 0.1651 - val_prec_1: 0.1702 - val_recall_1: 0.0788 - val_prec_2: 0.1703 - val_recall_2: 0.1789 - val_prec_3: 0.0801 - val_recall_3: 0.0158 - val_prec_4: 0.1762 - val_recall_4: 0.0983 - val_prec_5: 0.1672 - val_recall_5: 0.1524 - val_f1_m: 0.5886 - val_recall_m: 0.5623 - val_precision_m: 0.6255 - val_precision: 0.6255 - val_recall_6: 0.5623 - val_f1score: 0.5886\n",
      "Epoch 116/300\n",
      "epoch:  115\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3700 - acc: 0.8370 - prec: 0.9527 - recall: 0.9647 - prec_1: 0.8546 - recall_1: 0.8762 - prec_2: 0.9135 - recall_2: 0.9168 - prec_3: 0.6954 - recall_3: 0.6682 - prec_4: 0.6746 - recall_4: 0.7110 - prec_5: 0.9248 - recall_5: 0.9314 - f1_m: 0.8350 - recall_m: 0.8213 - precision_m: 0.8493 - precision: 0.8493 - recall_6: 0.8213 - f1score: 0.8350 - val_loss: 5.1169 - val_acc: 0.4570 - val_prec: 0.1756 - val_recall: 0.1717 - val_prec_1: 0.1762 - val_recall_1: 0.0402 - val_prec_2: 0.1904 - val_recall_2: 0.1240 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1508 - val_f1_m: 0.4451 - val_recall_m: 0.4324 - val_precision_m: 0.4598 - val_precision: 0.4598 - val_recall_6: 0.4324 - val_f1score: 0.4451\n",
      "Epoch 117/300\n",
      "epoch:  116\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3682 - acc: 0.8367 - prec: 0.9514 - recall: 0.9659 - prec_1: 0.8625 - recall_1: 0.8778 - prec_2: 0.9286 - recall_2: 0.9191 - prec_3: 0.6875 - recall_3: 0.6454 - prec_4: 0.6631 - recall_4: 0.7056 - prec_5: 0.9282 - recall_5: 0.9375 - f1_m: 0.8358 - recall_m: 0.8210 - precision_m: 0.8515 - precision: 0.8515 - recall_6: 0.8210 - f1score: 0.8358 - val_loss: 2.0649 - val_acc: 0.4800 - val_prec: 0.1762 - val_recall: 0.1227 - val_prec_1: 0.1705 - val_recall_1: 0.0713 - val_prec_2: 0.1922 - val_recall_2: 0.1242 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1055 - val_recall_4: 0.0403 - val_prec_5: 0.1672 - val_recall_5: 0.1574 - val_f1_m: 0.4777 - val_recall_m: 0.4690 - val_precision_m: 0.4877 - val_precision: 0.4877 - val_recall_6: 0.4690 - val_f1score: 0.4777\n",
      "Epoch 118/300\n",
      "epoch:  117\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3669 - acc: 0.8403 - prec: 0.9521 - recall: 0.9627 - prec_1: 0.8683 - recall_1: 0.8863 - prec_2: 0.9405 - recall_2: 0.9252 - prec_3: 0.6913 - recall_3: 0.6538 - prec_4: 0.6662 - recall_4: 0.7076 - prec_5: 0.9218 - recall_5: 0.9458 - f1_m: 0.8373 - recall_m: 0.8230 - precision_m: 0.8524 - precision: 0.8524 - recall_6: 0.8230 - f1score: 0.8373 - val_loss: 3.1219 - val_acc: 0.5053 - val_prec: 0.1762 - val_recall: 0.1314 - val_prec_1: 0.1714 - val_recall_1: 0.1727 - val_prec_2: 0.1922 - val_recall_2: 0.1018 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0641 - val_recall_4: 0.0068 - val_prec_5: 0.1672 - val_recall_5: 0.1179 - val_f1_m: 0.5036 - val_recall_m: 0.4997 - val_precision_m: 0.5076 - val_precision: 0.5076 - val_recall_6: 0.4997 - val_f1score: 0.5036\n",
      "Epoch 119/300\n",
      "epoch:  118\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3627 - acc: 0.8411 - prec: 0.9519 - recall: 0.9664 - prec_1: 0.8633 - recall_1: 0.8723 - prec_2: 0.9374 - recall_2: 0.9193 - prec_3: 0.6848 - recall_3: 0.6865 - prec_4: 0.6807 - recall_4: 0.6909 - prec_5: 0.9213 - recall_5: 0.9389 - f1_m: 0.8389 - recall_m: 0.8261 - precision_m: 0.8524 - precision: 0.8524 - recall_6: 0.8261 - f1score: 0.8389 - val_loss: 1.4552 - val_acc: 0.5538 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1731 - val_recall_1: 0.1589 - val_prec_2: 0.1922 - val_recall_2: 0.0992 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1662 - val_recall_4: 0.1199 - val_prec_5: 0.1351 - val_recall_5: 0.0425 - val_f1_m: 0.5399 - val_recall_m: 0.5250 - val_precision_m: 0.5639 - val_precision: 0.5639 - val_recall_6: 0.5250 - val_f1score: 0.5399\n",
      "Epoch 120/300\n",
      "epoch:  119\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 667us/step - loss: 0.3630 - acc: 0.8382 - prec: 0.9490 - recall: 0.9609 - prec_1: 0.8696 - recall_1: 0.8798 - prec_2: 0.9374 - recall_2: 0.9283 - prec_3: 0.6881 - recall_3: 0.6454 - prec_4: 0.6671 - recall_4: 0.7132 - prec_5: 0.9254 - recall_5: 0.9332 - f1_m: 0.8357 - recall_m: 0.8217 - precision_m: 0.8505 - precision: 0.8505 - recall_6: 0.8217 - f1score: 0.8357 - val_loss: 1.6199 - val_acc: 0.5085 - val_prec: 0.1762 - val_recall: 0.1702 - val_prec_1: 0.0961 - val_recall_1: 0.0105 - val_prec_2: 0.1826 - val_recall_2: 0.1538 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1121 - val_recall_4: 0.0578 - val_prec_5: 0.1672 - val_recall_5: 0.1626 - val_f1_m: 0.5070 - val_recall_m: 0.5033 - val_precision_m: 0.5110 - val_precision: 0.5110 - val_recall_6: 0.5033 - val_f1score: 0.5070\n",
      "Epoch 121/300\n",
      "epoch:  120\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3599 - acc: 0.8420 - prec: 0.9561 - recall: 0.9650 - prec_1: 0.8613 - recall_1: 0.8769 - prec_2: 0.9393 - recall_2: 0.9187 - prec_3: 0.7077 - recall_3: 0.6582 - prec_4: 0.6735 - recall_4: 0.7286 - prec_5: 0.9156 - recall_5: 0.9434 - f1_m: 0.8418 - recall_m: 0.8287 - precision_m: 0.8556 - precision: 0.8556 - recall_6: 0.8287 - f1score: 0.8418 - val_loss: 1.6780 - val_acc: 0.5323 - val_prec: 0.1762 - val_recall: 0.1601 - val_prec_1: 0.1602 - val_recall_1: 0.0740 - val_prec_2: 0.1766 - val_recall_2: 0.1831 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1491 - val_f1_m: 0.5337 - val_recall_m: 0.5278 - val_precision_m: 0.5400 - val_precision: 0.5400 - val_recall_6: 0.5278 - val_f1score: 0.5337\n",
      "Epoch 122/300\n",
      "epoch:  121\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 6s 687us/step - loss: 0.3622 - acc: 0.8372 - prec: 0.9571 - recall: 0.9678 - prec_1: 0.8670 - recall_1: 0.8803 - prec_2: 0.9356 - recall_2: 0.9195 - prec_3: 0.6856 - recall_3: 0.6605 - prec_4: 0.6653 - recall_4: 0.6856 - prec_5: 0.9193 - recall_5: 0.9392 - f1_m: 0.8343 - recall_m: 0.8211 - precision_m: 0.8482 - precision: 0.8482 - recall_6: 0.8211 - f1score: 0.8343 - val_loss: 4.8508 - val_acc: 0.4760 - val_prec: 0.1762 - val_recall: 0.1085 - val_prec_1: 0.1717 - val_recall_1: 0.1717 - val_prec_2: 0.1922 - val_recall_2: 0.1329 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0971 - val_f1_m: 0.4761 - val_recall_m: 0.4720 - val_precision_m: 0.4805 - val_precision: 0.4805 - val_recall_6: 0.4720 - val_f1score: 0.4761\n",
      "Epoch 123/300\n",
      "epoch:  122\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3589 - acc: 0.8415 - prec: 0.9656 - recall: 0.9651 - prec_1: 0.8663 - recall_1: 0.8936 - prec_2: 0.9359 - recall_2: 0.9192 - prec_3: 0.6918 - recall_3: 0.6605 - prec_4: 0.6707 - recall_4: 0.7078 - prec_5: 0.9269 - recall_5: 0.9406 - f1_m: 0.8393 - recall_m: 0.8265 - precision_m: 0.8528 - precision: 0.8528 - recall_6: 0.8265 - f1score: 0.8393 - val_loss: 2.2626 - val_acc: 0.5658 - val_prec: 0.1711 - val_recall: 0.1737 - val_prec_1: 0.1762 - val_recall_1: 0.1072 - val_prec_2: 0.1888 - val_recall_2: 0.1780 - val_prec_3: 0.0160 - val_recall_3: 0.0010 - val_prec_4: 0.1281 - val_recall_4: 0.0576 - val_prec_5: 0.1672 - val_recall_5: 0.0964 - val_f1_m: 0.5528 - val_recall_m: 0.5315 - val_precision_m: 0.5797 - val_precision: 0.5797 - val_recall_6: 0.5315 - val_f1score: 0.5528\n",
      "Epoch 124/300\n",
      "epoch:  123\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3584 - acc: 0.8411 - prec: 0.9571 - recall: 0.9636 - prec_1: 0.8596 - recall_1: 0.8826 - prec_2: 0.9320 - recall_2: 0.9208 - prec_3: 0.6902 - recall_3: 0.6703 - prec_4: 0.6759 - recall_4: 0.7003 - prec_5: 0.9299 - recall_5: 0.9397 - f1_m: 0.8404 - recall_m: 0.8283 - precision_m: 0.8531 - precision: 0.8531 - recall_6: 0.8283 - f1score: 0.8404 - val_loss: 4.1778 - val_acc: 0.4690 - val_prec: 0.1762 - val_recall: 0.1498 - val_prec_1: 0.1641 - val_recall_1: 0.1293 - val_prec_2: 0.1602 - val_recall_2: 0.0616 - val_prec_3: 0.0961 - val_recall_3: 0.0321 - val_prec_4: 0.0160 - val_recall_4: 2.5025e-04 - val_prec_5: 0.1672 - val_recall_5: 0.1158 - val_f1_m: 0.4400 - val_recall_m: 0.4342 - val_precision_m: 0.4494 - val_precision: 0.4494 - val_recall_6: 0.4342 - val_f1score: 0.4400\n",
      "Epoch 125/300\n",
      "epoch:  124\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3640 - acc: 0.8410 - prec: 0.9537 - recall: 0.9613 - prec_1: 0.8720 - recall_1: 0.8711 - prec_2: 0.9214 - recall_2: 0.9220 - prec_3: 0.7015 - recall_3: 0.6726 - prec_4: 0.6818 - recall_4: 0.7168 - prec_5: 0.9183 - recall_5: 0.9382 - f1_m: 0.8393 - recall_m: 0.8261 - precision_m: 0.8532 - precision: 0.8532 - recall_6: 0.8261 - f1score: 0.8393 - val_loss: 4.3380 - val_acc: 0.3801 - val_prec: 0.1680 - val_recall: 0.1747 - val_prec_1: 0.1748 - val_recall_1: 0.0781 - val_prec_2: 0.1902 - val_recall_2: 0.1185 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1351 - val_recall_5: 0.0365 - val_f1_m: 0.3767 - val_recall_m: 0.3694 - val_precision_m: 0.3849 - val_precision: 0.3849 - val_recall_6: 0.3694 - val_f1score: 0.3767\n",
      "Epoch 126/300\n",
      "epoch:  125\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3628 - acc: 0.8405 - prec: 0.9520 - recall: 0.9633 - prec_1: 0.8712 - recall_1: 0.8797 - prec_2: 0.9355 - recall_2: 0.9257 - prec_3: 0.7030 - recall_3: 0.6537 - prec_4: 0.6656 - recall_4: 0.7210 - prec_5: 0.9237 - recall_5: 0.9373 - f1_m: 0.8370 - recall_m: 0.8241 - precision_m: 0.8506 - precision: 0.8506 - recall_6: 0.8241 - f1score: 0.8370 - val_loss: 6.3693 - val_acc: 0.3899 - val_prec: 0.1668 - val_recall: 0.1754 - val_prec_1: 0.1750 - val_recall_1: 0.0422 - val_prec_2: 0.1842 - val_recall_2: 0.1517 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1512 - val_recall_5: 0.0495 - val_f1_m: 0.3853 - val_recall_m: 0.3774 - val_precision_m: 0.3946 - val_precision: 0.3946 - val_recall_6: 0.3774 - val_f1score: 0.3853\n",
      "Epoch 127/300\n",
      "epoch:  126\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3598 - acc: 0.8418 - prec: 0.9556 - recall: 0.9697 - prec_1: 0.8772 - recall_1: 0.8888 - prec_2: 0.9383 - recall_2: 0.9230 - prec_3: 0.6745 - recall_3: 0.6681 - prec_4: 0.6647 - recall_4: 0.6842 - prec_5: 0.9320 - recall_5: 0.9426 - f1_m: 0.8384 - recall_m: 0.8260 - precision_m: 0.8515 - precision: 0.8515 - recall_6: 0.8260 - f1score: 0.8384 - val_loss: 1.1496 - val_acc: 0.5931 - val_prec: 0.1762 - val_recall: 0.1307 - val_prec_1: 0.1598 - val_recall_1: 0.0813 - val_prec_2: 0.1842 - val_recall_2: 0.0858 - val_prec_3: 0.1702 - val_recall_3: 0.1197 - val_prec_4: 0.1281 - val_recall_4: 0.0495 - val_prec_5: 0.1672 - val_recall_5: 0.1614 - val_f1_m: 0.5766 - val_recall_m: 0.5571 - val_precision_m: 0.6143 - val_precision: 0.6143 - val_recall_6: 0.5571 - val_f1score: 0.5766\n",
      "Epoch 128/300\n",
      "epoch:  127\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.3508 - acc: 0.8446 - prec: 0.9569 - recall: 0.9653 - prec_1: 0.8670 - recall_1: 0.8937 - prec_2: 0.9321 - recall_2: 0.9236 - prec_3: 0.7034 - recall_3: 0.6453 - prec_4: 0.6675 - recall_4: 0.7305 - prec_5: 0.9394 - recall_5: 0.9378 - f1_m: 0.8424 - recall_m: 0.8298 - precision_m: 0.8556 - precision: 0.8556 - recall_6: 0.8298 - f1score: 0.8424 - val_loss: 5.9013 - val_acc: 0.4017 - val_prec: 0.1662 - val_recall: 0.1720 - val_prec_1: 0.1640 - val_recall_1: 0.0928 - val_prec_2: 0.1762 - val_recall_2: 0.0723 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1512 - val_recall_5: 0.0803 - val_f1_m: 0.3985 - val_recall_m: 0.3921 - val_precision_m: 0.4056 - val_precision: 0.4056 - val_recall_6: 0.3921 - val_f1score: 0.3985\n",
      "Epoch 129/300\n",
      "epoch:  128\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3580 - acc: 0.8447 - prec: 0.9525 - recall: 0.9681 - prec_1: 0.8818 - recall_1: 0.8849 - prec_2: 0.9383 - recall_2: 0.9280 - prec_3: 0.6910 - recall_3: 0.6877 - prec_4: 0.6780 - recall_4: 0.6940 - prec_5: 0.9243 - recall_5: 0.9456 - f1_m: 0.8429 - recall_m: 0.8292 - precision_m: 0.8572 - precision: 0.8572 - recall_6: 0.8292 - f1score: 0.8429 - val_loss: 0.7076 - val_acc: 0.6994 - val_prec: 0.1762 - val_recall: 0.1311 - val_prec_1: 0.1726 - val_recall_1: 0.1700 - val_prec_2: 0.1922 - val_recall_2: 0.1126 - val_prec_3: 0.1685 - val_recall_3: 0.0815 - val_prec_4: 0.1762 - val_recall_4: 0.1124 - val_prec_5: 0.1672 - val_recall_5: 0.1318 - val_f1_m: 0.6749 - val_recall_m: 0.6534 - val_precision_m: 0.7124 - val_precision: 0.7124 - val_recall_6: 0.6534 - val_f1score: 0.6749\n",
      "Epoch 130/300\n",
      "epoch:  129\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3499 - acc: 0.8421 - prec: 0.9645 - recall: 0.9705 - prec_1: 0.8583 - recall_1: 0.8865 - prec_2: 0.9372 - recall_2: 0.9219 - prec_3: 0.6899 - recall_3: 0.6586 - prec_4: 0.6720 - recall_4: 0.7088 - prec_5: 0.9343 - recall_5: 0.9374 - f1_m: 0.8396 - recall_m: 0.8276 - precision_m: 0.8521 - precision: 0.8521 - recall_6: 0.8276 - f1score: 0.8396 - val_loss: 4.6476 - val_acc: 0.5178 - val_prec: 0.1762 - val_recall: 0.1739 - val_prec_1: 0.1751 - val_recall_1: 0.1139 - val_prec_2: 0.1810 - val_recall_2: 0.1427 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1150 - val_f1_m: 0.5121 - val_recall_m: 0.5003 - val_precision_m: 0.5255 - val_precision: 0.5255 - val_recall_6: 0.5003 - val_f1score: 0.5121\n",
      "Epoch 131/300\n",
      "epoch:  130\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3522 - acc: 0.8435 - prec: 0.9597 - recall: 0.9687 - prec_1: 0.8711 - recall_1: 0.8745 - prec_2: 0.9364 - recall_2: 0.9207 - prec_3: 0.6956 - recall_3: 0.6790 - prec_4: 0.6786 - recall_4: 0.7041 - prec_5: 0.9214 - recall_5: 0.9469 - f1_m: 0.8420 - recall_m: 0.8292 - precision_m: 0.8555 - precision: 0.8555 - recall_6: 0.8292 - f1score: 0.8420 - val_loss: 2.9519 - val_acc: 0.4667 - val_prec: 0.1762 - val_recall: 0.1630 - val_prec_1: 0.0961 - val_recall_1: 0.0099 - val_prec_2: 0.1696 - val_recall_2: 0.1889 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.1401 - val_f1_m: 0.4634 - val_recall_m: 0.4607 - val_precision_m: 0.4661 - val_precision: 0.4661 - val_recall_6: 0.4607 - val_f1score: 0.4634\n",
      "Epoch 132/300\n",
      "epoch:  131\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3550 - acc: 0.8416 - prec: 0.9608 - recall: 0.9616 - prec_1: 0.8703 - recall_1: 0.8758 - prec_2: 0.9360 - recall_2: 0.9232 - prec_3: 0.6997 - recall_3: 0.6609 - prec_4: 0.6743 - recall_4: 0.7190 - prec_5: 0.9220 - recall_5: 0.9421 - f1_m: 0.8416 - recall_m: 0.8295 - precision_m: 0.8543 - precision: 0.8543 - recall_6: 0.8295 - f1score: 0.8416 - val_loss: 5.8225 - val_acc: 0.4437 - val_prec: 0.1669 - val_recall: 0.1736 - val_prec_1: 0.1715 - val_recall_1: 0.1159 - val_prec_2: 0.1869 - val_recall_2: 0.0958 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0796 - val_f1_m: 0.4416 - val_recall_m: 0.4349 - val_precision_m: 0.4490 - val_precision: 0.4490 - val_recall_6: 0.4349 - val_f1score: 0.4416\n",
      "Epoch 133/300\n",
      "epoch:  132\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 679us/step - loss: 0.3562 - acc: 0.8403 - prec: 0.9518 - recall: 0.9696 - prec_1: 0.8730 - recall_1: 0.8940 - prec_2: 0.9418 - recall_2: 0.9207 - prec_3: 0.6776 - recall_3: 0.6741 - prec_4: 0.6609 - recall_4: 0.6705 - prec_5: 0.9306 - recall_5: 0.9374 - f1_m: 0.8395 - recall_m: 0.8265 - precision_m: 0.8533 - precision: 0.8533 - recall_6: 0.8265 - f1score: 0.8395 - val_loss: 5.1249 - val_acc: 0.4832 - val_prec: 0.1694 - val_recall: 0.1749 - val_prec_1: 0.1762 - val_recall_1: 0.0668 - val_prec_2: 0.1869 - val_recall_2: 0.1828 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0978 - val_f1_m: 0.4743 - val_recall_m: 0.4615 - val_precision_m: 0.4903 - val_precision: 0.4903 - val_recall_6: 0.4615 - val_f1score: 0.4743\n",
      "Epoch 134/300\n",
      "epoch:  133\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3518 - acc: 0.8425 - prec: 0.9560 - recall: 0.9658 - prec_1: 0.8711 - recall_1: 0.8917 - prec_2: 0.9410 - recall_2: 0.9250 - prec_3: 0.6881 - recall_3: 0.6490 - prec_4: 0.6646 - recall_4: 0.7077 - prec_5: 0.9347 - recall_5: 0.9429 - f1_m: 0.8418 - recall_m: 0.8306 - precision_m: 0.8534 - precision: 0.8534 - recall_6: 0.8306 - f1score: 0.8418 - val_loss: 1.3320 - val_acc: 0.5568 - val_prec: 0.1664 - val_recall: 0.1717 - val_prec_1: 0.1602 - val_recall_1: 0.0455 - val_prec_2: 0.1813 - val_recall_2: 0.1634 - val_prec_3: 0.1762 - val_recall_3: 0.1102 - val_prec_4: 0.1602 - val_recall_4: 0.0543 - val_prec_5: 0.1512 - val_recall_5: 0.0628 - val_f1_m: 0.5361 - val_recall_m: 0.5235 - val_precision_m: 0.5513 - val_precision: 0.5513 - val_recall_6: 0.5235 - val_f1score: 0.5361\n",
      "Epoch 135/300\n",
      "epoch:  134\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3507 - acc: 0.8478 - prec: 0.9575 - recall: 0.9710 - prec_1: 0.8767 - recall_1: 0.9010 - prec_2: 0.9464 - recall_2: 0.9206 - prec_3: 0.7075 - recall_3: 0.6673 - prec_4: 0.6775 - recall_4: 0.7277 - prec_5: 0.9292 - recall_5: 0.9427 - f1_m: 0.8452 - recall_m: 0.8326 - precision_m: 0.8583 - precision: 0.8583 - recall_6: 0.8326 - f1score: 0.8452 - val_loss: 2.3938 - val_acc: 0.4517 - val_prec: 0.1685 - val_recall: 0.1749 - val_prec_1: 0.1754 - val_recall_1: 0.0486 - val_prec_2: 0.1869 - val_recall_2: 0.1639 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0986 - val_f1_m: 0.4421 - val_recall_m: 0.4302 - val_precision_m: 0.4565 - val_precision: 0.4565 - val_recall_6: 0.4302 - val_f1score: 0.4421\n",
      "Epoch 136/300\n",
      "epoch:  135\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3441 - acc: 0.8485 - prec: 0.9594 - recall: 0.9740 - prec_1: 0.8815 - recall_1: 0.8819 - prec_2: 0.9285 - recall_2: 0.9324 - prec_3: 0.7012 - recall_3: 0.6821 - prec_4: 0.6846 - recall_4: 0.7147 - prec_5: 0.9389 - recall_5: 0.9473 - f1_m: 0.8466 - recall_m: 0.8341 - precision_m: 0.8597 - precision: 0.8597 - recall_6: 0.8341 - f1score: 0.8466 - val_loss: 1.5550 - val_acc: 0.5681 - val_prec: 0.1664 - val_recall: 0.1734 - val_prec_1: 0.1598 - val_recall_1: 0.0767 - val_prec_2: 0.1922 - val_recall_2: 0.0844 - val_prec_3: 0.1441 - val_recall_3: 0.0694 - val_prec_4: 0.1762 - val_recall_4: 0.1209 - val_prec_5: 0.1512 - val_recall_5: 0.0723 - val_f1_m: 0.5504 - val_recall_m: 0.5355 - val_precision_m: 0.5703 - val_precision: 0.5703 - val_recall_6: 0.5355 - val_f1score: 0.5504\n",
      "Epoch 137/300\n",
      "epoch:  136\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3456 - acc: 0.8465 - prec: 0.9566 - recall: 0.9663 - prec_1: 0.8748 - recall_1: 0.8981 - prec_2: 0.9390 - recall_2: 0.9315 - prec_3: 0.6998 - recall_3: 0.6528 - prec_4: 0.6693 - recall_4: 0.7174 - prec_5: 0.9402 - recall_5: 0.9439 - f1_m: 0.8442 - recall_m: 0.8327 - precision_m: 0.8562 - precision: 0.8562 - recall_6: 0.8327 - f1score: 0.8442 - val_loss: 3.1295 - val_acc: 0.5931 - val_prec: 0.1762 - val_recall: 0.1718 - val_prec_1: 0.1736 - val_recall_1: 0.1465 - val_prec_2: 0.1869 - val_recall_2: 0.1061 - val_prec_3: 0.0160 - val_recall_3: 0.0018 - val_prec_4: 0.1121 - val_recall_4: 0.0558 - val_prec_5: 0.1672 - val_recall_5: 0.1488 - val_f1_m: 0.5867 - val_recall_m: 0.5771 - val_precision_m: 0.5970 - val_precision: 0.5970 - val_recall_6: 0.5771 - val_f1score: 0.5867\n",
      "Epoch 138/300\n",
      "epoch:  137\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3496 - acc: 0.8440 - prec: 0.9539 - recall: 0.9713 - prec_1: 0.8727 - recall_1: 0.9003 - prec_2: 0.9470 - recall_2: 0.9279 - prec_3: 0.6636 - recall_3: 0.6586 - prec_4: 0.6701 - recall_4: 0.6837 - prec_5: 0.9415 - recall_5: 0.9449 - f1_m: 0.8415 - recall_m: 0.8303 - precision_m: 0.8533 - precision: 0.8533 - recall_6: 0.8303 - f1score: 0.8415 - val_loss: 2.4856 - val_acc: 0.5268 - val_prec: 0.1762 - val_recall: 0.1698 - val_prec_1: 0.1441 - val_recall_1: 0.0248 - val_prec_2: 0.1899 - val_recall_2: 0.1563 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1441 - val_recall_4: 0.0430 - val_prec_5: 0.1666 - val_recall_5: 0.1629 - val_f1_m: 0.5130 - val_recall_m: 0.5060 - val_precision_m: 0.5207 - val_precision: 0.5207 - val_recall_6: 0.5060 - val_f1score: 0.5130\n",
      "Epoch 139/300\n",
      "epoch:  138\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.3437 - acc: 0.8521 - prec: 0.9572 - recall: 0.9701 - prec_1: 0.8881 - recall_1: 0.8896 - prec_2: 0.9352 - recall_2: 0.9271 - prec_3: 0.7103 - recall_3: 0.6897 - prec_4: 0.6933 - recall_4: 0.7189 - prec_5: 0.9292 - recall_5: 0.9490 - f1_m: 0.8501 - recall_m: 0.8391 - precision_m: 0.8615 - precision: 0.8615 - recall_6: 0.8391 - f1score: 0.8501 - val_loss: 3.6307 - val_acc: 0.4187 - val_prec: 0.1762 - val_recall: 0.1639 - val_prec_1: 0.0801 - val_recall_1: 0.0081 - val_prec_2: 0.1805 - val_recall_2: 0.1914 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1672 - val_recall_5: 0.0917 - val_f1_m: 0.4171 - val_recall_m: 0.4164 - val_precision_m: 0.4178 - val_precision: 0.4178 - val_recall_6: 0.4164 - val_f1score: 0.4171\n",
      "Epoch 140/300\n",
      "epoch:  139\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3465 - acc: 0.8457 - prec: 0.9569 - recall: 0.9666 - prec_1: 0.8892 - recall_1: 0.8874 - prec_2: 0.9362 - recall_2: 0.9339 - prec_3: 0.6911 - recall_3: 0.6611 - prec_4: 0.6713 - recall_4: 0.7058 - prec_5: 0.9235 - recall_5: 0.9458 - f1_m: 0.8450 - recall_m: 0.8328 - precision_m: 0.8578 - precision: 0.8578 - recall_6: 0.8328 - f1score: 0.8450 - val_loss: 5.1519 - val_acc: 0.3108 - val_prec: 0.1670 - val_recall: 0.1752 - val_prec_1: 0.1758 - val_recall_1: 0.0472 - val_prec_2: 0.1922 - val_recall_2: 0.0830 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.0961 - val_recall_5: 0.0245 - val_f1_m: 0.3075 - val_recall_m: 0.3033 - val_precision_m: 0.3123 - val_precision: 0.3123 - val_recall_6: 0.3033 - val_f1score: 0.3075\n",
      "Epoch 141/300\n",
      "epoch:  140\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3484 - acc: 0.8448 - prec: 0.9505 - recall: 0.9684 - prec_1: 0.8703 - recall_1: 0.8932 - prec_2: 0.9390 - recall_2: 0.9272 - prec_3: 0.6964 - recall_3: 0.6690 - prec_4: 0.6827 - recall_4: 0.7129 - prec_5: 0.9304 - recall_5: 0.9325 - f1_m: 0.8438 - recall_m: 0.8310 - precision_m: 0.8574 - precision: 0.8574 - recall_6: 0.8310 - f1score: 0.8438 - val_loss: 1.6742 - val_acc: 0.5941 - val_prec: 0.1762 - val_recall: 0.1663 - val_prec_1: 0.1762 - val_recall_1: 0.0459 - val_prec_2: 0.1922 - val_recall_2: 0.1063 - val_prec_3: 0.1701 - val_recall_3: 0.1395 - val_prec_4: 0.1441 - val_recall_4: 0.0045 - val_prec_5: 0.1672 - val_recall_5: 0.1631 - val_f1_m: 0.5797 - val_recall_m: 0.5691 - val_precision_m: 0.5918 - val_precision: 0.5918 - val_recall_6: 0.5691 - val_f1score: 0.5797\n",
      "Epoch 142/300\n",
      "epoch:  141\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3377 - acc: 0.8524 - prec: 0.9584 - recall: 0.9695 - prec_1: 0.8901 - recall_1: 0.8948 - prec_2: 0.9431 - recall_2: 0.9340 - prec_3: 0.7008 - recall_3: 0.6575 - prec_4: 0.6815 - recall_4: 0.7297 - prec_5: 0.9406 - recall_5: 0.9493 - f1_m: 0.8520 - recall_m: 0.8416 - precision_m: 0.8630 - precision: 0.8630 - recall_6: 0.8416 - f1score: 0.8520 - val_loss: 2.8690 - val_acc: 0.4630 - val_prec: 0.1677 - val_recall: 0.1714 - val_prec_1: 0.1762 - val_recall_1: 0.0443 - val_prec_2: 0.1832 - val_recall_2: 0.1790 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1281 - val_recall_4: 0.0528 - val_prec_5: 0.1512 - val_recall_5: 0.0611 - val_f1_m: 0.4574 - val_recall_m: 0.4505 - val_precision_m: 0.4660 - val_precision: 0.4660 - val_recall_6: 0.4505 - val_f1score: 0.4574\n",
      "Epoch 143/300\n",
      "epoch:  142\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3436 - acc: 0.8514 - prec: 0.9574 - recall: 0.9684 - prec_1: 0.8804 - recall_1: 0.9033 - prec_2: 0.9466 - recall_2: 0.9208 - prec_3: 0.7004 - recall_3: 0.6855 - prec_4: 0.6843 - recall_4: 0.7023 - prec_5: 0.9389 - recall_5: 0.9476 - f1_m: 0.8502 - recall_m: 0.8388 - precision_m: 0.8622 - precision: 0.8622 - recall_6: 0.8388 - f1score: 0.8502 - val_loss: 6.6520 - val_acc: 0.3726 - val_prec: 0.1688 - val_recall: 0.1749 - val_prec_1: 0.1441 - val_recall_1: 0.0266 - val_prec_2: 0.1826 - val_recall_2: 0.1790 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1351 - val_recall_5: 0.0268 - val_f1_m: 0.3700 - val_recall_m: 0.3661 - val_precision_m: 0.3745 - val_precision: 0.3745 - val_recall_6: 0.3661 - val_f1score: 0.3700\n",
      "Epoch 144/300\n",
      "epoch:  143\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3443 - acc: 0.8442 - prec: 0.9547 - recall: 0.9647 - prec_1: 0.8752 - recall_1: 0.9015 - prec_2: 0.9533 - recall_2: 0.9251 - prec_3: 0.6812 - recall_3: 0.6699 - prec_4: 0.6654 - recall_4: 0.6887 - prec_5: 0.9311 - recall_5: 0.9456 - f1_m: 0.8434 - recall_m: 0.8321 - precision_m: 0.8551 - precision: 0.8551 - recall_6: 0.8321 - f1score: 0.8434 - val_loss: 4.1569 - val_acc: 0.5253 - val_prec: 0.1733 - val_recall: 0.1590 - val_prec_1: 0.1710 - val_recall_1: 0.1587 - val_prec_2: 0.1869 - val_recall_2: 0.0906 - val_prec_3: 0.1121 - val_recall_3: 0.0539 - val_prec_4: 0.0801 - val_recall_4: 0.0268 - val_prec_5: 0.1672 - val_recall_5: 0.0721 - val_f1_m: 0.5248 - val_recall_m: 0.5210 - val_precision_m: 0.5289 - val_precision: 0.5289 - val_recall_6: 0.5210 - val_f1score: 0.5248\n",
      "Epoch 145/300\n",
      "epoch:  144\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 681us/step - loss: 0.3377 - acc: 0.8487 - prec: 0.9667 - recall: 0.9711 - prec_1: 0.8922 - recall_1: 0.8934 - prec_2: 0.9462 - recall_2: 0.9348 - prec_3: 0.6859 - recall_3: 0.6965 - prec_4: 0.6831 - recall_4: 0.6799 - prec_5: 0.9259 - recall_5: 0.9492 - f1_m: 0.8477 - recall_m: 0.8362 - precision_m: 0.8596 - precision: 0.8596 - recall_6: 0.8362 - f1score: 0.8477 - val_loss: 1.8513 - val_acc: 0.4810 - val_prec: 0.1717 - val_recall: 0.1749 - val_prec_1: 0.1752 - val_recall_1: 0.1189 - val_prec_2: 0.1890 - val_recall_2: 0.1632 - val_prec_3: 0.1121 - val_recall_3: 0.0308 - val_prec_4: 0.0641 - val_recall_4: 0.0190 - val_prec_5: 0.0711 - val_recall_5: 0.0113 - val_f1_m: 0.4663 - val_recall_m: 0.4590 - val_precision_m: 0.4744 - val_precision: 0.4744 - val_recall_6: 0.4590 - val_f1score: 0.4663\n",
      "Epoch 146/300\n",
      "epoch:  145\n",
      "Learning rate:  0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3406 - acc: 0.8487 - prec: 0.9633 - recall: 0.9740 - prec_1: 0.8873 - recall_1: 0.8959 - prec_2: 0.9389 - recall_2: 0.9338 - prec_3: 0.7017 - recall_3: 0.6610 - prec_4: 0.6755 - recall_4: 0.7223 - prec_5: 0.9345 - recall_5: 0.9451 - f1_m: 0.8469 - recall_m: 0.8363 - precision_m: 0.8580 - precision: 0.8580 - recall_6: 0.8363 - f1score: 0.8469 - val_loss: 4.9173 - val_acc: 0.4815 - val_prec: 0.1762 - val_recall: 0.1079 - val_prec_1: 0.1743 - val_recall_1: 0.1620 - val_prec_2: 0.1909 - val_recall_2: 0.1662 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.0000e+00 - val_recall_4: 0.0000e+00 - val_prec_5: 0.1666 - val_recall_5: 0.0865 - val_f1_m: 0.4779 - val_recall_m: 0.4710 - val_precision_m: 0.4856 - val_precision: 0.4856 - val_recall_6: 0.4710 - val_f1score: 0.4779\n",
      "Epoch 147/300\n",
      "epoch:  146\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3331 - acc: 0.8529 - prec: 0.9638 - recall: 0.9660 - prec_1: 0.8879 - recall_1: 0.9025 - prec_2: 0.9468 - recall_2: 0.9387 - prec_3: 0.6996 - recall_3: 0.6609 - prec_4: 0.6805 - recall_4: 0.7330 - prec_5: 0.9375 - recall_5: 0.9448 - f1_m: 0.8512 - recall_m: 0.8402 - precision_m: 0.8626 - precision: 0.8626 - recall_6: 0.8402 - f1score: 0.8512 - val_loss: 1.0185 - val_acc: 0.5918 - val_prec: 0.1756 - val_recall: 0.1693 - val_prec_1: 0.1762 - val_recall_1: 0.0455 - val_prec_2: 0.1869 - val_recall_2: 0.1694 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1826 - val_recall_4: 0.1114 - val_prec_5: 0.1672 - val_recall_5: 0.1503 - val_f1_m: 0.5461 - val_recall_m: 0.5218 - val_precision_m: 0.5900 - val_precision: 0.5900 - val_recall_6: 0.5218 - val_f1score: 0.5461\n",
      "Epoch 148/300\n",
      "epoch:  147\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.3390 - acc: 0.8457 - prec: 0.9625 - recall: 0.9658 - prec_1: 0.8745 - recall_1: 0.9032 - prec_2: 0.9435 - recall_2: 0.9295 - prec_3: 0.6805 - recall_3: 0.6790 - prec_4: 0.6788 - recall_4: 0.6819 - prec_5: 0.9386 - recall_5: 0.9397 - f1_m: 0.8458 - recall_m: 0.8345 - precision_m: 0.8576 - precision: 0.8576 - recall_6: 0.8345 - f1score: 0.8458 - val_loss: 1.8061 - val_acc: 0.6201 - val_prec: 0.1755 - val_recall: 0.1679 - val_prec_1: 0.1756 - val_recall_1: 0.1646 - val_prec_2: 0.1769 - val_recall_2: 0.1484 - val_prec_3: 0.0160 - val_recall_3: 2.5025e-04 - val_prec_4: 0.1441 - val_recall_4: 0.0516 - val_prec_5: 0.1672 - val_recall_5: 0.1255 - val_f1_m: 0.6181 - val_recall_m: 0.6071 - val_precision_m: 0.6302 - val_precision: 0.6302 - val_recall_6: 0.6071 - val_f1score: 0.6181\n",
      "Epoch 149/300\n",
      "epoch:  148\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3357 - acc: 0.8473 - prec: 0.9547 - recall: 0.9683 - prec_1: 0.8895 - recall_1: 0.9081 - prec_2: 0.9538 - recall_2: 0.9317 - prec_3: 0.6779 - recall_3: 0.6673 - prec_4: 0.6671 - recall_4: 0.6886 - prec_5: 0.9367 - recall_5: 0.9455 - f1_m: 0.8464 - recall_m: 0.8357 - precision_m: 0.8575 - precision: 0.8575 - recall_6: 0.8357 - f1score: 0.8464 - val_loss: 2.2046 - val_acc: 0.5908 - val_prec: 0.1685 - val_recall: 0.1729 - val_prec_1: 0.1762 - val_recall_1: 0.0695 - val_prec_2: 0.1863 - val_recall_2: 0.1820 - val_prec_3: 0.0480 - val_recall_3: 0.0170 - val_prec_4: 0.1602 - val_recall_4: 0.1106 - val_prec_5: 0.1672 - val_recall_5: 0.0764 - val_f1_m: 0.5866 - val_recall_m: 0.5763 - val_precision_m: 0.5988 - val_precision: 0.5988 - val_recall_6: 0.5763 - val_f1score: 0.5866\n",
      "Epoch 150/300\n",
      "epoch:  149\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 679us/step - loss: 0.3380 - acc: 0.8512 - prec: 0.9606 - recall: 0.9726 - prec_1: 0.8680 - recall_1: 0.9008 - prec_2: 0.9398 - recall_2: 0.9315 - prec_3: 0.7040 - recall_3: 0.6806 - prec_4: 0.6884 - recall_4: 0.7235 - prec_5: 0.9373 - recall_5: 0.9404 - f1_m: 0.8497 - recall_m: 0.8382 - precision_m: 0.8617 - precision: 0.8617 - recall_6: 0.8382 - f1score: 0.8497 - val_loss: 3.0349 - val_acc: 0.4962 - val_prec: 0.1680 - val_recall: 0.1691 - val_prec_1: 0.1741 - val_recall_1: 0.0968 - val_prec_2: 0.1838 - val_recall_2: 0.1720 - val_prec_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_prec_4: 0.1441 - val_recall_4: 0.0553 - val_prec_5: 0.1512 - val_recall_5: 0.0398 - val_f1_m: 0.4645 - val_recall_m: 0.4492 - val_precision_m: 0.4888 - val_precision: 0.4888 - val_recall_6: 0.4492 - val_f1score: 0.4645\n",
      "Epoch 151/300\n",
      "epoch:  150\n",
      "Learning rate:  0.01\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3422 - acc: 0.8503 - prec: 0.9617 - recall: 0.9656 - prec_1: 0.8811 - recall_1: 0.8869 - prec_2: 0.9254 - recall_2: 0.9362 - prec_3: 0.7050 - recall_3: 0.7031 - prec_4: 0.6926 - recall_4: 0.7009 - prec_5: 0.9350 - recall_5: 0.9419 - f1_m: 0.8489 - recall_m: 0.8377 - precision_m: 0.8607 - precision: 0.8607 - recall_6: 0.8377 - f1score: 0.8489 - val_loss: 1.2506 - val_acc: 0.6099 - val_prec: 0.1715 - val_recall: 0.1704 - val_prec_1: 0.1762 - val_recall_1: 0.0960 - val_prec_2: 0.1875 - val_recall_2: 0.1810 - val_prec_3: 0.0961 - val_recall_3: 0.0316 - val_prec_4: 0.1662 - val_recall_4: 0.1069 - val_prec_5: 0.1656 - val_recall_5: 0.0750 - val_f1_m: 0.6055 - val_recall_m: 0.5861 - val_precision_m: 0.6301 - val_precision: 0.6301 - val_recall_6: 0.5861 - val_f1score: 0.6055\n",
      "Epoch 152/300\n",
      "epoch:  151\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3357 - acc: 0.8538 - prec: 0.9572 - recall: 0.9696 - prec_1: 0.8704 - recall_1: 0.9139 - prec_2: 0.9569 - recall_2: 0.9158 - prec_3: 0.7036 - recall_3: 0.7062 - prec_4: 0.6981 - recall_4: 0.7054 - prec_5: 0.9327 - recall_5: 0.9475 - f1_m: 0.8529 - recall_m: 0.8428 - precision_m: 0.8635 - precision: 0.8635 - recall_6: 0.8428 - f1score: 0.8529 - val_loss: 0.6741 - val_acc: 0.7778 - val_prec: 0.1762 - val_recall: 0.1707 - val_prec_1: 0.1759 - val_recall_1: 0.1498 - val_prec_2: 0.1922 - val_recall_2: 0.1707 - val_prec_3: 0.1702 - val_recall_3: 0.1752 - val_prec_4: 0.0480 - val_recall_4: 0.0043 - val_prec_5: 0.1672 - val_recall_5: 0.1569 - val_f1_m: 0.7776 - val_recall_m: 0.7683 - val_precision_m: 0.7874 - val_precision: 0.7874 - val_recall_6: 0.7683 - val_f1score: 0.7776\n",
      "Epoch 153/300\n",
      "epoch:  152\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3231 - acc: 0.8572 - prec: 0.9582 - recall: 0.9730 - prec_1: 0.8872 - recall_1: 0.9162 - prec_2: 0.9476 - recall_2: 0.9347 - prec_3: 0.6980 - recall_3: 0.7020 - prec_4: 0.6911 - recall_4: 0.7024 - prec_5: 0.9490 - recall_5: 0.9490 - f1_m: 0.8558 - recall_m: 0.8450 - precision_m: 0.8671 - precision: 0.8671 - recall_6: 0.8450 - f1score: 0.8558 - val_loss: 1.5563 - val_acc: 0.7533 - val_prec: 0.1762 - val_recall: 0.1718 - val_prec_1: 0.1759 - val_recall_1: 0.1663 - val_prec_2: 0.1909 - val_recall_2: 0.1687 - val_prec_3: 0.1281 - val_recall_3: 0.0644 - val_prec_4: 0.1662 - val_recall_4: 0.0921 - val_prec_5: 0.1672 - val_recall_5: 0.1446 - val_f1_m: 0.7528 - val_recall_m: 0.7460 - val_precision_m: 0.7599 - val_precision: 0.7599 - val_recall_6: 0.7460 - val_f1score: 0.7528\n",
      "Epoch 154/300\n",
      "epoch:  153\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3207 - acc: 0.8628 - prec: 0.9653 - recall: 0.9758 - prec_1: 0.8992 - recall_1: 0.9235 - prec_2: 0.9560 - recall_2: 0.9364 - prec_3: 0.7107 - recall_3: 0.6962 - prec_4: 0.6945 - recall_4: 0.7222 - prec_5: 0.9498 - recall_5: 0.9547 - f1_m: 0.8617 - recall_m: 0.8513 - precision_m: 0.8725 - precision: 0.8725 - recall_6: 0.8513 - f1score: 0.8617 - val_loss: 0.6041 - val_acc: 0.8063 - val_prec: 0.1762 - val_recall: 0.1696 - val_prec_1: 0.1759 - val_recall_1: 0.1483 - val_prec_2: 0.1897 - val_recall_2: 0.1732 - val_prec_3: 0.1441 - val_recall_3: 0.0968 - val_prec_4: 0.1762 - val_recall_4: 0.1199 - val_prec_5: 0.1672 - val_recall_5: 0.1574 - val_f1_m: 0.8058 - val_recall_m: 0.7968 - val_precision_m: 0.8153 - val_precision: 0.8153 - val_recall_6: 0.7968 - val_f1score: 0.8058\n",
      "Epoch 155/300\n",
      "epoch:  154\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3164 - acc: 0.8621 - prec: 0.9679 - recall: 0.9727 - prec_1: 0.8984 - recall_1: 0.9144 - prec_2: 0.9416 - recall_2: 0.9366 - prec_3: 0.7236 - recall_3: 0.6886 - prec_4: 0.6949 - recall_4: 0.7417 - prec_5: 0.9425 - recall_5: 0.9583 - f1_m: 0.8605 - recall_m: 0.8501 - precision_m: 0.8714 - precision: 0.8714 - recall_6: 0.8501 - f1score: 0.8605 - val_loss: 0.5072 - val_acc: 0.8061 - val_prec: 0.1762 - val_recall: 0.1707 - val_prec_1: 0.1758 - val_recall_1: 0.1546 - val_prec_2: 0.1897 - val_recall_2: 0.1787 - val_prec_3: 0.0641 - val_recall_3: 0.0250 - val_prec_4: 0.1822 - val_recall_4: 0.1869 - val_prec_5: 0.1672 - val_recall_5: 0.1571 - val_f1_m: 0.8046 - val_recall_m: 0.7928 - val_precision_m: 0.8171 - val_precision: 0.8171 - val_recall_6: 0.7928 - val_f1score: 0.8046\n",
      "Epoch 156/300\n",
      "epoch:  155\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 6s 689us/step - loss: 0.3186 - acc: 0.8569 - prec: 0.9659 - recall: 0.9704 - prec_1: 0.8982 - recall_1: 0.9103 - prec_2: 0.9450 - recall_2: 0.9419 - prec_3: 0.7139 - recall_3: 0.6761 - prec_4: 0.6785 - recall_4: 0.7324 - prec_5: 0.9438 - recall_5: 0.9540 - f1_m: 0.8552 - recall_m: 0.8452 - precision_m: 0.8656 - precision: 0.8656 - recall_6: 0.8452 - f1score: 0.8552 - val_loss: 0.3727 - val_acc: 0.8253 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1756 - val_recall_1: 0.1653 - val_prec_2: 0.1907 - val_recall_2: 0.1649 - val_prec_3: 0.1762 - val_recall_3: 0.1177 - val_prec_4: 0.1916 - val_recall_4: 0.1284 - val_prec_5: 0.1672 - val_recall_5: 0.1528 - val_f1_m: 0.8260 - val_recall_m: 0.8181 - val_precision_m: 0.8344 - val_precision: 0.8344 - val_recall_6: 0.8181 - val_f1score: 0.8260\n",
      "Epoch 157/300\n",
      "epoch:  156\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3204 - acc: 0.8614 - prec: 0.9623 - recall: 0.9740 - prec_1: 0.8996 - recall_1: 0.9232 - prec_2: 0.9600 - recall_2: 0.9419 - prec_3: 0.7052 - recall_3: 0.6807 - prec_4: 0.6852 - recall_4: 0.7240 - prec_5: 0.9466 - recall_5: 0.9539 - f1_m: 0.8590 - recall_m: 0.8493 - precision_m: 0.8691 - precision: 0.8691 - recall_6: 0.8493 - f1score: 0.8590 - val_loss: 0.3984 - val_acc: 0.8086 - val_prec: 0.1762 - val_recall: 0.1718 - val_prec_1: 0.1758 - val_recall_1: 0.1378 - val_prec_2: 0.1897 - val_recall_2: 0.1765 - val_prec_3: 0.1702 - val_recall_3: 0.1612 - val_prec_4: 0.1602 - val_recall_4: 0.0668 - val_prec_5: 0.1672 - val_recall_5: 0.1601 - val_f1_m: 0.8054 - val_recall_m: 0.7933 - val_precision_m: 0.8183 - val_precision: 0.8183 - val_recall_6: 0.7933 - val_f1score: 0.8054\n",
      "Epoch 158/300\n",
      "epoch:  157\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3160 - acc: 0.8649 - prec: 0.9674 - recall: 0.9743 - prec_1: 0.9014 - recall_1: 0.9186 - prec_2: 0.9545 - recall_2: 0.9408 - prec_3: 0.7168 - recall_3: 0.6999 - prec_4: 0.7063 - recall_4: 0.7313 - prec_5: 0.9412 - recall_5: 0.9544 - f1_m: 0.8626 - recall_m: 0.8529 - precision_m: 0.8726 - precision: 0.8726 - recall_6: 0.8529 - f1score: 0.8626 - val_loss: 0.3667 - val_acc: 0.8261 - val_prec: 0.1762 - val_recall: 0.1709 - val_prec_1: 0.1759 - val_recall_1: 0.1574 - val_prec_2: 0.1909 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.0854 - val_prec_4: 0.1827 - val_recall_4: 0.1424 - val_prec_5: 0.1672 - val_recall_5: 0.1571 - val_f1_m: 0.8234 - val_recall_m: 0.8123 - val_precision_m: 0.8351 - val_precision: 0.8351 - val_recall_6: 0.8123 - val_f1score: 0.8234\n",
      "Epoch 159/300\n",
      "epoch:  158\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3177 - acc: 0.8611 - prec: 0.9632 - recall: 0.9713 - prec_1: 0.9014 - recall_1: 0.9166 - prec_2: 0.9485 - recall_2: 0.9446 - prec_3: 0.7138 - recall_3: 0.6915 - prec_4: 0.6903 - recall_4: 0.7269 - prec_5: 0.9476 - recall_5: 0.9457 - f1_m: 0.8592 - recall_m: 0.8485 - precision_m: 0.8703 - precision: 0.8703 - recall_6: 0.8485 - f1score: 0.8592 - val_loss: 0.5054 - val_acc: 0.8051 - val_prec: 0.1762 - val_recall: 0.1712 - val_prec_1: 0.1759 - val_recall_1: 0.1525 - val_prec_2: 0.1897 - val_recall_2: 0.1810 - val_prec_3: 0.1121 - val_recall_3: 0.0386 - val_prec_4: 0.1822 - val_recall_4: 0.1772 - val_prec_5: 0.1672 - val_recall_5: 0.1543 - val_f1_m: 0.8032 - val_recall_m: 0.7913 - val_precision_m: 0.8158 - val_precision: 0.8158 - val_recall_6: 0.7913 - val_f1score: 0.8032\n",
      "Epoch 160/300\n",
      "epoch:  159\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3189 - acc: 0.8568 - prec: 0.9554 - recall: 0.9759 - prec_1: 0.9083 - recall_1: 0.9103 - prec_2: 0.9600 - recall_2: 0.9460 - prec_3: 0.6961 - recall_3: 0.6790 - prec_4: 0.6755 - recall_4: 0.7036 - prec_5: 0.9377 - recall_5: 0.9570 - f1_m: 0.8569 - recall_m: 0.8476 - precision_m: 0.8666 - precision: 0.8666 - recall_6: 0.8476 - f1score: 0.8569 - val_loss: 0.3975 - val_acc: 0.8261 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1614 - val_prec_2: 0.1909 - val_recall_2: 0.1742 - val_prec_3: 0.1602 - val_recall_3: 0.0730 - val_prec_4: 0.1822 - val_recall_4: 0.1507 - val_prec_5: 0.1672 - val_recall_5: 0.1553 - val_f1_m: 0.8240 - val_recall_m: 0.8128 - val_precision_m: 0.8359 - val_precision: 0.8359 - val_recall_6: 0.8128 - val_f1score: 0.8240\n",
      "Epoch 161/300\n",
      "epoch:  160\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3159 - acc: 0.8644 - prec: 0.9663 - recall: 0.9728 - prec_1: 0.8969 - recall_1: 0.9230 - prec_2: 0.9529 - recall_2: 0.9410 - prec_3: 0.7236 - recall_3: 0.6826 - prec_4: 0.6912 - recall_4: 0.7403 - prec_5: 0.9549 - recall_5: 0.9548 - f1_m: 0.8618 - recall_m: 0.8521 - precision_m: 0.8719 - precision: 0.8719 - recall_6: 0.8521 - f1score: 0.8618 - val_loss: 0.3866 - val_acc: 0.8306 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1554 - val_prec_2: 0.1897 - val_recall_2: 0.1714 - val_prec_3: 0.1709 - val_recall_3: 0.1475 - val_prec_4: 0.1895 - val_recall_4: 0.0942 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8309 - val_recall_m: 0.8216 - val_precision_m: 0.8406 - val_precision: 0.8406 - val_recall_6: 0.8216 - val_f1score: 0.8309\n",
      "Epoch 162/300\n",
      "epoch:  161\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 684us/step - loss: 0.3167 - acc: 0.8613 - prec: 0.9642 - recall: 0.9770 - prec_1: 0.8977 - recall_1: 0.9099 - prec_2: 0.9522 - recall_2: 0.9394 - prec_3: 0.7199 - recall_3: 0.6777 - prec_4: 0.6950 - recall_4: 0.7436 - prec_5: 0.9457 - recall_5: 0.9529 - f1_m: 0.8598 - recall_m: 0.8493 - precision_m: 0.8707 - precision: 0.8707 - recall_6: 0.8493 - f1score: 0.8598 - val_loss: 0.4857 - val_acc: 0.7963 - val_prec: 0.1762 - val_recall: 0.1727 - val_prec_1: 0.1758 - val_recall_1: 0.1463 - val_prec_2: 0.1897 - val_recall_2: 0.1775 - val_prec_3: 0.1121 - val_recall_3: 0.0431 - val_prec_4: 0.1822 - val_recall_4: 0.1669 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.7937 - val_recall_m: 0.7823 - val_precision_m: 0.8060 - val_precision: 0.8060 - val_recall_6: 0.7823 - val_f1score: 0.7937\n",
      "Epoch 163/300\n",
      "epoch:  162\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 666us/step - loss: 0.3200 - acc: 0.8611 - prec: 0.9599 - recall: 0.9756 - prec_1: 0.8951 - recall_1: 0.9079 - prec_2: 0.9526 - recall_2: 0.9351 - prec_3: 0.7243 - recall_3: 0.6827 - prec_4: 0.6974 - recall_4: 0.7557 - prec_5: 0.9355 - recall_5: 0.9500 - f1_m: 0.8589 - recall_m: 0.8472 - precision_m: 0.8711 - precision: 0.8711 - recall_6: 0.8472 - f1score: 0.8589 - val_loss: 0.4532 - val_acc: 0.8121 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1524 - val_prec_2: 0.1909 - val_recall_2: 0.1812 - val_prec_3: 0.1441 - val_recall_3: 0.0348 - val_prec_4: 0.1822 - val_recall_4: 0.1847 - val_prec_5: 0.1672 - val_recall_5: 0.1551 - val_f1_m: 0.8080 - val_recall_m: 0.7953 - val_precision_m: 0.8216 - val_precision: 0.8216 - val_recall_6: 0.7953 - val_f1score: 0.8080\n",
      "Epoch 164/300\n",
      "epoch:  163\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3190 - acc: 0.8602 - prec: 0.9605 - recall: 0.9713 - prec_1: 0.8903 - recall_1: 0.9149 - prec_2: 0.9516 - recall_2: 0.9408 - prec_3: 0.7009 - recall_3: 0.7006 - prec_4: 0.6977 - recall_4: 0.7132 - prec_5: 0.9482 - recall_5: 0.9512 - f1_m: 0.8583 - recall_m: 0.8472 - precision_m: 0.8698 - precision: 0.8698 - recall_6: 0.8472 - f1score: 0.8583 - val_loss: 0.3612 - val_acc: 0.8318 - val_prec: 0.1762 - val_recall: 0.1704 - val_prec_1: 0.1758 - val_recall_1: 0.1446 - val_prec_2: 0.1897 - val_recall_2: 0.1807 - val_prec_3: 0.1744 - val_recall_3: 0.0835 - val_prec_4: 0.1829 - val_recall_4: 0.1660 - val_prec_5: 0.1672 - val_recall_5: 0.1573 - val_f1_m: 0.8271 - val_recall_m: 0.8133 - val_precision_m: 0.8419 - val_precision: 0.8419 - val_recall_6: 0.8133 - val_f1score: 0.8271\n",
      "Epoch 165/300\n",
      "epoch:  164\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3177 - acc: 0.8619 - prec: 0.9588 - recall: 0.9749 - prec_1: 0.8993 - recall_1: 0.9141 - prec_2: 0.9572 - recall_2: 0.9462 - prec_3: 0.7178 - recall_3: 0.6673 - prec_4: 0.6830 - recall_4: 0.7389 - prec_5: 0.9420 - recall_5: 0.9478 - f1_m: 0.8604 - recall_m: 0.8492 - precision_m: 0.8722 - precision: 0.8722 - recall_6: 0.8492 - f1score: 0.8604 - val_loss: 0.3620 - val_acc: 0.8458 - val_prec: 0.1762 - val_recall: 0.1724 - val_prec_1: 0.1759 - val_recall_1: 0.1632 - val_prec_2: 0.1897 - val_recall_2: 0.1752 - val_prec_3: 0.1762 - val_recall_3: 0.0986 - val_prec_4: 0.1836 - val_recall_4: 0.1491 - val_prec_5: 0.1672 - val_recall_5: 0.1498 - val_f1_m: 0.8444 - val_recall_m: 0.8351 - val_precision_m: 0.8542 - val_precision: 0.8542 - val_recall_6: 0.8351 - val_f1score: 0.8444\n",
      "Epoch 166/300\n",
      "epoch:  165\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 680us/step - loss: 0.3202 - acc: 0.8599 - prec: 0.9642 - recall: 0.9663 - prec_1: 0.8955 - recall_1: 0.9278 - prec_2: 0.9615 - recall_2: 0.9356 - prec_3: 0.7008 - recall_3: 0.6952 - prec_4: 0.6853 - recall_4: 0.7097 - prec_5: 0.9454 - recall_5: 0.9544 - f1_m: 0.8599 - recall_m: 0.8498 - precision_m: 0.8704 - precision: 0.8704 - recall_6: 0.8498 - f1score: 0.8599 - val_loss: 0.4458 - val_acc: 0.8058 - val_prec: 0.1762 - val_recall: 0.1712 - val_prec_1: 0.1759 - val_recall_1: 0.1605 - val_prec_2: 0.1897 - val_recall_2: 0.1787 - val_prec_3: 0.1281 - val_recall_3: 0.0613 - val_prec_4: 0.1662 - val_recall_4: 0.1391 - val_prec_5: 0.1672 - val_recall_5: 0.1490 - val_f1_m: 0.8071 - val_recall_m: 0.7978 - val_precision_m: 0.8170 - val_precision: 0.8170 - val_recall_6: 0.7978 - val_f1score: 0.8071\n",
      "Epoch 167/300\n",
      "epoch:  166\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3123 - acc: 0.8652 - prec: 0.9728 - recall: 0.9718 - prec_1: 0.9028 - recall_1: 0.9196 - prec_2: 0.9455 - recall_2: 0.9416 - prec_3: 0.7249 - recall_3: 0.6899 - prec_4: 0.6963 - recall_4: 0.7455 - prec_5: 0.9535 - recall_5: 0.9564 - f1_m: 0.8621 - recall_m: 0.8513 - precision_m: 0.8733 - precision: 0.8733 - recall_6: 0.8513 - f1score: 0.8621 - val_loss: 0.5173 - val_acc: 0.7965 - val_prec: 0.1762 - val_recall: 0.1678 - val_prec_1: 0.1751 - val_recall_1: 0.1291 - val_prec_2: 0.1909 - val_recall_2: 0.1775 - val_prec_3: 0.1441 - val_recall_3: 0.0941 - val_prec_4: 0.1762 - val_recall_4: 0.1264 - val_prec_5: 0.1672 - val_recall_5: 0.1594 - val_f1_m: 0.7966 - val_recall_m: 0.7865 - val_precision_m: 0.8073 - val_precision: 0.8073 - val_recall_6: 0.7865 - val_f1score: 0.7966\n",
      "Epoch 168/300\n",
      "epoch:  167\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3142 - acc: 0.8657 - prec: 0.9688 - recall: 0.9698 - prec_1: 0.9061 - recall_1: 0.9175 - prec_2: 0.9528 - recall_2: 0.9485 - prec_3: 0.7260 - recall_3: 0.6994 - prec_4: 0.6998 - recall_4: 0.7376 - prec_5: 0.9378 - recall_5: 0.9536 - f1_m: 0.8643 - recall_m: 0.8549 - precision_m: 0.8740 - precision: 0.8740 - recall_6: 0.8549 - f1score: 0.8643 - val_loss: 0.3987 - val_acc: 0.8016 - val_prec: 0.1762 - val_recall: 0.1727 - val_prec_1: 0.1758 - val_recall_1: 0.1603 - val_prec_2: 0.1909 - val_recall_2: 0.1717 - val_prec_3: 0.1698 - val_recall_3: 0.0825 - val_prec_4: 0.1821 - val_recall_4: 0.1210 - val_prec_5: 0.1672 - val_recall_5: 0.1553 - val_f1_m: 0.8016 - val_recall_m: 0.7885 - val_precision_m: 0.8157 - val_precision: 0.8157 - val_recall_6: 0.7885 - val_f1score: 0.8016\n",
      "Epoch 169/300\n",
      "epoch:  168\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3165 - acc: 0.8633 - prec: 0.9553 - recall: 0.9709 - prec_1: 0.9028 - recall_1: 0.9141 - prec_2: 0.9577 - recall_2: 0.9451 - prec_3: 0.7103 - recall_3: 0.7104 - prec_4: 0.7019 - recall_4: 0.7149 - prec_5: 0.9505 - recall_5: 0.9539 - f1_m: 0.8612 - recall_m: 0.8507 - precision_m: 0.8722 - precision: 0.8722 - recall_6: 0.8507 - f1score: 0.8612 - val_loss: 0.3922 - val_acc: 0.8396 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1755 - val_recall_1: 0.1616 - val_prec_2: 0.1895 - val_recall_2: 0.1674 - val_prec_3: 0.1602 - val_recall_3: 0.1136 - val_prec_4: 0.1820 - val_recall_4: 0.1377 - val_prec_5: 0.1672 - val_recall_5: 0.1563 - val_f1_m: 0.8395 - val_recall_m: 0.8306 - val_precision_m: 0.8489 - val_precision: 0.8489 - val_recall_6: 0.8306 - val_f1score: 0.8395\n",
      "Epoch 170/300\n",
      "epoch:  169\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3188 - acc: 0.8579 - prec: 0.9670 - recall: 0.9718 - prec_1: 0.8992 - recall_1: 0.9243 - prec_2: 0.9577 - recall_2: 0.9402 - prec_3: 0.6991 - recall_3: 0.6596 - prec_4: 0.6718 - recall_4: 0.7204 - prec_5: 0.9473 - recall_5: 0.9616 - f1_m: 0.8559 - recall_m: 0.8465 - precision_m: 0.8658 - precision: 0.8658 - recall_6: 0.8465 - f1score: 0.8559 - val_loss: 0.3553 - val_acc: 0.8341 - val_prec: 0.1762 - val_recall: 0.1724 - val_prec_1: 0.1756 - val_recall_1: 0.1666 - val_prec_2: 0.1907 - val_recall_2: 0.1666 - val_prec_3: 0.1762 - val_recall_3: 0.0944 - val_prec_4: 0.1832 - val_recall_4: 0.1544 - val_prec_5: 0.1672 - val_recall_5: 0.1508 - val_f1_m: 0.8348 - val_recall_m: 0.8268 - val_precision_m: 0.8432 - val_precision: 0.8432 - val_recall_6: 0.8268 - val_f1score: 0.8348\n",
      "Epoch 171/300\n",
      "epoch:  170\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3151 - acc: 0.8643 - prec: 0.9656 - recall: 0.9709 - prec_1: 0.9004 - recall_1: 0.9248 - prec_2: 0.9489 - recall_2: 0.9402 - prec_3: 0.7099 - recall_3: 0.7056 - prec_4: 0.7016 - recall_4: 0.7246 - prec_5: 0.9539 - recall_5: 0.9526 - f1_m: 0.8646 - recall_m: 0.8542 - precision_m: 0.8754 - precision: 0.8754 - recall_6: 0.8542 - f1score: 0.8646 - val_loss: 0.4270 - val_acc: 0.8193 - val_prec: 0.1762 - val_recall: 0.1727 - val_prec_1: 0.1756 - val_recall_1: 0.1665 - val_prec_2: 0.1907 - val_recall_2: 0.1659 - val_prec_3: 0.1602 - val_recall_3: 0.0856 - val_prec_4: 0.1822 - val_recall_4: 0.1389 - val_prec_5: 0.1672 - val_recall_5: 0.1518 - val_f1_m: 0.8083 - val_recall_m: 0.7883 - val_precision_m: 0.8368 - val_precision: 0.8368 - val_recall_6: 0.7883 - val_f1score: 0.8083\n",
      "Epoch 172/300\n",
      "epoch:  171\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3194 - acc: 0.8603 - prec: 0.9656 - recall: 0.9734 - prec_1: 0.9012 - recall_1: 0.9215 - prec_2: 0.9551 - recall_2: 0.9373 - prec_3: 0.7028 - recall_3: 0.6734 - prec_4: 0.6859 - recall_4: 0.7256 - prec_5: 0.9454 - recall_5: 0.9496 - f1_m: 0.8593 - recall_m: 0.8487 - precision_m: 0.8704 - precision: 0.8704 - recall_6: 0.8487 - f1score: 0.8593 - val_loss: 0.3840 - val_acc: 0.8363 - val_prec: 0.1762 - val_recall: 0.1692 - val_prec_1: 0.1758 - val_recall_1: 0.1394 - val_prec_2: 0.1909 - val_recall_2: 0.1800 - val_prec_3: 0.1750 - val_recall_3: 0.1112 - val_prec_4: 0.1835 - val_recall_4: 0.1465 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8346 - val_recall_m: 0.8201 - val_precision_m: 0.8503 - val_precision: 0.8503 - val_recall_6: 0.8201 - val_f1score: 0.8346\n",
      "Epoch 173/300\n",
      "epoch:  172\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 6s 692us/step - loss: 0.3163 - acc: 0.8638 - prec: 0.9671 - recall: 0.9797 - prec_1: 0.9054 - recall_1: 0.9057 - prec_2: 0.9509 - recall_2: 0.9449 - prec_3: 0.7211 - recall_3: 0.6872 - prec_4: 0.6984 - recall_4: 0.7385 - prec_5: 0.9416 - recall_5: 0.9521 - f1_m: 0.8625 - recall_m: 0.8527 - precision_m: 0.8727 - precision: 0.8727 - recall_6: 0.8527 - f1score: 0.8625 - val_loss: 0.3548 - val_acc: 0.8396 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1759 - val_recall_1: 0.1530 - val_prec_2: 0.1909 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1188 - val_prec_4: 0.1922 - val_recall_4: 0.1391 - val_prec_5: 0.1672 - val_recall_5: 0.1561 - val_f1_m: 0.8410 - val_recall_m: 0.8318 - val_precision_m: 0.8507 - val_precision: 0.8507 - val_recall_6: 0.8318 - val_f1score: 0.8410\n",
      "Epoch 174/300\n",
      "epoch:  173\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3181 - acc: 0.8602 - prec: 0.9651 - recall: 0.9768 - prec_1: 0.8986 - recall_1: 0.9170 - prec_2: 0.9541 - recall_2: 0.9390 - prec_3: 0.7010 - recall_3: 0.7024 - prec_4: 0.6953 - recall_4: 0.7085 - prec_5: 0.9433 - recall_5: 0.9529 - f1_m: 0.8574 - recall_m: 0.8463 - precision_m: 0.8688 - precision: 0.8688 - recall_6: 0.8463 - f1score: 0.8574 - val_loss: 0.4400 - val_acc: 0.8221 - val_prec: 0.1762 - val_recall: 0.1709 - val_prec_1: 0.1759 - val_recall_1: 0.1665 - val_prec_2: 0.1897 - val_recall_2: 0.1729 - val_prec_3: 0.1602 - val_recall_3: 0.0973 - val_prec_4: 0.1822 - val_recall_4: 0.1261 - val_prec_5: 0.1672 - val_recall_5: 0.1506 - val_f1_m: 0.8189 - val_recall_m: 0.8088 - val_precision_m: 0.8296 - val_precision: 0.8296 - val_recall_6: 0.8088 - val_f1score: 0.8189\n",
      "Epoch 175/300\n",
      "epoch:  174\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3147 - acc: 0.8577 - prec: 0.9645 - recall: 0.9737 - prec_1: 0.9040 - recall_1: 0.9148 - prec_2: 0.9538 - recall_2: 0.9425 - prec_3: 0.6996 - recall_3: 0.6808 - prec_4: 0.6755 - recall_4: 0.7021 - prec_5: 0.9486 - recall_5: 0.9548 - f1_m: 0.8579 - recall_m: 0.8477 - precision_m: 0.8684 - precision: 0.8684 - recall_6: 0.8477 - f1score: 0.8579 - val_loss: 0.4435 - val_acc: 0.7980 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1759 - val_recall_1: 0.1602 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1602 - val_recall_3: 0.0759 - val_prec_4: 0.1822 - val_recall_4: 0.1201 - val_prec_5: 0.1672 - val_recall_5: 0.1551 - val_f1_m: 0.7953 - val_recall_m: 0.7850 - val_precision_m: 0.8062 - val_precision: 0.8062 - val_recall_6: 0.7850 - val_f1score: 0.7953\n",
      "Epoch 176/300\n",
      "epoch:  175\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3138 - acc: 0.8672 - prec: 0.9669 - recall: 0.9783 - prec_1: 0.9028 - recall_1: 0.9236 - prec_2: 0.9566 - recall_2: 0.9468 - prec_3: 0.7313 - recall_3: 0.6854 - prec_4: 0.6874 - recall_4: 0.7498 - prec_5: 0.9502 - recall_5: 0.9543 - f1_m: 0.8647 - recall_m: 0.8556 - precision_m: 0.8743 - precision: 0.8743 - recall_6: 0.8556 - f1score: 0.8647 - val_loss: 0.3649 - val_acc: 0.8281 - val_prec: 0.1762 - val_recall: 0.1724 - val_prec_1: 0.1753 - val_recall_1: 0.1632 - val_prec_2: 0.1906 - val_recall_2: 0.1650 - val_prec_3: 0.1762 - val_recall_3: 0.1080 - val_prec_4: 0.1823 - val_recall_4: 0.1256 - val_prec_5: 0.1672 - val_recall_5: 0.1546 - val_f1_m: 0.8300 - val_recall_m: 0.8216 - val_precision_m: 0.8389 - val_precision: 0.8389 - val_recall_6: 0.8216 - val_f1score: 0.8300\n",
      "Epoch 177/300\n",
      "epoch:  176\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3153 - acc: 0.8608 - prec: 0.9619 - recall: 0.9736 - prec_1: 0.8987 - recall_1: 0.9243 - prec_2: 0.9561 - recall_2: 0.9376 - prec_3: 0.7017 - recall_3: 0.6919 - prec_4: 0.6895 - recall_4: 0.7115 - prec_5: 0.9496 - recall_5: 0.9511 - f1_m: 0.8606 - recall_m: 0.8504 - precision_m: 0.8712 - precision: 0.8712 - recall_6: 0.8504 - f1score: 0.8606 - val_loss: 0.4832 - val_acc: 0.8103 - val_prec: 0.1762 - val_recall: 0.1702 - val_prec_1: 0.1759 - val_recall_1: 0.1596 - val_prec_2: 0.1922 - val_recall_2: 0.1712 - val_prec_3: 0.1281 - val_recall_3: 0.0639 - val_prec_4: 0.1822 - val_recall_4: 0.1491 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8081 - val_recall_m: 0.7988 - val_precision_m: 0.8178 - val_precision: 0.8178 - val_recall_6: 0.7988 - val_f1score: 0.8081\n",
      "Epoch 178/300\n",
      "epoch:  177\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3153 - acc: 0.8662 - prec: 0.9664 - recall: 0.9756 - prec_1: 0.9051 - recall_1: 0.9174 - prec_2: 0.9584 - recall_2: 0.9478 - prec_3: 0.7078 - recall_3: 0.7041 - prec_4: 0.6969 - recall_4: 0.7185 - prec_5: 0.9503 - recall_5: 0.9593 - f1_m: 0.8641 - recall_m: 0.8537 - precision_m: 0.8750 - precision: 0.8750 - recall_6: 0.8537 - f1score: 0.8641 - val_loss: 0.4588 - val_acc: 0.8088 - val_prec: 0.1762 - val_recall: 0.1727 - val_prec_1: 0.1758 - val_recall_1: 0.1522 - val_prec_2: 0.1897 - val_recall_2: 0.1772 - val_prec_3: 0.1602 - val_recall_3: 0.0782 - val_prec_4: 0.1822 - val_recall_4: 0.1424 - val_prec_5: 0.1672 - val_recall_5: 0.1558 - val_f1_m: 0.8058 - val_recall_m: 0.7930 - val_precision_m: 0.8194 - val_precision: 0.8194 - val_recall_6: 0.7930 - val_f1score: 0.8058\n",
      "Epoch 179/300\n",
      "epoch:  178\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 684us/step - loss: 0.3148 - acc: 0.8662 - prec: 0.9678 - recall: 0.9754 - prec_1: 0.9024 - recall_1: 0.9201 - prec_2: 0.9496 - recall_2: 0.9382 - prec_3: 0.7115 - recall_3: 0.7137 - prec_4: 0.7080 - recall_4: 0.7169 - prec_5: 0.9514 - recall_5: 0.9577 - f1_m: 0.8641 - recall_m: 0.8537 - precision_m: 0.8750 - precision: 0.8750 - recall_6: 0.8537 - f1score: 0.8641 - val_loss: 0.3670 - val_acc: 0.8253 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1758 - val_recall_1: 0.1444 - val_prec_2: 0.1897 - val_recall_2: 0.1792 - val_prec_3: 0.1762 - val_recall_3: 0.0644 - val_prec_4: 0.1830 - val_recall_4: 0.1747 - val_prec_5: 0.1672 - val_recall_5: 0.1599 - val_f1_m: 0.8214 - val_recall_m: 0.8066 - val_precision_m: 0.8375 - val_precision: 0.8375 - val_recall_6: 0.8066 - val_f1score: 0.8214\n",
      "Epoch 180/300\n",
      "epoch:  179\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3155 - acc: 0.8639 - prec: 0.9621 - recall: 0.9714 - prec_1: 0.8957 - recall_1: 0.9162 - prec_2: 0.9564 - recall_2: 0.9419 - prec_3: 0.7126 - recall_3: 0.6990 - prec_4: 0.7024 - recall_4: 0.7251 - prec_5: 0.9461 - recall_5: 0.9520 - f1_m: 0.8624 - recall_m: 0.8521 - precision_m: 0.8731 - precision: 0.8731 - recall_6: 0.8521 - f1score: 0.8624 - val_loss: 0.6940 - val_acc: 0.7913 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1473 - val_prec_2: 0.1909 - val_recall_2: 0.1782 - val_prec_3: 0.0961 - val_recall_3: 0.0314 - val_prec_4: 0.1822 - val_recall_4: 0.1722 - val_prec_5: 0.1672 - val_recall_5: 0.1599 - val_f1_m: 0.7871 - val_recall_m: 0.7753 - val_precision_m: 0.7997 - val_precision: 0.7997 - val_recall_6: 0.7753 - val_f1score: 0.7871\n",
      "Epoch 181/300\n",
      "epoch:  180\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3184 - acc: 0.8594 - prec: 0.9645 - recall: 0.9737 - prec_1: 0.8929 - recall_1: 0.9101 - prec_2: 0.9560 - recall_2: 0.9339 - prec_3: 0.7055 - recall_3: 0.6823 - prec_4: 0.6837 - recall_4: 0.7229 - prec_5: 0.9455 - recall_5: 0.9563 - f1_m: 0.8594 - recall_m: 0.8487 - precision_m: 0.8706 - precision: 0.8706 - recall_6: 0.8487 - f1score: 0.8594 - val_loss: 1.7985 - val_acc: 0.7460 - val_prec: 0.1762 - val_recall: 0.1697 - val_prec_1: 0.1759 - val_recall_1: 0.1660 - val_prec_2: 0.1909 - val_recall_2: 0.1684 - val_prec_3: 0.1281 - val_recall_3: 0.0647 - val_prec_4: 0.1602 - val_recall_4: 0.0701 - val_prec_5: 0.1672 - val_recall_5: 0.1513 - val_f1_m: 0.7450 - val_recall_m: 0.7372 - val_precision_m: 0.7531 - val_precision: 0.7531 - val_recall_6: 0.7372 - val_f1score: 0.7450\n",
      "Epoch 182/300\n",
      "epoch:  181\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3120 - acc: 0.8653 - prec: 0.9656 - recall: 0.9753 - prec_1: 0.9106 - recall_1: 0.9164 - prec_2: 0.9581 - recall_2: 0.9482 - prec_3: 0.7175 - recall_3: 0.6727 - prec_4: 0.6902 - recall_4: 0.7433 - prec_5: 0.9556 - recall_5: 0.9594 - f1_m: 0.8630 - recall_m: 0.8532 - precision_m: 0.8731 - precision: 0.8731 - recall_6: 0.8532 - f1score: 0.8630 - val_loss: 0.3873 - val_acc: 0.8296 - val_prec: 0.1762 - val_recall: 0.1716 - val_prec_1: 0.1759 - val_recall_1: 0.1607 - val_prec_2: 0.1909 - val_recall_2: 0.1749 - val_prec_3: 0.1754 - val_recall_3: 0.1244 - val_prec_4: 0.1922 - val_recall_4: 0.1178 - val_prec_5: 0.1672 - val_recall_5: 0.1566 - val_f1_m: 0.8296 - val_recall_m: 0.8176 - val_precision_m: 0.8426 - val_precision: 0.8426 - val_recall_6: 0.8176 - val_f1score: 0.8296\n",
      "Epoch 183/300\n",
      "epoch:  182\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3197 - acc: 0.8618 - prec: 0.9592 - recall: 0.9754 - prec_1: 0.9027 - recall_1: 0.9205 - prec_2: 0.9545 - recall_2: 0.9414 - prec_3: 0.6977 - recall_3: 0.7141 - prec_4: 0.6977 - recall_4: 0.6972 - prec_5: 0.9559 - recall_5: 0.9531 - f1_m: 0.8607 - recall_m: 0.8508 - precision_m: 0.8711 - precision: 0.8711 - recall_6: 0.8508 - f1score: 0.8607 - val_loss: 0.4636 - val_acc: 0.8166 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1752 - val_recall_1: 0.1621 - val_prec_2: 0.1893 - val_recall_2: 0.1660 - val_prec_3: 0.1281 - val_recall_3: 0.0461 - val_prec_4: 0.1822 - val_recall_4: 0.1809 - val_prec_5: 0.1672 - val_recall_5: 0.1563 - val_f1_m: 0.8194 - val_recall_m: 0.8098 - val_precision_m: 0.8295 - val_precision: 0.8295 - val_recall_6: 0.8098 - val_f1score: 0.8194\n",
      "Epoch 184/300\n",
      "epoch:  183\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 679us/step - loss: 0.3132 - acc: 0.8648 - prec: 0.9665 - recall: 0.9750 - prec_1: 0.9064 - recall_1: 0.9150 - prec_2: 0.9498 - recall_2: 0.9452 - prec_3: 0.7160 - recall_3: 0.6979 - prec_4: 0.6960 - recall_4: 0.7361 - prec_5: 0.9473 - recall_5: 0.9511 - f1_m: 0.8635 - recall_m: 0.8529 - precision_m: 0.8746 - precision: 0.8746 - recall_6: 0.8529 - f1score: 0.8635 - val_loss: 0.9035 - val_acc: 0.7675 - val_prec: 0.1762 - val_recall: 0.1690 - val_prec_1: 0.1757 - val_recall_1: 0.1252 - val_prec_2: 0.1890 - val_recall_2: 0.1860 - val_prec_3: 0.0801 - val_recall_3: 0.0308 - val_prec_4: 0.1822 - val_recall_4: 0.1784 - val_prec_5: 0.1672 - val_recall_5: 0.1478 - val_f1_m: 0.7653 - val_recall_m: 0.7495 - val_precision_m: 0.7832 - val_precision: 0.7832 - val_recall_6: 0.7495 - val_f1score: 0.7653\n",
      "Epoch 185/300\n",
      "epoch:  184\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.3161 - acc: 0.8616 - prec: 0.9656 - recall: 0.9761 - prec_1: 0.9008 - recall_1: 0.9281 - prec_2: 0.9622 - recall_2: 0.9357 - prec_3: 0.7063 - recall_3: 0.6795 - prec_4: 0.6830 - recall_4: 0.7161 - prec_5: 0.9467 - recall_5: 0.9592 - f1_m: 0.8593 - recall_m: 0.8487 - precision_m: 0.8704 - precision: 0.8704 - recall_6: 0.8487 - f1score: 0.8593 - val_loss: 0.6419 - val_acc: 0.7848 - val_prec: 0.1762 - val_recall: 0.1699 - val_prec_1: 0.1753 - val_recall_1: 0.1589 - val_prec_2: 0.1906 - val_recall_2: 0.1631 - val_prec_3: 0.1702 - val_recall_3: 0.1752 - val_prec_4: 0.1281 - val_recall_4: 0.0188 - val_prec_5: 0.1672 - val_recall_5: 0.1541 - val_f1_m: 0.7824 - val_recall_m: 0.7763 - val_precision_m: 0.7889 - val_precision: 0.7889 - val_recall_6: 0.7763 - val_f1score: 0.7824\n",
      "Epoch 186/300\n",
      "epoch:  185\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 680us/step - loss: 0.3180 - acc: 0.8609 - prec: 0.9694 - recall: 0.9727 - prec_1: 0.9051 - recall_1: 0.9241 - prec_2: 0.9554 - recall_2: 0.9448 - prec_3: 0.7051 - recall_3: 0.6724 - prec_4: 0.6843 - recall_4: 0.7286 - prec_5: 0.9469 - recall_5: 0.9587 - f1_m: 0.8591 - recall_m: 0.8490 - precision_m: 0.8697 - precision: 0.8697 - recall_6: 0.8490 - f1score: 0.8591 - val_loss: 1.0717 - val_acc: 0.7380 - val_prec: 0.1762 - val_recall: 0.1716 - val_prec_1: 0.1758 - val_recall_1: 0.1592 - val_prec_2: 0.1909 - val_recall_2: 0.1726 - val_prec_3: 0.0320 - val_recall_3: 0.0018 - val_prec_4: 0.1662 - val_recall_4: 0.1276 - val_prec_5: 0.1672 - val_recall_5: 0.1563 - val_f1_m: 0.7335 - val_recall_m: 0.7235 - val_precision_m: 0.7442 - val_precision: 0.7442 - val_recall_6: 0.7235 - val_f1score: 0.7335\n",
      "Epoch 187/300\n",
      "epoch:  186\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3128 - acc: 0.8658 - prec: 0.9600 - recall: 0.9737 - prec_1: 0.9024 - recall_1: 0.9239 - prec_2: 0.9531 - recall_2: 0.9455 - prec_3: 0.7120 - recall_3: 0.7064 - prec_4: 0.7068 - recall_4: 0.7219 - prec_5: 0.9541 - recall_5: 0.9525 - f1_m: 0.8638 - recall_m: 0.8541 - precision_m: 0.8739 - precision: 0.8739 - recall_6: 0.8541 - f1score: 0.8638 - val_loss: 0.4891 - val_acc: 0.8213 - val_prec: 0.1762 - val_recall: 0.1707 - val_prec_1: 0.1750 - val_recall_1: 0.1621 - val_prec_2: 0.1904 - val_recall_2: 0.1617 - val_prec_3: 0.1727 - val_recall_3: 0.1269 - val_prec_4: 0.1922 - val_recall_4: 0.1140 - val_prec_5: 0.1672 - val_recall_5: 0.1546 - val_f1_m: 0.8224 - val_recall_m: 0.8166 - val_precision_m: 0.8285 - val_precision: 0.8285 - val_recall_6: 0.8166 - val_f1score: 0.8224\n",
      "Epoch 188/300\n",
      "epoch:  187\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3137 - acc: 0.8639 - prec: 0.9709 - recall: 0.9702 - prec_1: 0.8909 - recall_1: 0.9285 - prec_2: 0.9659 - recall_2: 0.9289 - prec_3: 0.7216 - recall_3: 0.6863 - prec_4: 0.6884 - recall_4: 0.7388 - prec_5: 0.9511 - recall_5: 0.9577 - f1_m: 0.8630 - recall_m: 0.8527 - precision_m: 0.8736 - precision: 0.8736 - recall_6: 0.8527 - f1score: 0.8630 - val_loss: 0.5070 - val_acc: 0.7833 - val_prec: 0.1762 - val_recall: 0.1684 - val_prec_1: 0.1758 - val_recall_1: 0.1498 - val_prec_2: 0.1897 - val_recall_2: 0.1807 - val_prec_3: 0.0320 - val_recall_3: 0.0033 - val_prec_4: 0.1822 - val_recall_4: 0.1902 - val_prec_5: 0.1672 - val_recall_5: 0.1579 - val_f1_m: 0.7836 - val_recall_m: 0.7710 - val_precision_m: 0.7971 - val_precision: 0.7971 - val_recall_6: 0.7710 - val_f1score: 0.7836\n",
      "Epoch 189/300\n",
      "epoch:  188\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3163 - acc: 0.8612 - prec: 0.9632 - recall: 0.9743 - prec_1: 0.9063 - recall_1: 0.9139 - prec_2: 0.9453 - recall_2: 0.9455 - prec_3: 0.7095 - recall_3: 0.6774 - prec_4: 0.6839 - recall_4: 0.7313 - prec_5: 0.9574 - recall_5: 0.9510 - f1_m: 0.8597 - recall_m: 0.8503 - precision_m: 0.8695 - precision: 0.8695 - recall_6: 0.8503 - f1score: 0.8597 - val_loss: 0.8769 - val_acc: 0.7600 - val_prec: 0.1762 - val_recall: 0.1687 - val_prec_1: 0.1759 - val_recall_1: 0.1510 - val_prec_2: 0.1888 - val_recall_2: 0.1795 - val_prec_3: 0.0961 - val_recall_3: 0.0358 - val_prec_4: 0.1662 - val_recall_4: 0.1229 - val_prec_5: 0.1672 - val_recall_5: 0.1533 - val_f1_m: 0.7606 - val_recall_m: 0.7485 - val_precision_m: 0.7735 - val_precision: 0.7735 - val_recall_6: 0.7485 - val_f1score: 0.7606\n",
      "Epoch 190/300\n",
      "epoch:  189\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3112 - acc: 0.8662 - prec: 0.9606 - recall: 0.9730 - prec_1: 0.9066 - recall_1: 0.9176 - prec_2: 0.9478 - recall_2: 0.9390 - prec_3: 0.7253 - recall_3: 0.7060 - prec_4: 0.7038 - recall_4: 0.7349 - prec_5: 0.9438 - recall_5: 0.9567 - f1_m: 0.8646 - recall_m: 0.8542 - precision_m: 0.8755 - precision: 0.8755 - recall_6: 0.8542 - f1score: 0.8646 - val_loss: 0.3705 - val_acc: 0.8458 - val_prec: 0.1762 - val_recall: 0.1727 - val_prec_1: 0.1753 - val_recall_1: 0.1517 - val_prec_2: 0.1922 - val_recall_2: 0.1577 - val_prec_3: 0.1762 - val_recall_3: 0.1263 - val_prec_4: 0.1910 - val_recall_4: 0.1499 - val_prec_5: 0.1672 - val_recall_5: 0.1596 - val_f1_m: 0.8448 - val_recall_m: 0.8333 - val_precision_m: 0.8569 - val_precision: 0.8569 - val_recall_6: 0.8333 - val_f1score: 0.8448\n",
      "Epoch 191/300\n",
      "epoch:  190\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3134 - acc: 0.8637 - prec: 0.9625 - recall: 0.9737 - prec_1: 0.9091 - recall_1: 0.9183 - prec_2: 0.9467 - recall_2: 0.9430 - prec_3: 0.7081 - recall_3: 0.6938 - prec_4: 0.6893 - recall_4: 0.7173 - prec_5: 0.9524 - recall_5: 0.9584 - f1_m: 0.8638 - recall_m: 0.8546 - precision_m: 0.8733 - precision: 0.8733 - recall_6: 0.8546 - f1score: 0.8638 - val_loss: 0.4495 - val_acc: 0.8103 - val_prec: 0.1762 - val_recall: 0.1728 - val_prec_1: 0.1756 - val_recall_1: 0.1650 - val_prec_2: 0.1907 - val_recall_2: 0.1691 - val_prec_3: 0.1702 - val_recall_3: 0.1424 - val_prec_4: 0.1602 - val_recall_4: 0.0768 - val_prec_5: 0.1672 - val_recall_5: 0.1503 - val_f1_m: 0.8113 - val_recall_m: 0.8028 - val_precision_m: 0.8202 - val_precision: 0.8202 - val_recall_6: 0.8028 - val_f1score: 0.8113\n",
      "Epoch 192/300\n",
      "epoch:  191\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3147 - acc: 0.8657 - prec: 0.9726 - recall: 0.9761 - prec_1: 0.9001 - recall_1: 0.9262 - prec_2: 0.9557 - recall_2: 0.9362 - prec_3: 0.7195 - recall_3: 0.6960 - prec_4: 0.6996 - recall_4: 0.7365 - prec_5: 0.9512 - recall_5: 0.9567 - f1_m: 0.8638 - recall_m: 0.8536 - precision_m: 0.8745 - precision: 0.8745 - recall_6: 0.8536 - f1score: 0.8638 - val_loss: 0.3821 - val_acc: 0.8133 - val_prec: 0.1762 - val_recall: 0.1707 - val_prec_1: 0.1758 - val_recall_1: 0.1425 - val_prec_2: 0.1897 - val_recall_2: 0.1807 - val_prec_3: 0.1744 - val_recall_3: 0.1341 - val_prec_4: 0.1922 - val_recall_4: 0.1000 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8110 - val_recall_m: 0.7998 - val_precision_m: 0.8230 - val_precision: 0.8230 - val_recall_6: 0.7998 - val_f1score: 0.8110\n",
      "Epoch 193/300\n",
      "epoch:  192\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3135 - acc: 0.8623 - prec: 0.9599 - recall: 0.9759 - prec_1: 0.9015 - recall_1: 0.9084 - prec_2: 0.9532 - recall_2: 0.9417 - prec_3: 0.7126 - recall_3: 0.6978 - prec_4: 0.6981 - recall_4: 0.7338 - prec_5: 0.9390 - recall_5: 0.9544 - f1_m: 0.8620 - recall_m: 0.8527 - precision_m: 0.8717 - precision: 0.8717 - recall_6: 0.8527 - f1score: 0.8620 - val_loss: 0.3840 - val_acc: 0.8156 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1610 - val_prec_2: 0.1897 - val_recall_2: 0.1752 - val_prec_3: 0.1441 - val_recall_3: 0.0771 - val_prec_4: 0.1822 - val_recall_4: 0.1364 - val_prec_5: 0.1672 - val_recall_5: 0.1561 - val_f1_m: 0.8130 - val_recall_m: 0.8018 - val_precision_m: 0.8248 - val_precision: 0.8248 - val_recall_6: 0.8018 - val_f1score: 0.8130\n",
      "Epoch 194/300\n",
      "epoch:  193\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3139 - acc: 0.8621 - prec: 0.9674 - recall: 0.9694 - prec_1: 0.8901 - recall_1: 0.9195 - prec_2: 0.9497 - recall_2: 0.9350 - prec_3: 0.7053 - recall_3: 0.7096 - prec_4: 0.7004 - recall_4: 0.7092 - prec_5: 0.9450 - recall_5: 0.9560 - f1_m: 0.8613 - recall_m: 0.8508 - precision_m: 0.8723 - precision: 0.8723 - recall_6: 0.8508 - f1score: 0.8613 - val_loss: 0.6541 - val_acc: 0.7893 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1645 - val_prec_2: 0.1909 - val_recall_2: 0.1734 - val_prec_3: 0.1441 - val_recall_3: 0.0856 - val_prec_4: 0.1660 - val_recall_4: 0.0957 - val_prec_5: 0.1672 - val_recall_5: 0.1528 - val_f1_m: 0.7890 - val_recall_m: 0.7803 - val_precision_m: 0.7982 - val_precision: 0.7982 - val_recall_6: 0.7803 - val_f1score: 0.7890\n",
      "Epoch 195/300\n",
      "epoch:  194\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3102 - acc: 0.8632 - prec: 0.9680 - recall: 0.9781 - prec_1: 0.9117 - recall_1: 0.9212 - prec_2: 0.9568 - recall_2: 0.9435 - prec_3: 0.6974 - recall_3: 0.6921 - prec_4: 0.6928 - recall_4: 0.7124 - prec_5: 0.9484 - recall_5: 0.9525 - f1_m: 0.8616 - recall_m: 0.8522 - precision_m: 0.8713 - precision: 0.8713 - recall_6: 0.8522 - f1score: 0.8616 - val_loss: 0.4566 - val_acc: 0.7978 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1759 - val_recall_1: 0.1653 - val_prec_2: 0.1897 - val_recall_2: 0.1737 - val_prec_3: 0.1441 - val_recall_3: 0.0627 - val_prec_4: 0.1662 - val_recall_4: 0.1246 - val_prec_5: 0.1672 - val_recall_5: 0.1538 - val_f1_m: 0.7935 - val_recall_m: 0.7818 - val_precision_m: 0.8060 - val_precision: 0.8060 - val_recall_6: 0.7818 - val_f1score: 0.7935\n",
      "Epoch 196/300\n",
      "epoch:  195\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 686us/step - loss: 0.3192 - acc: 0.8601 - prec: 0.9594 - recall: 0.9738 - prec_1: 0.8981 - recall_1: 0.9135 - prec_2: 0.9532 - recall_2: 0.9428 - prec_3: 0.7221 - recall_3: 0.6732 - prec_4: 0.6846 - recall_4: 0.7432 - prec_5: 0.9461 - recall_5: 0.9471 - f1_m: 0.8586 - recall_m: 0.8481 - precision_m: 0.8697 - precision: 0.8697 - recall_6: 0.8481 - f1score: 0.8586 - val_loss: 0.4939 - val_acc: 0.7990 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1758 - val_recall_1: 0.1410 - val_prec_2: 0.1897 - val_recall_2: 0.1817 - val_prec_3: 0.1281 - val_recall_3: 0.0524 - val_prec_4: 0.1820 - val_recall_4: 0.1620 - val_prec_5: 0.1672 - val_recall_5: 0.1594 - val_f1_m: 0.7948 - val_recall_m: 0.7820 - val_precision_m: 0.8088 - val_precision: 0.8088 - val_recall_6: 0.7820 - val_f1score: 0.7948\n",
      "Epoch 197/300\n",
      "epoch:  196\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3149 - acc: 0.8633 - prec: 0.9626 - recall: 0.9725 - prec_1: 0.8962 - recall_1: 0.9186 - prec_2: 0.9603 - recall_2: 0.9402 - prec_3: 0.7181 - recall_3: 0.6890 - prec_4: 0.7010 - recall_4: 0.7434 - prec_5: 0.9433 - recall_5: 0.9543 - f1_m: 0.8614 - recall_m: 0.8502 - precision_m: 0.8731 - precision: 0.8731 - recall_6: 0.8502 - f1score: 0.8614 - val_loss: 0.4322 - val_acc: 0.8083 - val_prec: 0.1762 - val_recall: 0.1702 - val_prec_1: 0.1759 - val_recall_1: 0.1486 - val_prec_2: 0.1909 - val_recall_2: 0.1709 - val_prec_3: 0.1720 - val_recall_3: 0.1614 - val_prec_4: 0.1922 - val_recall_4: 0.0677 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8063 - val_recall_m: 0.7965 - val_precision_m: 0.8166 - val_precision: 0.8166 - val_recall_6: 0.7965 - val_f1score: 0.8063\n",
      "Epoch 198/300\n",
      "epoch:  197\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3162 - acc: 0.8623 - prec: 0.9631 - recall: 0.9732 - prec_1: 0.8994 - recall_1: 0.9040 - prec_2: 0.9488 - recall_2: 0.9423 - prec_3: 0.7181 - recall_3: 0.7051 - prec_4: 0.6985 - recall_4: 0.7239 - prec_5: 0.9391 - recall_5: 0.9512 - f1_m: 0.8621 - recall_m: 0.8513 - precision_m: 0.8733 - precision: 0.8733 - recall_6: 0.8513 - f1score: 0.8621 - val_loss: 0.3598 - val_acc: 0.8381 - val_prec: 0.1762 - val_recall: 0.1727 - val_prec_1: 0.1759 - val_recall_1: 0.1591 - val_prec_2: 0.1897 - val_recall_2: 0.1727 - val_prec_3: 0.1754 - val_recall_3: 0.1521 - val_prec_4: 0.1909 - val_recall_4: 0.1033 - val_prec_5: 0.1672 - val_recall_5: 0.1536 - val_f1_m: 0.8374 - val_recall_m: 0.8266 - val_precision_m: 0.8488 - val_precision: 0.8488 - val_recall_6: 0.8266 - val_f1score: 0.8374\n",
      "Epoch 199/300\n",
      "epoch:  198\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3147 - acc: 0.8627 - prec: 0.9580 - recall: 0.9742 - prec_1: 0.9076 - recall_1: 0.9132 - prec_2: 0.9385 - recall_2: 0.9416 - prec_3: 0.7089 - recall_3: 0.7096 - prec_4: 0.7042 - recall_4: 0.7201 - prec_5: 0.9477 - recall_5: 0.9514 - f1_m: 0.8632 - recall_m: 0.8522 - precision_m: 0.8747 - precision: 0.8747 - recall_6: 0.8522 - f1score: 0.8632 - val_loss: 0.4197 - val_acc: 0.8156 - val_prec: 0.1762 - val_recall: 0.1729 - val_prec_1: 0.1758 - val_recall_1: 0.1585 - val_prec_2: 0.1897 - val_recall_2: 0.1775 - val_prec_3: 0.1441 - val_recall_3: 0.0718 - val_prec_4: 0.1822 - val_recall_4: 0.1411 - val_prec_5: 0.1672 - val_recall_5: 0.1561 - val_f1_m: 0.8125 - val_recall_m: 0.8001 - val_precision_m: 0.8260 - val_precision: 0.8260 - val_recall_6: 0.8001 - val_f1score: 0.8125\n",
      "Epoch 200/300\n",
      "epoch:  199\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3157 - acc: 0.8654 - prec: 0.9636 - recall: 0.9759 - prec_1: 0.8932 - recall_1: 0.9170 - prec_2: 0.9515 - recall_2: 0.9412 - prec_3: 0.7230 - recall_3: 0.6943 - prec_4: 0.6990 - recall_4: 0.7367 - prec_5: 0.9509 - recall_5: 0.9527 - f1_m: 0.8635 - recall_m: 0.8539 - precision_m: 0.8735 - precision: 0.8735 - recall_6: 0.8539 - f1score: 0.8635 - val_loss: 0.5088 - val_acc: 0.7845 - val_prec: 0.1762 - val_recall: 0.1721 - val_prec_1: 0.1758 - val_recall_1: 0.1363 - val_prec_2: 0.1897 - val_recall_2: 0.1795 - val_prec_3: 0.1702 - val_recall_3: 0.1729 - val_prec_4: 0.1121 - val_recall_4: 0.0148 - val_prec_5: 0.1672 - val_recall_5: 0.1594 - val_f1_m: 0.7784 - val_recall_m: 0.7663 - val_precision_m: 0.7914 - val_precision: 0.7914 - val_recall_6: 0.7663 - val_f1score: 0.7784\n",
      "Epoch 201/300\n",
      "epoch:  200\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3133 - acc: 0.8659 - prec: 0.9745 - recall: 0.9692 - prec_1: 0.9041 - recall_1: 0.9251 - prec_2: 0.9615 - recall_2: 0.9441 - prec_3: 0.7142 - recall_3: 0.6942 - prec_4: 0.6988 - recall_4: 0.7301 - prec_5: 0.9504 - recall_5: 0.9591 - f1_m: 0.8629 - recall_m: 0.8522 - precision_m: 0.8741 - precision: 0.8741 - recall_6: 0.8522 - f1score: 0.8629 - val_loss: 0.4405 - val_acc: 0.8093 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1758 - val_recall_1: 0.1529 - val_prec_2: 0.1909 - val_recall_2: 0.1800 - val_prec_3: 0.1441 - val_recall_3: 0.0347 - val_prec_4: 0.1822 - val_recall_4: 0.1822 - val_prec_5: 0.1672 - val_recall_5: 0.1556 - val_f1_m: 0.8054 - val_recall_m: 0.7918 - val_precision_m: 0.8200 - val_precision: 0.8200 - val_recall_6: 0.7918 - val_f1score: 0.8054\n",
      "Epoch 202/300\n",
      "epoch:  201\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 683us/step - loss: 0.3112 - acc: 0.8656 - prec: 0.9637 - recall: 0.9739 - prec_1: 0.9082 - recall_1: 0.9231 - prec_2: 0.9496 - recall_2: 0.9394 - prec_3: 0.7261 - recall_3: 0.6896 - prec_4: 0.6921 - recall_4: 0.7319 - prec_5: 0.9439 - recall_5: 0.9587 - f1_m: 0.8644 - recall_m: 0.8548 - precision_m: 0.8743 - precision: 0.8743 - recall_6: 0.8548 - f1score: 0.8644 - val_loss: 0.4419 - val_acc: 0.8263 - val_prec: 0.1762 - val_recall: 0.1709 - val_prec_1: 0.1759 - val_recall_1: 0.1614 - val_prec_2: 0.1909 - val_recall_2: 0.1687 - val_prec_3: 0.1744 - val_recall_3: 0.1314 - val_prec_4: 0.1922 - val_recall_4: 0.1125 - val_prec_5: 0.1672 - val_recall_5: 0.1559 - val_f1_m: 0.8272 - val_recall_m: 0.8198 - val_precision_m: 0.8348 - val_precision: 0.8348 - val_recall_6: 0.8198 - val_f1score: 0.8272\n",
      "Epoch 203/300\n",
      "epoch:  202\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3167 - acc: 0.8591 - prec: 0.9661 - recall: 0.9735 - prec_1: 0.8871 - recall_1: 0.9209 - prec_2: 0.9539 - recall_2: 0.9349 - prec_3: 0.7042 - recall_3: 0.6848 - prec_4: 0.6883 - recall_4: 0.7205 - prec_5: 0.9486 - recall_5: 0.9482 - f1_m: 0.8590 - recall_m: 0.8487 - precision_m: 0.8697 - precision: 0.8697 - recall_6: 0.8487 - f1score: 0.8590 - val_loss: 0.5634 - val_acc: 0.7883 - val_prec: 0.1762 - val_recall: 0.1723 - val_prec_1: 0.1758 - val_recall_1: 0.1610 - val_prec_2: 0.1897 - val_recall_2: 0.1780 - val_prec_3: 0.0801 - val_recall_3: 0.0083 - val_prec_4: 0.1822 - val_recall_4: 0.1837 - val_prec_5: 0.1672 - val_recall_5: 0.1528 - val_f1_m: 0.7872 - val_recall_m: 0.7765 - val_precision_m: 0.7986 - val_precision: 0.7986 - val_recall_6: 0.7765 - val_f1score: 0.7872\n",
      "Epoch 204/300\n",
      "epoch:  203\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3147 - acc: 0.8616 - prec: 0.9609 - recall: 0.9754 - prec_1: 0.9108 - recall_1: 0.9083 - prec_2: 0.9508 - recall_2: 0.9470 - prec_3: 0.7032 - recall_3: 0.6914 - prec_4: 0.6934 - recall_4: 0.7119 - prec_5: 0.9454 - recall_5: 0.9612 - f1_m: 0.8598 - recall_m: 0.8493 - precision_m: 0.8707 - precision: 0.8707 - recall_6: 0.8493 - f1score: 0.8598 - val_loss: 0.4082 - val_acc: 0.8023 - val_prec: 0.1762 - val_recall: 0.1702 - val_prec_1: 0.1755 - val_recall_1: 0.1643 - val_prec_2: 0.1895 - val_recall_2: 0.1719 - val_prec_3: 0.1602 - val_recall_3: 0.0704 - val_prec_4: 0.1822 - val_recall_4: 0.1346 - val_prec_5: 0.1672 - val_recall_5: 0.1521 - val_f1_m: 0.7999 - val_recall_m: 0.7893 - val_precision_m: 0.8112 - val_precision: 0.8112 - val_recall_6: 0.7893 - val_f1score: 0.7999\n",
      "Epoch 205/300\n",
      "epoch:  204\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3103 - acc: 0.8647 - prec: 0.9644 - recall: 0.9762 - prec_1: 0.9006 - recall_1: 0.9205 - prec_2: 0.9546 - recall_2: 0.9414 - prec_3: 0.7162 - recall_3: 0.6939 - prec_4: 0.6951 - recall_4: 0.7302 - prec_5: 0.9500 - recall_5: 0.9553 - f1_m: 0.8628 - recall_m: 0.8531 - precision_m: 0.8728 - precision: 0.8728 - recall_6: 0.8531 - f1score: 0.8628 - val_loss: 0.4027 - val_acc: 0.8216 - val_prec: 0.1762 - val_recall: 0.1724 - val_prec_1: 0.1758 - val_recall_1: 0.1620 - val_prec_2: 0.1897 - val_recall_2: 0.1754 - val_prec_3: 0.1441 - val_recall_3: 0.0774 - val_prec_4: 0.1822 - val_recall_4: 0.1426 - val_prec_5: 0.1672 - val_recall_5: 0.1533 - val_f1_m: 0.8204 - val_recall_m: 0.8101 - val_precision_m: 0.8314 - val_precision: 0.8314 - val_recall_6: 0.8101 - val_f1score: 0.8204\n",
      "Epoch 206/300\n",
      "epoch:  205\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3134 - acc: 0.8651 - prec: 0.9666 - recall: 0.9715 - prec_1: 0.9054 - recall_1: 0.9144 - prec_2: 0.9528 - recall_2: 0.9476 - prec_3: 0.7197 - recall_3: 0.7029 - prec_4: 0.7010 - recall_4: 0.7319 - prec_5: 0.9422 - recall_5: 0.9577 - f1_m: 0.8639 - recall_m: 0.8532 - precision_m: 0.8751 - precision: 0.8751 - recall_6: 0.8532 - f1score: 0.8639 - val_loss: 0.4764 - val_acc: 0.7893 - val_prec: 0.1762 - val_recall: 0.1689 - val_prec_1: 0.1756 - val_recall_1: 0.1663 - val_prec_2: 0.1907 - val_recall_2: 0.1699 - val_prec_3: 0.1121 - val_recall_3: 0.0480 - val_prec_4: 0.1822 - val_recall_4: 0.1446 - val_prec_5: 0.1672 - val_recall_5: 0.1503 - val_f1_m: 0.7873 - val_recall_m: 0.7773 - val_precision_m: 0.7980 - val_precision: 0.7980 - val_recall_6: 0.7773 - val_f1score: 0.7873\n",
      "Epoch 207/300\n",
      "epoch:  206\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3134 - acc: 0.8661 - prec: 0.9672 - recall: 0.9738 - prec_1: 0.9032 - recall_1: 0.9196 - prec_2: 0.9514 - recall_2: 0.9404 - prec_3: 0.7180 - recall_3: 0.7084 - prec_4: 0.7051 - recall_4: 0.7258 - prec_5: 0.9476 - recall_5: 0.9579 - f1_m: 0.8646 - recall_m: 0.8541 - precision_m: 0.8756 - precision: 0.8756 - recall_6: 0.8541 - f1score: 0.8646 - val_loss: 0.4587 - val_acc: 0.8091 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1543 - val_prec_2: 0.1897 - val_recall_2: 0.1754 - val_prec_3: 0.1758 - val_recall_3: 0.1687 - val_prec_4: 0.1762 - val_recall_4: 0.0489 - val_prec_5: 0.1672 - val_recall_5: 0.1579 - val_f1_m: 0.8074 - val_recall_m: 0.7960 - val_precision_m: 0.8195 - val_precision: 0.8195 - val_recall_6: 0.7960 - val_f1score: 0.8074\n",
      "Epoch 208/300\n",
      "epoch:  207\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3150 - acc: 0.8641 - prec: 0.9605 - recall: 0.9772 - prec_1: 0.8972 - recall_1: 0.9195 - prec_2: 0.9554 - recall_2: 0.9417 - prec_3: 0.7150 - recall_3: 0.7075 - prec_4: 0.7015 - recall_4: 0.7290 - prec_5: 0.9463 - recall_5: 0.9475 - f1_m: 0.8632 - recall_m: 0.8537 - precision_m: 0.8731 - precision: 0.8731 - recall_6: 0.8537 - f1score: 0.8632 - val_loss: 0.8245 - val_acc: 0.7715 - val_prec: 0.1762 - val_recall: 0.1682 - val_prec_1: 0.1759 - val_recall_1: 0.1443 - val_prec_2: 0.1897 - val_recall_2: 0.1790 - val_prec_3: 0.1441 - val_recall_3: 0.0839 - val_prec_4: 0.1602 - val_recall_4: 0.0846 - val_prec_5: 0.1672 - val_recall_5: 0.1558 - val_f1_m: 0.7701 - val_recall_m: 0.7583 - val_precision_m: 0.7828 - val_precision: 0.7828 - val_recall_6: 0.7583 - val_f1score: 0.7701\n",
      "Epoch 209/300\n",
      "epoch:  208\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3131 - acc: 0.8644 - prec: 0.9697 - recall: 0.9750 - prec_1: 0.9041 - recall_1: 0.9224 - prec_2: 0.9506 - recall_2: 0.9499 - prec_3: 0.7050 - recall_3: 0.6944 - prec_4: 0.6884 - recall_4: 0.7193 - prec_5: 0.9565 - recall_5: 0.9534 - f1_m: 0.8618 - recall_m: 0.8517 - precision_m: 0.8723 - precision: 0.8723 - recall_6: 0.8517 - f1score: 0.8618 - val_loss: 0.3431 - val_acc: 0.8466 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1759 - val_recall_1: 0.1630 - val_prec_2: 0.1909 - val_recall_2: 0.1712 - val_prec_3: 0.1762 - val_recall_3: 0.0980 - val_prec_4: 0.1828 - val_recall_4: 0.1579 - val_prec_5: 0.1672 - val_recall_5: 0.1566 - val_f1_m: 0.8443 - val_recall_m: 0.8343 - val_precision_m: 0.8548 - val_precision: 0.8548 - val_recall_6: 0.8343 - val_f1score: 0.8443\n",
      "Epoch 210/300\n",
      "epoch:  209\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3149 - acc: 0.8636 - prec: 0.9660 - recall: 0.9770 - prec_1: 0.9004 - recall_1: 0.9168 - prec_2: 0.9483 - recall_2: 0.9381 - prec_3: 0.7092 - recall_3: 0.6844 - prec_4: 0.6934 - recall_4: 0.7258 - prec_5: 0.9464 - recall_5: 0.9601 - f1_m: 0.8611 - recall_m: 0.8507 - precision_m: 0.8720 - precision: 0.8720 - recall_6: 0.8507 - f1score: 0.8611 - val_loss: 0.7509 - val_acc: 0.7908 - val_prec: 0.1762 - val_recall: 0.1712 - val_prec_1: 0.1758 - val_recall_1: 0.1421 - val_prec_2: 0.1897 - val_recall_2: 0.1807 - val_prec_3: 0.1121 - val_recall_3: 0.0439 - val_prec_4: 0.1822 - val_recall_4: 0.1639 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.7881 - val_recall_m: 0.7763 - val_precision_m: 0.8009 - val_precision: 0.8009 - val_recall_6: 0.7763 - val_f1score: 0.7881\n",
      "Epoch 211/300\n",
      "epoch:  210\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3139 - acc: 0.8581 - prec: 0.9603 - recall: 0.9703 - prec_1: 0.9018 - recall_1: 0.9120 - prec_2: 0.9525 - recall_2: 0.9397 - prec_3: 0.6988 - recall_3: 0.6821 - prec_4: 0.6805 - recall_4: 0.7082 - prec_5: 0.9431 - recall_5: 0.9531 - f1_m: 0.8573 - recall_m: 0.8465 - precision_m: 0.8687 - precision: 0.8687 - recall_6: 0.8465 - f1score: 0.8573 - val_loss: 0.5349 - val_acc: 0.8308 - val_prec: 0.1762 - val_recall: 0.1709 - val_prec_1: 0.1759 - val_recall_1: 0.1611 - val_prec_2: 0.1909 - val_recall_2: 0.1770 - val_prec_3: 0.1562 - val_recall_3: 0.0915 - val_prec_4: 0.1823 - val_recall_4: 0.1457 - val_prec_5: 0.1672 - val_recall_5: 0.1554 - val_f1_m: 0.8282 - val_recall_m: 0.8191 - val_precision_m: 0.8378 - val_precision: 0.8378 - val_recall_6: 0.8191 - val_f1score: 0.8282\n",
      "Epoch 212/300\n",
      "epoch:  211\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3123 - acc: 0.8631 - prec: 0.9652 - recall: 0.9742 - prec_1: 0.9020 - recall_1: 0.9126 - prec_2: 0.9504 - recall_2: 0.9438 - prec_3: 0.7159 - recall_3: 0.6954 - prec_4: 0.6940 - recall_4: 0.7256 - prec_5: 0.9406 - recall_5: 0.9535 - f1_m: 0.8634 - recall_m: 0.8528 - precision_m: 0.8745 - precision: 0.8745 - recall_6: 0.8528 - f1score: 0.8634 - val_loss: 0.3657 - val_acc: 0.8361 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1558 - val_prec_2: 0.1895 - val_recall_2: 0.1696 - val_prec_3: 0.1739 - val_recall_3: 0.1005 - val_prec_4: 0.1826 - val_recall_4: 0.1482 - val_prec_5: 0.1672 - val_recall_5: 0.1599 - val_f1_m: 0.8355 - val_recall_m: 0.8271 - val_precision_m: 0.8444 - val_precision: 0.8444 - val_recall_6: 0.8271 - val_f1score: 0.8355\n",
      "Epoch 213/300\n",
      "epoch:  212\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 681us/step - loss: 0.3128 - acc: 0.8644 - prec: 0.9621 - recall: 0.9775 - prec_1: 0.9016 - recall_1: 0.9232 - prec_2: 0.9613 - recall_2: 0.9423 - prec_3: 0.7136 - recall_3: 0.6929 - prec_4: 0.6957 - recall_4: 0.7341 - prec_5: 0.9481 - recall_5: 0.9542 - f1_m: 0.8629 - recall_m: 0.8534 - precision_m: 0.8727 - precision: 0.8727 - recall_6: 0.8534 - f1score: 0.8629 - val_loss: 0.5110 - val_acc: 0.7938 - val_prec: 0.1762 - val_recall: 0.1692 - val_prec_1: 0.1759 - val_recall_1: 0.1632 - val_prec_2: 0.1897 - val_recall_2: 0.1717 - val_prec_3: 0.1602 - val_recall_3: 0.0807 - val_prec_4: 0.1822 - val_recall_4: 0.1164 - val_prec_5: 0.1672 - val_recall_5: 0.1546 - val_f1_m: 0.7951 - val_recall_m: 0.7848 - val_precision_m: 0.8060 - val_precision: 0.8060 - val_recall_6: 0.7848 - val_f1score: 0.7951\n",
      "Epoch 214/300\n",
      "epoch:  213\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3075 - acc: 0.8644 - prec: 0.9638 - recall: 0.9750 - prec_1: 0.9103 - recall_1: 0.9234 - prec_2: 0.9553 - recall_2: 0.9427 - prec_3: 0.7121 - recall_3: 0.6872 - prec_4: 0.6942 - recall_4: 0.7244 - prec_5: 0.9492 - recall_5: 0.9533 - f1_m: 0.8645 - recall_m: 0.8537 - precision_m: 0.8757 - precision: 0.8757 - recall_6: 0.8537 - f1score: 0.8645 - val_loss: 0.4045 - val_acc: 0.8291 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1759 - val_recall_1: 0.1556 - val_prec_2: 0.1909 - val_recall_2: 0.1709 - val_prec_3: 0.1758 - val_recall_3: 0.1344 - val_prec_4: 0.1922 - val_recall_4: 0.1137 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8286 - val_recall_m: 0.8206 - val_precision_m: 0.8371 - val_precision: 0.8371 - val_recall_6: 0.8206 - val_f1score: 0.8286\n",
      "Epoch 215/300\n",
      "epoch:  214\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3063 - acc: 0.8711 - prec: 0.9721 - recall: 0.9726 - prec_1: 0.9080 - recall_1: 0.9233 - prec_2: 0.9544 - recall_2: 0.9451 - prec_3: 0.7313 - recall_3: 0.7067 - prec_4: 0.7082 - recall_4: 0.7470 - prec_5: 0.9498 - recall_5: 0.9568 - f1_m: 0.8691 - recall_m: 0.8591 - precision_m: 0.8795 - precision: 0.8795 - recall_6: 0.8591 - f1score: 0.8691 - val_loss: 0.4185 - val_acc: 0.7868 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1377 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1602 - val_recall_3: 0.0717 - val_prec_4: 0.1822 - val_recall_4: 0.1286 - val_prec_5: 0.1672 - val_recall_5: 0.1604 - val_f1_m: 0.7831 - val_recall_m: 0.7718 - val_precision_m: 0.7953 - val_precision: 0.7953 - val_recall_6: 0.7718 - val_f1score: 0.7831\n",
      "Epoch 216/300\n",
      "epoch:  215\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3070 - acc: 0.8669 - prec: 0.9609 - recall: 0.9713 - prec_1: 0.8973 - recall_1: 0.9186 - prec_2: 0.9576 - recall_2: 0.9403 - prec_3: 0.7314 - recall_3: 0.6958 - prec_4: 0.6974 - recall_4: 0.7457 - prec_5: 0.9537 - recall_5: 0.9511 - f1_m: 0.8665 - recall_m: 0.8563 - precision_m: 0.8770 - precision: 0.8770 - recall_6: 0.8563 - f1score: 0.8665 - val_loss: 0.5702 - val_acc: 0.8271 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1661 - val_prec_2: 0.1909 - val_recall_2: 0.1732 - val_prec_3: 0.1441 - val_recall_3: 0.0522 - val_prec_4: 0.1822 - val_recall_4: 0.1794 - val_prec_5: 0.1672 - val_recall_5: 0.1526 - val_f1_m: 0.8249 - val_recall_m: 0.8163 - val_precision_m: 0.8340 - val_precision: 0.8340 - val_recall_6: 0.8163 - val_f1score: 0.8249\n",
      "Epoch 217/300\n",
      "epoch:  216\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3099 - acc: 0.8657 - prec: 0.9676 - recall: 0.9764 - prec_1: 0.9036 - recall_1: 0.9227 - prec_2: 0.9565 - recall_2: 0.9459 - prec_3: 0.7208 - recall_3: 0.6753 - prec_4: 0.6894 - recall_4: 0.7534 - prec_5: 0.9549 - recall_5: 0.9509 - f1_m: 0.8649 - recall_m: 0.8551 - precision_m: 0.8751 - precision: 0.8751 - recall_6: 0.8551 - f1score: 0.8649 - val_loss: 0.3815 - val_acc: 0.8211 - val_prec: 0.1762 - val_recall: 0.1726 - val_prec_1: 0.1758 - val_recall_1: 0.1630 - val_prec_2: 0.1888 - val_recall_2: 0.1737 - val_prec_3: 0.1441 - val_recall_3: 0.0908 - val_prec_4: 0.1773 - val_recall_4: 0.1301 - val_prec_5: 0.1672 - val_recall_5: 0.1533 - val_f1_m: 0.8207 - val_recall_m: 0.8113 - val_precision_m: 0.8306 - val_precision: 0.8306 - val_recall_6: 0.8113 - val_f1score: 0.8207\n",
      "Epoch 218/300\n",
      "epoch:  217\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3115 - acc: 0.8623 - prec: 0.9682 - recall: 0.9725 - prec_1: 0.9064 - recall_1: 0.9173 - prec_2: 0.9582 - recall_2: 0.9438 - prec_3: 0.7028 - recall_3: 0.7006 - prec_4: 0.6981 - recall_4: 0.7095 - prec_5: 0.9458 - recall_5: 0.9592 - f1_m: 0.8612 - recall_m: 0.8511 - precision_m: 0.8717 - precision: 0.8717 - recall_6: 0.8511 - f1score: 0.8612 - val_loss: 0.5563 - val_acc: 0.8126 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1759 - val_recall_1: 0.1633 - val_prec_2: 0.1909 - val_recall_2: 0.1714 - val_prec_3: 0.1741 - val_recall_3: 0.1054 - val_prec_4: 0.1844 - val_recall_4: 0.1109 - val_prec_5: 0.1672 - val_recall_5: 0.1541 - val_f1_m: 0.8120 - val_recall_m: 0.8023 - val_precision_m: 0.8223 - val_precision: 0.8223 - val_recall_6: 0.8023 - val_f1score: 0.8120\n",
      "Epoch 219/300\n",
      "epoch:  218\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 683us/step - loss: 0.3136 - acc: 0.8663 - prec: 0.9699 - recall: 0.9737 - prec_1: 0.9047 - recall_1: 0.9253 - prec_2: 0.9557 - recall_2: 0.9502 - prec_3: 0.7127 - recall_3: 0.6854 - prec_4: 0.6929 - recall_4: 0.7290 - prec_5: 0.9489 - recall_5: 0.9516 - f1_m: 0.8642 - recall_m: 0.8549 - precision_m: 0.8738 - precision: 0.8738 - recall_6: 0.8549 - f1score: 0.8642 - val_loss: 0.4775 - val_acc: 0.7990 - val_prec: 0.1762 - val_recall: 0.1712 - val_prec_1: 0.1758 - val_recall_1: 0.1351 - val_prec_2: 0.1897 - val_recall_2: 0.1825 - val_prec_3: 0.1441 - val_recall_3: 0.0384 - val_prec_4: 0.1822 - val_recall_4: 0.1804 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.7964 - val_recall_m: 0.7833 - val_precision_m: 0.8106 - val_precision: 0.8106 - val_recall_6: 0.7833 - val_f1score: 0.7964\n",
      "Epoch 220/300\n",
      "epoch:  219\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3140 - acc: 0.8623 - prec: 0.9644 - recall: 0.9751 - prec_1: 0.9086 - recall_1: 0.9217 - prec_2: 0.9566 - recall_2: 0.9416 - prec_3: 0.7098 - recall_3: 0.6832 - prec_4: 0.6920 - recall_4: 0.7266 - prec_5: 0.9426 - recall_5: 0.9597 - f1_m: 0.8610 - recall_m: 0.8506 - precision_m: 0.8720 - precision: 0.8720 - recall_6: 0.8506 - f1score: 0.8610 - val_loss: 0.3756 - val_acc: 0.8366 - val_prec: 0.1762 - val_recall: 0.1694 - val_prec_1: 0.1759 - val_recall_1: 0.1586 - val_prec_2: 0.1897 - val_recall_2: 0.1727 - val_prec_3: 0.1758 - val_recall_3: 0.1236 - val_prec_4: 0.1909 - val_recall_4: 0.1307 - val_prec_5: 0.1672 - val_recall_5: 0.1574 - val_f1_m: 0.8357 - val_recall_m: 0.8268 - val_precision_m: 0.8450 - val_precision: 0.8450 - val_recall_6: 0.8268 - val_f1score: 0.8357\n",
      "Epoch 221/300\n",
      "epoch:  220\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3143 - acc: 0.8631 - prec: 0.9696 - recall: 0.9728 - prec_1: 0.8909 - recall_1: 0.9255 - prec_2: 0.9586 - recall_2: 0.9390 - prec_3: 0.7146 - recall_3: 0.6906 - prec_4: 0.6952 - recall_4: 0.7334 - prec_5: 0.9429 - recall_5: 0.9532 - f1_m: 0.8613 - recall_m: 0.8512 - precision_m: 0.8718 - precision: 0.8718 - recall_6: 0.8512 - f1score: 0.8613 - val_loss: 0.9186 - val_acc: 0.7838 - val_prec: 0.1762 - val_recall: 0.1718 - val_prec_1: 0.1757 - val_recall_1: 0.1306 - val_prec_2: 0.1899 - val_recall_2: 0.1845 - val_prec_3: 0.0641 - val_recall_3: 0.0308 - val_prec_4: 0.1822 - val_recall_4: 0.1834 - val_prec_5: 0.1672 - val_recall_5: 0.1523 - val_f1_m: 0.7816 - val_recall_m: 0.7663 - val_precision_m: 0.7987 - val_precision: 0.7987 - val_recall_6: 0.7663 - val_f1score: 0.7816\n",
      "Epoch 222/300\n",
      "epoch:  221\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3098 - acc: 0.8656 - prec: 0.9587 - recall: 0.9678 - prec_1: 0.9086 - recall_1: 0.9197 - prec_2: 0.9546 - recall_2: 0.9452 - prec_3: 0.7113 - recall_3: 0.6910 - prec_4: 0.6955 - recall_4: 0.7329 - prec_5: 0.9523 - recall_5: 0.9608 - f1_m: 0.8653 - recall_m: 0.8563 - precision_m: 0.8747 - precision: 0.8747 - recall_6: 0.8563 - f1score: 0.8653 - val_loss: 0.4170 - val_acc: 0.7965 - val_prec: 0.1762 - val_recall: 0.1699 - val_prec_1: 0.1758 - val_recall_1: 0.1533 - val_prec_2: 0.1897 - val_recall_2: 0.1805 - val_prec_3: 0.1441 - val_recall_3: 0.0666 - val_prec_4: 0.1762 - val_recall_4: 0.1234 - val_prec_5: 0.1672 - val_recall_5: 0.1526 - val_f1_m: 0.7945 - val_recall_m: 0.7820 - val_precision_m: 0.8078 - val_precision: 0.8078 - val_recall_6: 0.7820 - val_f1score: 0.7945\n",
      "Epoch 223/300\n",
      "epoch:  222\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3164 - acc: 0.8648 - prec: 0.9636 - recall: 0.9760 - prec_1: 0.9054 - recall_1: 0.9258 - prec_2: 0.9574 - recall_2: 0.9440 - prec_3: 0.7114 - recall_3: 0.6984 - prec_4: 0.6990 - recall_4: 0.7238 - prec_5: 0.9481 - recall_5: 0.9533 - f1_m: 0.8624 - recall_m: 0.8531 - precision_m: 0.8721 - precision: 0.8721 - recall_6: 0.8531 - f1score: 0.8624 - val_loss: 0.5822 - val_acc: 0.8258 - val_prec: 0.1762 - val_recall: 0.1704 - val_prec_1: 0.1759 - val_recall_1: 0.1535 - val_prec_2: 0.1909 - val_recall_2: 0.1702 - val_prec_3: 0.1745 - val_recall_3: 0.1209 - val_prec_4: 0.1890 - val_recall_4: 0.1262 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8252 - val_recall_m: 0.8171 - val_precision_m: 0.8337 - val_precision: 0.8337 - val_recall_6: 0.8171 - val_f1score: 0.8252\n",
      "Epoch 224/300\n",
      "epoch:  223\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3122 - acc: 0.8619 - prec: 0.9668 - recall: 0.9772 - prec_1: 0.8981 - recall_1: 0.9229 - prec_2: 0.9560 - recall_2: 0.9436 - prec_3: 0.7127 - recall_3: 0.6893 - prec_4: 0.6910 - recall_4: 0.7263 - prec_5: 0.9478 - recall_5: 0.9526 - f1_m: 0.8603 - recall_m: 0.8507 - precision_m: 0.8704 - precision: 0.8704 - recall_6: 0.8507 - f1score: 0.8603 - val_loss: 0.4353 - val_acc: 0.8083 - val_prec: 0.1762 - val_recall: 0.1721 - val_prec_1: 0.1755 - val_recall_1: 0.1522 - val_prec_2: 0.1906 - val_recall_2: 0.1655 - val_prec_3: 0.1747 - val_recall_3: 0.1697 - val_prec_4: 0.1922 - val_recall_4: 0.0541 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8088 - val_recall_m: 0.7985 - val_precision_m: 0.8197 - val_precision: 0.8197 - val_recall_6: 0.7985 - val_f1score: 0.8088\n",
      "Epoch 225/300\n",
      "epoch:  224\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 680us/step - loss: 0.3114 - acc: 0.8658 - prec: 0.9623 - recall: 0.9756 - prec_1: 0.9003 - recall_1: 0.9173 - prec_2: 0.9402 - recall_2: 0.9381 - prec_3: 0.7177 - recall_3: 0.7144 - prec_4: 0.7127 - recall_4: 0.7255 - prec_5: 0.9556 - recall_5: 0.9603 - f1_m: 0.8648 - recall_m: 0.8544 - precision_m: 0.8756 - precision: 0.8756 - recall_6: 0.8544 - f1score: 0.8648 - val_loss: 0.3730 - val_acc: 0.8086 - val_prec: 0.1762 - val_recall: 0.1704 - val_prec_1: 0.1759 - val_recall_1: 0.1486 - val_prec_2: 0.1907 - val_recall_2: 0.1731 - val_prec_3: 0.1762 - val_recall_3: 0.0939 - val_prec_4: 0.1853 - val_recall_4: 0.1326 - val_prec_5: 0.1672 - val_recall_5: 0.1609 - val_f1_m: 0.8075 - val_recall_m: 0.7975 - val_precision_m: 0.8180 - val_precision: 0.8180 - val_recall_6: 0.7975 - val_f1score: 0.8075\n",
      "Epoch 226/300\n",
      "epoch:  225\n",
      "Learning rate:  0.001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3095 - acc: 0.8649 - prec: 0.9670 - recall: 0.9761 - prec_1: 0.9068 - recall_1: 0.9162 - prec_2: 0.9582 - recall_2: 0.9409 - prec_3: 0.7168 - recall_3: 0.6877 - prec_4: 0.6947 - recall_4: 0.7400 - prec_5: 0.9459 - recall_5: 0.9546 - f1_m: 0.8647 - recall_m: 0.8539 - precision_m: 0.8759 - precision: 0.8759 - recall_6: 0.8539 - f1score: 0.8647 - val_loss: 0.5636 - val_acc: 0.7818 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1591 - val_prec_2: 0.1897 - val_recall_2: 0.1790 - val_prec_3: 0.1121 - val_recall_3: 0.0353 - val_prec_4: 0.1662 - val_recall_4: 0.1376 - val_prec_5: 0.1672 - val_recall_5: 0.1508 - val_f1_m: 0.7808 - val_recall_m: 0.7690 - val_precision_m: 0.7935 - val_precision: 0.7935 - val_recall_6: 0.7690 - val_f1score: 0.7808\n",
      "Epoch 227/300\n",
      "epoch:  226\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3112 - acc: 0.8663 - prec: 0.9703 - recall: 0.9771 - prec_1: 0.8983 - recall_1: 0.9200 - prec_2: 0.9567 - recall_2: 0.9382 - prec_3: 0.7121 - recall_3: 0.7001 - prec_4: 0.7011 - recall_4: 0.7302 - prec_5: 0.9467 - recall_5: 0.9588 - f1_m: 0.8660 - recall_m: 0.8563 - precision_m: 0.8761 - precision: 0.8761 - recall_6: 0.8563 - f1score: 0.8660 - val_loss: 0.3436 - val_acc: 0.8551 - val_prec: 0.1762 - val_recall: 0.1727 - val_prec_1: 0.1758 - val_recall_1: 0.1575 - val_prec_2: 0.1897 - val_recall_2: 0.1785 - val_prec_3: 0.1762 - val_recall_3: 0.1068 - val_prec_4: 0.1844 - val_recall_4: 0.1532 - val_prec_5: 0.1672 - val_recall_5: 0.1573 - val_f1_m: 0.8526 - val_recall_m: 0.8413 - val_precision_m: 0.8646 - val_precision: 0.8646 - val_recall_6: 0.8413 - val_f1score: 0.8526\n",
      "Epoch 228/300\n",
      "epoch:  227\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3094 - acc: 0.8646 - prec: 0.9630 - recall: 0.9726 - prec_1: 0.9070 - recall_1: 0.9227 - prec_2: 0.9561 - recall_2: 0.9414 - prec_3: 0.7212 - recall_3: 0.7021 - prec_4: 0.6958 - recall_4: 0.7299 - prec_5: 0.9457 - recall_5: 0.9618 - f1_m: 0.8642 - recall_m: 0.8546 - precision_m: 0.8742 - precision: 0.8742 - recall_6: 0.8546 - f1score: 0.8642 - val_loss: 0.3241 - val_acc: 0.8651 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1597 - val_prec_2: 0.1897 - val_recall_2: 0.1754 - val_prec_3: 0.1762 - val_recall_3: 0.1350 - val_prec_4: 0.1899 - val_recall_4: 0.1401 - val_prec_5: 0.1672 - val_recall_5: 0.1589 - val_f1_m: 0.8626 - val_recall_m: 0.8524 - val_precision_m: 0.8733 - val_precision: 0.8733 - val_recall_6: 0.8524 - val_f1score: 0.8626\n",
      "Epoch 229/300\n",
      "epoch:  228\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3141 - acc: 0.8622 - prec: 0.9705 - recall: 0.9723 - prec_1: 0.9050 - recall_1: 0.9144 - prec_2: 0.9567 - recall_2: 0.9420 - prec_3: 0.7127 - recall_3: 0.6892 - prec_4: 0.6893 - recall_4: 0.7285 - prec_5: 0.9382 - recall_5: 0.9606 - f1_m: 0.8596 - recall_m: 0.8500 - precision_m: 0.8696 - precision: 0.8696 - recall_6: 0.8500 - f1score: 0.8596 - val_loss: 0.3360 - val_acc: 0.8514 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1570 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1372 - val_prec_4: 0.1916 - val_recall_4: 0.1261 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8507 - val_recall_m: 0.8413 - val_precision_m: 0.8605 - val_precision: 0.8605 - val_recall_6: 0.8413 - val_f1score: 0.8507\n",
      "Epoch 230/300\n",
      "epoch:  229\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.3059 - acc: 0.8702 - prec: 0.9694 - recall: 0.9752 - prec_1: 0.8996 - recall_1: 0.9276 - prec_2: 0.9516 - recall_2: 0.9413 - prec_3: 0.7297 - recall_3: 0.7098 - prec_4: 0.7053 - recall_4: 0.7457 - prec_5: 0.9544 - recall_5: 0.9533 - f1_m: 0.8705 - recall_m: 0.8611 - precision_m: 0.8802 - precision: 0.8802 - recall_6: 0.8611 - f1score: 0.8705 - val_loss: 0.3311 - val_acc: 0.8554 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1586 - val_prec_2: 0.1909 - val_recall_2: 0.1749 - val_prec_3: 0.1762 - val_recall_3: 0.1429 - val_prec_4: 0.1904 - val_recall_4: 0.1239 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8523 - val_recall_m: 0.8428 - val_precision_m: 0.8622 - val_precision: 0.8622 - val_recall_6: 0.8428 - val_f1score: 0.8523\n",
      "Epoch 231/300\n",
      "epoch:  230\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3090 - acc: 0.8651 - prec: 0.9620 - recall: 0.9765 - prec_1: 0.9072 - recall_1: 0.9212 - prec_2: 0.9506 - recall_2: 0.9417 - prec_3: 0.7102 - recall_3: 0.6887 - prec_4: 0.6927 - recall_4: 0.7283 - prec_5: 0.9525 - recall_5: 0.9605 - f1_m: 0.8651 - recall_m: 0.8556 - precision_m: 0.8749 - precision: 0.8749 - recall_6: 0.8556 - f1score: 0.8651 - val_loss: 0.3273 - val_acc: 0.8581 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1577 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1298 - val_prec_4: 0.1886 - val_recall_4: 0.1389 - val_prec_5: 0.1672 - val_recall_5: 0.1589 - val_f1_m: 0.8568 - val_recall_m: 0.8466 - val_precision_m: 0.8674 - val_precision: 0.8674 - val_recall_6: 0.8466 - val_f1score: 0.8568\n",
      "Epoch 232/300\n",
      "epoch:  231\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3073 - acc: 0.8679 - prec: 0.9679 - recall: 0.9759 - prec_1: 0.9200 - recall_1: 0.9228 - prec_2: 0.9570 - recall_2: 0.9506 - prec_3: 0.7152 - recall_3: 0.7076 - prec_4: 0.6999 - recall_4: 0.7281 - prec_5: 0.9461 - recall_5: 0.9626 - f1_m: 0.8665 - recall_m: 0.8564 - precision_m: 0.8769 - precision: 0.8769 - recall_6: 0.8564 - f1score: 0.8665 - val_loss: 0.3313 - val_acc: 0.8546 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1557 - val_prec_2: 0.1897 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1246 - val_prec_4: 0.1894 - val_recall_4: 0.1421 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8532 - val_recall_m: 0.8431 - val_precision_m: 0.8638 - val_precision: 0.8638 - val_recall_6: 0.8431 - val_f1score: 0.8532\n",
      "Epoch 233/300\n",
      "epoch:  232\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3120 - acc: 0.8647 - prec: 0.9728 - recall: 0.9731 - prec_1: 0.9065 - recall_1: 0.9220 - prec_2: 0.9473 - recall_2: 0.9452 - prec_3: 0.7206 - recall_3: 0.7055 - prec_4: 0.7039 - recall_4: 0.7332 - prec_5: 0.9487 - recall_5: 0.9553 - f1_m: 0.8631 - recall_m: 0.8531 - precision_m: 0.8736 - precision: 0.8736 - recall_6: 0.8531 - f1score: 0.8631 - val_loss: 0.3286 - val_acc: 0.8581 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1570 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1454 - val_prec_4: 0.1904 - val_recall_4: 0.1244 - val_prec_5: 0.1672 - val_recall_5: 0.1589 - val_f1_m: 0.8575 - val_recall_m: 0.8483 - val_precision_m: 0.8672 - val_precision: 0.8672 - val_recall_6: 0.8483 - val_f1score: 0.8575\n",
      "Epoch 234/300\n",
      "epoch:  233\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3073 - acc: 0.8677 - prec: 0.9673 - recall: 0.9746 - prec_1: 0.9142 - recall_1: 0.9238 - prec_2: 0.9591 - recall_2: 0.9463 - prec_3: 0.7123 - recall_3: 0.7026 - prec_4: 0.6994 - recall_4: 0.7232 - prec_5: 0.9496 - recall_5: 0.9610 - f1_m: 0.8658 - recall_m: 0.8546 - precision_m: 0.8776 - precision: 0.8776 - recall_6: 0.8546 - f1score: 0.8658 - val_loss: 0.3269 - val_acc: 0.8496 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1759 - val_recall_1: 0.1601 - val_prec_2: 0.1909 - val_recall_2: 0.1749 - val_prec_3: 0.1762 - val_recall_3: 0.1125 - val_prec_4: 0.1847 - val_recall_4: 0.1451 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8466 - val_recall_m: 0.8371 - val_precision_m: 0.8565 - val_precision: 0.8565 - val_recall_6: 0.8371 - val_f1score: 0.8466\n",
      "Epoch 235/300\n",
      "epoch:  234\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3095 - acc: 0.8634 - prec: 0.9737 - recall: 0.9752 - prec_1: 0.9018 - recall_1: 0.9189 - prec_2: 0.9557 - recall_2: 0.9439 - prec_3: 0.7077 - recall_3: 0.7029 - prec_4: 0.6998 - recall_4: 0.7218 - prec_5: 0.9432 - recall_5: 0.9564 - f1_m: 0.8626 - recall_m: 0.8523 - precision_m: 0.8732 - precision: 0.8732 - recall_6: 0.8523 - f1score: 0.8626 - val_loss: 0.3241 - val_acc: 0.8621 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1580 - val_prec_2: 0.1897 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1261 - val_prec_4: 0.1875 - val_recall_4: 0.1449 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8606 - val_recall_m: 0.8514 - val_precision_m: 0.8703 - val_precision: 0.8703 - val_recall_6: 0.8514 - val_f1score: 0.8606\n",
      "Epoch 236/300\n",
      "epoch:  235\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 686us/step - loss: 0.3114 - acc: 0.8657 - prec: 0.9676 - recall: 0.9719 - prec_1: 0.8955 - recall_1: 0.9222 - prec_2: 0.9529 - recall_2: 0.9391 - prec_3: 0.7161 - recall_3: 0.6952 - prec_4: 0.7039 - recall_4: 0.7334 - prec_5: 0.9487 - recall_5: 0.9580 - f1_m: 0.8647 - recall_m: 0.8553 - precision_m: 0.8745 - precision: 0.8745 - recall_6: 0.8553 - f1score: 0.8647 - val_loss: 0.3243 - val_acc: 0.8506 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1759 - val_recall_1: 0.1593 - val_prec_2: 0.1909 - val_recall_2: 0.1754 - val_prec_3: 0.1762 - val_recall_3: 0.1153 - val_prec_4: 0.1869 - val_recall_4: 0.1446 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8488 - val_recall_m: 0.8398 - val_precision_m: 0.8583 - val_precision: 0.8583 - val_recall_6: 0.8398 - val_f1score: 0.8488\n",
      "Epoch 237/300\n",
      "epoch:  236\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3108 - acc: 0.8677 - prec: 0.9667 - recall: 0.9747 - prec_1: 0.9075 - recall_1: 0.9277 - prec_2: 0.9640 - recall_2: 0.9485 - prec_3: 0.7240 - recall_3: 0.6942 - prec_4: 0.7004 - recall_4: 0.7404 - prec_5: 0.9462 - recall_5: 0.9556 - f1_m: 0.8676 - recall_m: 0.8578 - precision_m: 0.8777 - precision: 0.8777 - recall_6: 0.8578 - f1score: 0.8676 - val_loss: 0.3238 - val_acc: 0.8649 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1759 - val_recall_1: 0.1593 - val_prec_2: 0.1909 - val_recall_2: 0.1754 - val_prec_3: 0.1762 - val_recall_3: 0.1352 - val_prec_4: 0.1916 - val_recall_4: 0.1409 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8612 - val_recall_m: 0.8506 - val_precision_m: 0.8722 - val_precision: 0.8722 - val_recall_6: 0.8506 - val_f1score: 0.8612\n",
      "Epoch 238/300\n",
      "epoch:  237\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3045 - acc: 0.8691 - prec: 0.9715 - recall: 0.9730 - prec_1: 0.9027 - recall_1: 0.9211 - prec_2: 0.9570 - recall_2: 0.9474 - prec_3: 0.7333 - recall_3: 0.6875 - prec_4: 0.6958 - recall_4: 0.7577 - prec_5: 0.9502 - recall_5: 0.9542 - f1_m: 0.8659 - recall_m: 0.8554 - precision_m: 0.8768 - precision: 0.8768 - recall_6: 0.8554 - f1score: 0.8659 - val_loss: 0.3242 - val_acc: 0.8566 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1759 - val_recall_1: 0.1596 - val_prec_2: 0.1909 - val_recall_2: 0.1749 - val_prec_3: 0.1762 - val_recall_3: 0.1294 - val_prec_4: 0.1904 - val_recall_4: 0.1381 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8544 - val_recall_m: 0.8456 - val_precision_m: 0.8637 - val_precision: 0.8637 - val_recall_6: 0.8456 - val_f1score: 0.8544\n",
      "Epoch 239/300\n",
      "epoch:  238\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3062 - acc: 0.8689 - prec: 0.9646 - recall: 0.9718 - prec_1: 0.9184 - recall_1: 0.9139 - prec_2: 0.9518 - recall_2: 0.9490 - prec_3: 0.7294 - recall_3: 0.6960 - prec_4: 0.7014 - recall_4: 0.7511 - prec_5: 0.9486 - recall_5: 0.9605 - f1_m: 0.8662 - recall_m: 0.8557 - precision_m: 0.8771 - precision: 0.8771 - recall_6: 0.8557 - f1score: 0.8662 - val_loss: 0.3289 - val_acc: 0.8586 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1580 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1432 - val_prec_4: 0.1916 - val_recall_4: 0.1266 - val_prec_5: 0.1672 - val_recall_5: 0.1589 - val_f1_m: 0.8550 - val_recall_m: 0.8453 - val_precision_m: 0.8651 - val_precision: 0.8651 - val_recall_6: 0.8453 - val_f1score: 0.8550\n",
      "Epoch 240/300\n",
      "epoch:  239\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 680us/step - loss: 0.3108 - acc: 0.8653 - prec: 0.9639 - recall: 0.9796 - prec_1: 0.9048 - recall_1: 0.9159 - prec_2: 0.9546 - recall_2: 0.9425 - prec_3: 0.7172 - recall_3: 0.6825 - prec_4: 0.6953 - recall_4: 0.7421 - prec_5: 0.9486 - recall_5: 0.9575 - f1_m: 0.8632 - recall_m: 0.8531 - precision_m: 0.8736 - precision: 0.8736 - recall_6: 0.8531 - f1score: 0.8632 - val_loss: 0.3221 - val_acc: 0.8606 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1589 - val_prec_2: 0.1897 - val_recall_2: 0.1759 - val_prec_3: 0.1762 - val_recall_3: 0.1299 - val_prec_4: 0.1894 - val_recall_4: 0.1414 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8591 - val_recall_m: 0.8491 - val_precision_m: 0.8696 - val_precision: 0.8696 - val_recall_6: 0.8491 - val_f1score: 0.8591\n",
      "Epoch 241/300\n",
      "epoch:  240\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3104 - acc: 0.8682 - prec: 0.9669 - recall: 0.9754 - prec_1: 0.9132 - recall_1: 0.9273 - prec_2: 0.9558 - recall_2: 0.9424 - prec_3: 0.7214 - recall_3: 0.6996 - prec_4: 0.7032 - recall_4: 0.7363 - prec_5: 0.9473 - recall_5: 0.9568 - f1_m: 0.8665 - recall_m: 0.8552 - precision_m: 0.8782 - precision: 0.8782 - recall_6: 0.8552 - f1score: 0.8665 - val_loss: 0.3299 - val_acc: 0.8491 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1759 - val_recall_1: 0.1607 - val_prec_2: 0.1909 - val_recall_2: 0.1752 - val_prec_3: 0.1762 - val_recall_3: 0.1077 - val_prec_4: 0.1836 - val_recall_4: 0.1476 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8465 - val_recall_m: 0.8376 - val_precision_m: 0.8558 - val_precision: 0.8558 - val_recall_6: 0.8376 - val_f1score: 0.8465\n",
      "Epoch 242/300\n",
      "epoch:  241\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3080 - acc: 0.8673 - prec: 0.9719 - recall: 0.9738 - prec_1: 0.9041 - recall_1: 0.9195 - prec_2: 0.9514 - recall_2: 0.9468 - prec_3: 0.7234 - recall_3: 0.7021 - prec_4: 0.7048 - recall_4: 0.7423 - prec_5: 0.9412 - recall_5: 0.9564 - f1_m: 0.8665 - recall_m: 0.8569 - precision_m: 0.8764 - precision: 0.8764 - recall_6: 0.8569 - f1score: 0.8665 - val_loss: 0.3231 - val_acc: 0.8591 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1583 - val_prec_2: 0.1909 - val_recall_2: 0.1765 - val_prec_3: 0.1762 - val_recall_3: 0.1282 - val_prec_4: 0.1890 - val_recall_4: 0.1416 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8575 - val_recall_m: 0.8471 - val_precision_m: 0.8685 - val_precision: 0.8685 - val_recall_6: 0.8471 - val_f1score: 0.8575\n",
      "Epoch 243/300\n",
      "epoch:  242\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3081 - acc: 0.8698 - prec: 0.9653 - recall: 0.9790 - prec_1: 0.9096 - recall_1: 0.9216 - prec_2: 0.9572 - recall_2: 0.9446 - prec_3: 0.7268 - recall_3: 0.7033 - prec_4: 0.7124 - recall_4: 0.7479 - prec_5: 0.9460 - recall_5: 0.9589 - f1_m: 0.8672 - recall_m: 0.8567 - precision_m: 0.8780 - precision: 0.8780 - recall_6: 0.8567 - f1score: 0.8672 - val_loss: 0.3266 - val_acc: 0.8536 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1575 - val_prec_2: 0.1897 - val_recall_2: 0.1772 - val_prec_3: 0.1762 - val_recall_3: 0.1095 - val_prec_4: 0.1863 - val_recall_4: 0.1532 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8519 - val_recall_m: 0.8421 - val_precision_m: 0.8623 - val_precision: 0.8623 - val_recall_6: 0.8421 - val_f1score: 0.8519\n",
      "Epoch 244/300\n",
      "epoch:  243\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3122 - acc: 0.8672 - prec: 0.9675 - recall: 0.9714 - prec_1: 0.9047 - recall_1: 0.9168 - prec_2: 0.9534 - recall_2: 0.9406 - prec_3: 0.7254 - recall_3: 0.7127 - prec_4: 0.7049 - recall_4: 0.7264 - prec_5: 0.9463 - recall_5: 0.9626 - f1_m: 0.8664 - recall_m: 0.8564 - precision_m: 0.8768 - precision: 0.8768 - recall_6: 0.8564 - f1score: 0.8664 - val_loss: 0.3304 - val_acc: 0.8483 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1572 - val_prec_2: 0.1897 - val_recall_2: 0.1775 - val_prec_3: 0.1762 - val_recall_3: 0.0959 - val_prec_4: 0.1832 - val_recall_4: 0.1599 - val_prec_5: 0.1672 - val_recall_5: 0.1579 - val_f1_m: 0.8493 - val_recall_m: 0.8393 - val_precision_m: 0.8598 - val_precision: 0.8598 - val_recall_6: 0.8393 - val_f1score: 0.8493\n",
      "Epoch 245/300\n",
      "epoch:  244\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3070 - acc: 0.8651 - prec: 0.9664 - recall: 0.9760 - prec_1: 0.8951 - recall_1: 0.9206 - prec_2: 0.9509 - recall_2: 0.9427 - prec_3: 0.7193 - recall_3: 0.6974 - prec_4: 0.7011 - recall_4: 0.7267 - prec_5: 0.9530 - recall_5: 0.9538 - f1_m: 0.8643 - recall_m: 0.8546 - precision_m: 0.8744 - precision: 0.8744 - recall_6: 0.8546 - f1score: 0.8643 - val_loss: 0.3239 - val_acc: 0.8586 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1765 - val_prec_3: 0.1762 - val_recall_3: 0.1223 - val_prec_4: 0.1856 - val_recall_4: 0.1459 - val_prec_5: 0.1672 - val_recall_5: 0.1574 - val_f1_m: 0.8583 - val_recall_m: 0.8478 - val_precision_m: 0.8692 - val_precision: 0.8692 - val_recall_6: 0.8478 - val_f1score: 0.8583\n",
      "Epoch 246/300\n",
      "epoch:  245\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3061 - acc: 0.8677 - prec: 0.9605 - recall: 0.9748 - prec_1: 0.9068 - recall_1: 0.9180 - prec_2: 0.9490 - recall_2: 0.9435 - prec_3: 0.7227 - recall_3: 0.7107 - prec_4: 0.7013 - recall_4: 0.7264 - prec_5: 0.9567 - recall_5: 0.9582 - f1_m: 0.8661 - recall_m: 0.8562 - precision_m: 0.8764 - precision: 0.8764 - recall_6: 0.8562 - f1score: 0.8661 - val_loss: 0.3241 - val_acc: 0.8636 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1589 - val_prec_2: 0.1897 - val_recall_2: 0.1757 - val_prec_3: 0.1762 - val_recall_3: 0.1362 - val_prec_4: 0.1904 - val_recall_4: 0.1389 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8626 - val_recall_m: 0.8524 - val_precision_m: 0.8733 - val_precision: 0.8733 - val_recall_6: 0.8524 - val_f1score: 0.8626\n",
      "Epoch 247/300\n",
      "epoch:  246\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 681us/step - loss: 0.3105 - acc: 0.8658 - prec: 0.9624 - recall: 0.9752 - prec_1: 0.9092 - recall_1: 0.9262 - prec_2: 0.9580 - recall_2: 0.9481 - prec_3: 0.7142 - recall_3: 0.6959 - prec_4: 0.7035 - recall_4: 0.7325 - prec_5: 0.9457 - recall_5: 0.9515 - f1_m: 0.8649 - recall_m: 0.8558 - precision_m: 0.8743 - precision: 0.8743 - recall_6: 0.8558 - f1score: 0.8649 - val_loss: 0.3228 - val_acc: 0.8589 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1600 - val_prec_2: 0.1909 - val_recall_2: 0.1752 - val_prec_3: 0.1762 - val_recall_3: 0.1268 - val_prec_4: 0.1878 - val_recall_4: 0.1421 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8570 - val_recall_m: 0.8471 - val_precision_m: 0.8674 - val_precision: 0.8674 - val_recall_6: 0.8471 - val_f1score: 0.8570\n",
      "Epoch 248/300\n",
      "epoch:  247\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 679us/step - loss: 0.3069 - acc: 0.8682 - prec: 0.9694 - recall: 0.9749 - prec_1: 0.9040 - recall_1: 0.9144 - prec_2: 0.9516 - recall_2: 0.9389 - prec_3: 0.7275 - recall_3: 0.7126 - prec_4: 0.7143 - recall_4: 0.7399 - prec_5: 0.9447 - recall_5: 0.9559 - f1_m: 0.8675 - recall_m: 0.8572 - precision_m: 0.8782 - precision: 0.8782 - recall_6: 0.8572 - f1score: 0.8675 - val_loss: 0.3241 - val_acc: 0.8551 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1597 - val_prec_2: 0.1897 - val_recall_2: 0.1759 - val_prec_3: 0.1762 - val_recall_3: 0.1176 - val_prec_4: 0.1845 - val_recall_4: 0.1456 - val_prec_5: 0.1672 - val_recall_5: 0.1579 - val_f1_m: 0.8539 - val_recall_m: 0.8436 - val_precision_m: 0.8647 - val_precision: 0.8647 - val_recall_6: 0.8436 - val_f1score: 0.8539\n",
      "Epoch 249/300\n",
      "epoch:  248\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3077 - acc: 0.8666 - prec: 0.9667 - recall: 0.9733 - prec_1: 0.8991 - recall_1: 0.9209 - prec_2: 0.9542 - recall_2: 0.9401 - prec_3: 0.7117 - recall_3: 0.7032 - prec_4: 0.7003 - recall_4: 0.7259 - prec_5: 0.9538 - recall_5: 0.9564 - f1_m: 0.8665 - recall_m: 0.8559 - precision_m: 0.8775 - precision: 0.8775 - recall_6: 0.8559 - f1score: 0.8665 - val_loss: 0.3234 - val_acc: 0.8574 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1757 - val_prec_3: 0.1762 - val_recall_3: 0.1286 - val_prec_4: 0.1886 - val_recall_4: 0.1396 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8551 - val_recall_m: 0.8453 - val_precision_m: 0.8653 - val_precision: 0.8653 - val_recall_6: 0.8453 - val_f1score: 0.8551\n",
      "Epoch 250/300\n",
      "epoch:  249\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3078 - acc: 0.8711 - prec: 0.9613 - recall: 0.9750 - prec_1: 0.9073 - recall_1: 0.9181 - prec_2: 0.9589 - recall_2: 0.9450 - prec_3: 0.7321 - recall_3: 0.7173 - prec_4: 0.7127 - recall_4: 0.7397 - prec_5: 0.9446 - recall_5: 0.9574 - f1_m: 0.8702 - recall_m: 0.8603 - precision_m: 0.8805 - precision: 0.8805 - recall_6: 0.8603 - f1score: 0.8702 - val_loss: 0.3302 - val_acc: 0.8589 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1561 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1415 - val_prec_4: 0.1916 - val_recall_4: 0.1301 - val_prec_5: 0.1672 - val_recall_5: 0.1591 - val_f1_m: 0.8571 - val_recall_m: 0.8478 - val_precision_m: 0.8668 - val_precision: 0.8668 - val_recall_6: 0.8478 - val_f1score: 0.8571\n",
      "Epoch 251/300\n",
      "epoch:  250\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 680us/step - loss: 0.3078 - acc: 0.8689 - prec: 0.9703 - recall: 0.9782 - prec_1: 0.9115 - recall_1: 0.9224 - prec_2: 0.9631 - recall_2: 0.9458 - prec_3: 0.7191 - recall_3: 0.6923 - prec_4: 0.7021 - recall_4: 0.7434 - prec_5: 0.9466 - recall_5: 0.9570 - f1_m: 0.8676 - recall_m: 0.8576 - precision_m: 0.8781 - precision: 0.8781 - recall_6: 0.8576 - f1score: 0.8676 - val_loss: 0.3245 - val_acc: 0.8581 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1579 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1201 - val_prec_4: 0.1845 - val_recall_4: 0.1466 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8563 - val_recall_m: 0.8463 - val_precision_m: 0.8668 - val_precision: 0.8668 - val_recall_6: 0.8463 - val_f1score: 0.8563\n",
      "Epoch 252/300\n",
      "epoch:  251\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3079 - acc: 0.8672 - prec: 0.9582 - recall: 0.9776 - prec_1: 0.9098 - recall_1: 0.9260 - prec_2: 0.9626 - recall_2: 0.9430 - prec_3: 0.7226 - recall_3: 0.6876 - prec_4: 0.6976 - recall_4: 0.7433 - prec_5: 0.9571 - recall_5: 0.9577 - f1_m: 0.8654 - recall_m: 0.8559 - precision_m: 0.8753 - precision: 0.8753 - recall_6: 0.8559 - f1score: 0.8654 - val_loss: 0.3262 - val_acc: 0.8534 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1765 - val_prec_3: 0.1762 - val_recall_3: 0.1096 - val_prec_4: 0.1837 - val_recall_4: 0.1512 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8523 - val_recall_m: 0.8431 - val_precision_m: 0.8619 - val_precision: 0.8619 - val_recall_6: 0.8431 - val_f1score: 0.8523\n",
      "Epoch 253/300\n",
      "epoch:  252\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 680us/step - loss: 0.3057 - acc: 0.8698 - prec: 0.9726 - recall: 0.9750 - prec_1: 0.9080 - recall_1: 0.9252 - prec_2: 0.9539 - recall_2: 0.9476 - prec_3: 0.7267 - recall_3: 0.6994 - prec_4: 0.7027 - recall_4: 0.7391 - prec_5: 0.9534 - recall_5: 0.9558 - f1_m: 0.8705 - recall_m: 0.8612 - precision_m: 0.8801 - precision: 0.8801 - recall_6: 0.8612 - f1score: 0.8705 - val_loss: 0.3258 - val_acc: 0.8511 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1582 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1107 - val_prec_4: 0.1836 - val_recall_4: 0.1479 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8496 - val_recall_m: 0.8403 - val_precision_m: 0.8592 - val_precision: 0.8592 - val_recall_6: 0.8403 - val_f1score: 0.8496\n",
      "Epoch 254/300\n",
      "epoch:  253\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3092 - acc: 0.8688 - prec: 0.9657 - recall: 0.9743 - prec_1: 0.9092 - recall_1: 0.9201 - prec_2: 0.9538 - recall_2: 0.9456 - prec_3: 0.7195 - recall_3: 0.7104 - prec_4: 0.7071 - recall_4: 0.7297 - prec_5: 0.9513 - recall_5: 0.9587 - f1_m: 0.8656 - recall_m: 0.8559 - precision_m: 0.8756 - precision: 0.8756 - recall_6: 0.8559 - f1score: 0.8656 - val_loss: 0.3243 - val_acc: 0.8524 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1592 - val_prec_2: 0.1897 - val_recall_2: 0.1765 - val_prec_3: 0.1762 - val_recall_3: 0.1148 - val_prec_4: 0.1842 - val_recall_4: 0.1454 - val_prec_5: 0.1672 - val_recall_5: 0.1576 - val_f1_m: 0.8513 - val_recall_m: 0.8416 - val_precision_m: 0.8615 - val_precision: 0.8615 - val_recall_6: 0.8416 - val_f1score: 0.8513\n",
      "Epoch 255/300\n",
      "epoch:  254\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3029 - acc: 0.8732 - prec: 0.9683 - recall: 0.9737 - prec_1: 0.9151 - recall_1: 0.9265 - prec_2: 0.9543 - recall_2: 0.9447 - prec_3: 0.7393 - recall_3: 0.7040 - prec_4: 0.7094 - recall_4: 0.7558 - prec_5: 0.9562 - recall_5: 0.9606 - f1_m: 0.8717 - recall_m: 0.8619 - precision_m: 0.8818 - precision: 0.8818 - recall_6: 0.8619 - f1score: 0.8717 - val_loss: 0.3274 - val_acc: 0.8614 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1575 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1325 - val_prec_4: 0.1916 - val_recall_4: 0.1406 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8592 - val_recall_m: 0.8491 - val_precision_m: 0.8699 - val_precision: 0.8699 - val_recall_6: 0.8491 - val_f1score: 0.8592\n",
      "Epoch 256/300\n",
      "epoch:  255\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3061 - acc: 0.8669 - prec: 0.9658 - recall: 0.9748 - prec_1: 0.9021 - recall_1: 0.9239 - prec_2: 0.9577 - recall_2: 0.9377 - prec_3: 0.7263 - recall_3: 0.6979 - prec_4: 0.7031 - recall_4: 0.7393 - prec_5: 0.9481 - recall_5: 0.9545 - f1_m: 0.8651 - recall_m: 0.8543 - precision_m: 0.8763 - precision: 0.8763 - recall_6: 0.8543 - f1score: 0.8651 - val_loss: 0.3244 - val_acc: 0.8614 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1577 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1353 - val_prec_4: 0.1899 - val_recall_4: 0.1369 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8599 - val_recall_m: 0.8491 - val_precision_m: 0.8713 - val_precision: 0.8713 - val_recall_6: 0.8491 - val_f1score: 0.8599\n",
      "Epoch 257/300\n",
      "epoch:  256\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3100 - acc: 0.8689 - prec: 0.9701 - recall: 0.9754 - prec_1: 0.9165 - recall_1: 0.9219 - prec_2: 0.9510 - recall_2: 0.9475 - prec_3: 0.7170 - recall_3: 0.7058 - prec_4: 0.7039 - recall_4: 0.7257 - prec_5: 0.9515 - recall_5: 0.9586 - f1_m: 0.8666 - recall_m: 0.8567 - precision_m: 0.8768 - precision: 0.8768 - recall_6: 0.8567 - f1score: 0.8666 - val_loss: 0.3283 - val_acc: 0.8666 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1575 - val_prec_2: 0.1897 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1422 - val_prec_4: 0.1916 - val_recall_4: 0.1354 - val_prec_5: 0.1672 - val_recall_5: 0.1589 - val_f1_m: 0.8644 - val_recall_m: 0.8544 - val_precision_m: 0.8749 - val_precision: 0.8749 - val_recall_6: 0.8544 - val_f1score: 0.8644\n",
      "Epoch 258/300\n",
      "epoch:  257\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3087 - acc: 0.8666 - prec: 0.9667 - recall: 0.9744 - prec_1: 0.9003 - recall_1: 0.9265 - prec_2: 0.9588 - recall_2: 0.9352 - prec_3: 0.7153 - recall_3: 0.7018 - prec_4: 0.6980 - recall_4: 0.7239 - prec_5: 0.9542 - recall_5: 0.9560 - f1_m: 0.8654 - recall_m: 0.8554 - precision_m: 0.8758 - precision: 0.8758 - recall_6: 0.8554 - f1score: 0.8654 - val_loss: 0.3234 - val_acc: 0.8649 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1579 - val_prec_2: 0.1897 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1372 - val_prec_4: 0.1890 - val_recall_4: 0.1379 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8624 - val_recall_m: 0.8524 - val_precision_m: 0.8730 - val_precision: 0.8730 - val_recall_6: 0.8524 - val_f1score: 0.8624\n",
      "Epoch 259/300\n",
      "epoch:  258\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.3072 - acc: 0.8653 - prec: 0.9594 - recall: 0.9783 - prec_1: 0.9153 - recall_1: 0.9169 - prec_2: 0.9597 - recall_2: 0.9457 - prec_3: 0.7130 - recall_3: 0.6900 - prec_4: 0.6955 - recall_4: 0.7298 - prec_5: 0.9526 - recall_5: 0.9574 - f1_m: 0.8651 - recall_m: 0.8546 - precision_m: 0.8761 - precision: 0.8761 - recall_6: 0.8546 - f1score: 0.8651 - val_loss: 0.3241 - val_acc: 0.8604 - val_prec: 0.1762 - val_recall: 0.1714 - val_prec_1: 0.1758 - val_recall_1: 0.1582 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1235 - val_prec_4: 0.1858 - val_recall_4: 0.1461 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8586 - val_recall_m: 0.8478 - val_precision_m: 0.8698 - val_precision: 0.8698 - val_recall_6: 0.8478 - val_f1score: 0.8586\n",
      "Epoch 260/300\n",
      "epoch:  259\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3072 - acc: 0.8637 - prec: 0.9684 - recall: 0.9738 - prec_1: 0.9137 - recall_1: 0.9235 - prec_2: 0.9605 - recall_2: 0.9496 - prec_3: 0.6991 - recall_3: 0.6747 - prec_4: 0.6832 - recall_4: 0.7228 - prec_5: 0.9461 - recall_5: 0.9552 - f1_m: 0.8622 - recall_m: 0.8522 - precision_m: 0.8726 - precision: 0.8726 - recall_6: 0.8522 - f1score: 0.8622 - val_loss: 0.3233 - val_acc: 0.8604 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1246 - val_prec_4: 0.1856 - val_recall_4: 0.1446 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8595 - val_recall_m: 0.8491 - val_precision_m: 0.8704 - val_precision: 0.8704 - val_recall_6: 0.8491 - val_f1score: 0.8595\n",
      "Epoch 261/300\n",
      "epoch:  260\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3085 - acc: 0.8669 - prec: 0.9632 - recall: 0.9731 - prec_1: 0.8980 - recall_1: 0.9179 - prec_2: 0.9608 - recall_2: 0.9418 - prec_3: 0.7225 - recall_3: 0.6929 - prec_4: 0.6962 - recall_4: 0.7354 - prec_5: 0.9419 - recall_5: 0.9549 - f1_m: 0.8654 - recall_m: 0.8559 - precision_m: 0.8753 - precision: 0.8753 - recall_6: 0.8559 - f1score: 0.8654 - val_loss: 0.3237 - val_acc: 0.8624 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1577 - val_prec_2: 0.1897 - val_recall_2: 0.1765 - val_prec_3: 0.1762 - val_recall_3: 0.1297 - val_prec_4: 0.1882 - val_recall_4: 0.1431 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8605 - val_recall_m: 0.8504 - val_precision_m: 0.8711 - val_precision: 0.8711 - val_recall_6: 0.8504 - val_f1score: 0.8605\n",
      "Epoch 262/300\n",
      "epoch:  261\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3077 - acc: 0.8656 - prec: 0.9659 - recall: 0.9742 - prec_1: 0.9036 - recall_1: 0.9163 - prec_2: 0.9597 - recall_2: 0.9429 - prec_3: 0.7161 - recall_3: 0.7034 - prec_4: 0.7002 - recall_4: 0.7323 - prec_5: 0.9399 - recall_5: 0.9560 - f1_m: 0.8636 - recall_m: 0.8537 - precision_m: 0.8740 - precision: 0.8740 - recall_6: 0.8537 - f1score: 0.8636 - val_loss: 0.3281 - val_acc: 0.8561 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1598 - val_prec_2: 0.1897 - val_recall_2: 0.1752 - val_prec_3: 0.1762 - val_recall_3: 0.1430 - val_prec_4: 0.1916 - val_recall_4: 0.1239 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8534 - val_recall_m: 0.8436 - val_precision_m: 0.8638 - val_precision: 0.8638 - val_recall_6: 0.8436 - val_f1score: 0.8534\n",
      "Epoch 263/300\n",
      "epoch:  262\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3066 - acc: 0.8684 - prec: 0.9716 - recall: 0.9737 - prec_1: 0.9035 - recall_1: 0.9218 - prec_2: 0.9599 - recall_2: 0.9437 - prec_3: 0.7258 - recall_3: 0.7120 - prec_4: 0.7081 - recall_4: 0.7350 - prec_5: 0.9443 - recall_5: 0.9566 - f1_m: 0.8674 - recall_m: 0.8573 - precision_m: 0.8779 - precision: 0.8779 - recall_6: 0.8573 - f1score: 0.8674 - val_loss: 0.3244 - val_acc: 0.8604 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1595 - val_prec_2: 0.1897 - val_recall_2: 0.1757 - val_prec_3: 0.1762 - val_recall_3: 0.1397 - val_prec_4: 0.1890 - val_recall_4: 0.1306 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8584 - val_recall_m: 0.8488 - val_precision_m: 0.8683 - val_precision: 0.8683 - val_recall_6: 0.8488 - val_f1score: 0.8584\n",
      "Epoch 264/300\n",
      "epoch:  263\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3062 - acc: 0.8681 - prec: 0.9705 - recall: 0.9770 - prec_1: 0.8996 - recall_1: 0.9169 - prec_2: 0.9452 - recall_2: 0.9393 - prec_3: 0.7306 - recall_3: 0.7184 - prec_4: 0.7130 - recall_4: 0.7369 - prec_5: 0.9512 - recall_5: 0.9547 - f1_m: 0.8672 - recall_m: 0.8569 - precision_m: 0.8779 - precision: 0.8779 - recall_6: 0.8569 - f1score: 0.8672 - val_loss: 0.3232 - val_acc: 0.8539 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1757 - val_prec_3: 0.1762 - val_recall_3: 0.1219 - val_prec_4: 0.1860 - val_recall_4: 0.1411 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8524 - val_recall_m: 0.8428 - val_precision_m: 0.8623 - val_precision: 0.8623 - val_recall_6: 0.8428 - val_f1score: 0.8524\n",
      "Epoch 265/300\n",
      "epoch:  264\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3103 - acc: 0.8663 - prec: 0.9637 - recall: 0.9787 - prec_1: 0.9060 - recall_1: 0.9264 - prec_2: 0.9592 - recall_2: 0.9457 - prec_3: 0.7125 - recall_3: 0.6947 - prec_4: 0.6986 - recall_4: 0.7266 - prec_5: 0.9605 - recall_5: 0.9590 - f1_m: 0.8641 - recall_m: 0.8544 - precision_m: 0.8742 - precision: 0.8742 - recall_6: 0.8544 - f1score: 0.8641 - val_loss: 0.3239 - val_acc: 0.8571 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1582 - val_prec_2: 0.1897 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1156 - val_prec_4: 0.1845 - val_recall_4: 0.1494 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8568 - val_recall_m: 0.8468 - val_precision_m: 0.8671 - val_precision: 0.8671 - val_recall_6: 0.8468 - val_f1score: 0.8568\n",
      "Epoch 266/300\n",
      "epoch:  265\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 670us/step - loss: 0.3134 - acc: 0.8628 - prec: 0.9666 - recall: 0.9744 - prec_1: 0.9079 - recall_1: 0.9098 - prec_2: 0.9587 - recall_2: 0.9481 - prec_3: 0.7118 - recall_3: 0.6894 - prec_4: 0.6902 - recall_4: 0.7272 - prec_5: 0.9372 - recall_5: 0.9568 - f1_m: 0.8631 - recall_m: 0.8542 - precision_m: 0.8724 - precision: 0.8724 - recall_6: 0.8542 - f1score: 0.8631 - val_loss: 0.3244 - val_acc: 0.8604 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1579 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1137 - val_prec_4: 0.1844 - val_recall_4: 0.1559 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8597 - val_recall_m: 0.8488 - val_precision_m: 0.8712 - val_precision: 0.8712 - val_recall_6: 0.8488 - val_f1score: 0.8597\n",
      "Epoch 267/300\n",
      "epoch:  266\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3046 - acc: 0.8683 - prec: 0.9663 - recall: 0.9753 - prec_1: 0.9104 - recall_1: 0.9175 - prec_2: 0.9577 - recall_2: 0.9446 - prec_3: 0.7274 - recall_3: 0.6996 - prec_4: 0.7019 - recall_4: 0.7379 - prec_5: 0.9438 - recall_5: 0.9589 - f1_m: 0.8681 - recall_m: 0.8582 - precision_m: 0.8784 - precision: 0.8784 - recall_6: 0.8582 - f1score: 0.8681 - val_loss: 0.3224 - val_acc: 0.8621 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1582 - val_prec_2: 0.1897 - val_recall_2: 0.1775 - val_prec_3: 0.1762 - val_recall_3: 0.1195 - val_prec_4: 0.1858 - val_recall_4: 0.1514 - val_prec_5: 0.1672 - val_recall_5: 0.1579 - val_f1_m: 0.8599 - val_recall_m: 0.8491 - val_precision_m: 0.8711 - val_precision: 0.8711 - val_recall_6: 0.8491 - val_f1score: 0.8599\n",
      "Epoch 268/300\n",
      "epoch:  267\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3069 - acc: 0.8686 - prec: 0.9710 - recall: 0.9689 - prec_1: 0.9085 - recall_1: 0.9244 - prec_2: 0.9585 - recall_2: 0.9426 - prec_3: 0.7188 - recall_3: 0.7114 - prec_4: 0.7050 - recall_4: 0.7271 - prec_5: 0.9523 - recall_5: 0.9586 - f1_m: 0.8674 - recall_m: 0.8581 - precision_m: 0.8770 - precision: 0.8770 - recall_6: 0.8581 - f1score: 0.8674 - val_loss: 0.3258 - val_acc: 0.8611 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1577 - val_prec_2: 0.1897 - val_recall_2: 0.1765 - val_prec_3: 0.1762 - val_recall_3: 0.1279 - val_prec_4: 0.1894 - val_recall_4: 0.1441 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8603 - val_recall_m: 0.8501 - val_precision_m: 0.8710 - val_precision: 0.8710 - val_recall_6: 0.8501 - val_f1score: 0.8603\n",
      "Epoch 269/300\n",
      "epoch:  268\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3062 - acc: 0.8654 - prec: 0.9689 - recall: 0.9757 - prec_1: 0.9102 - recall_1: 0.9218 - prec_2: 0.9548 - recall_2: 0.9452 - prec_3: 0.7155 - recall_3: 0.6961 - prec_4: 0.7005 - recall_4: 0.7309 - prec_5: 0.9513 - recall_5: 0.9564 - f1_m: 0.8647 - recall_m: 0.8543 - precision_m: 0.8756 - precision: 0.8756 - recall_6: 0.8543 - f1score: 0.8647 - val_loss: 0.3220 - val_acc: 0.8619 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1585 - val_prec_2: 0.1897 - val_recall_2: 0.1757 - val_prec_3: 0.1762 - val_recall_3: 0.1301 - val_prec_4: 0.1886 - val_recall_4: 0.1424 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8585 - val_recall_m: 0.8478 - val_precision_m: 0.8697 - val_precision: 0.8697 - val_recall_6: 0.8478 - val_f1score: 0.8585\n",
      "Epoch 270/300\n",
      "epoch:  269\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3101 - acc: 0.8689 - prec: 0.9624 - recall: 0.9746 - prec_1: 0.9106 - recall_1: 0.9247 - prec_2: 0.9533 - recall_2: 0.9449 - prec_3: 0.7293 - recall_3: 0.7089 - prec_4: 0.7064 - recall_4: 0.7419 - prec_5: 0.9430 - recall_5: 0.9575 - f1_m: 0.8662 - recall_m: 0.8566 - precision_m: 0.8763 - precision: 0.8763 - recall_6: 0.8566 - f1score: 0.8662 - val_loss: 0.3228 - val_acc: 0.8586 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1752 - val_prec_3: 0.1762 - val_recall_3: 0.1271 - val_prec_4: 0.1886 - val_recall_4: 0.1421 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8565 - val_recall_m: 0.8471 - val_precision_m: 0.8664 - val_precision: 0.8664 - val_recall_6: 0.8471 - val_f1score: 0.8565\n",
      "Epoch 271/300\n",
      "epoch:  270\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 679us/step - loss: 0.3081 - acc: 0.8656 - prec: 0.9683 - recall: 0.9782 - prec_1: 0.9055 - recall_1: 0.9259 - prec_2: 0.9564 - recall_2: 0.9403 - prec_3: 0.7168 - recall_3: 0.6872 - prec_4: 0.6944 - recall_4: 0.7333 - prec_5: 0.9536 - recall_5: 0.9520 - f1_m: 0.8645 - recall_m: 0.8536 - precision_m: 0.8759 - precision: 0.8759 - recall_6: 0.8536 - f1score: 0.8645 - val_loss: 0.3234 - val_acc: 0.8549 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1600 - val_prec_2: 0.1897 - val_recall_2: 0.1759 - val_prec_3: 0.1762 - val_recall_3: 0.1112 - val_prec_4: 0.1844 - val_recall_4: 0.1519 - val_prec_5: 0.1672 - val_recall_5: 0.1571 - val_f1_m: 0.8538 - val_recall_m: 0.8441 - val_precision_m: 0.8639 - val_precision: 0.8639 - val_recall_6: 0.8441 - val_f1score: 0.8538\n",
      "Epoch 272/300\n",
      "epoch:  271\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3053 - acc: 0.8697 - prec: 0.9634 - recall: 0.9672 - prec_1: 0.9145 - recall_1: 0.9256 - prec_2: 0.9529 - recall_2: 0.9527 - prec_3: 0.7290 - recall_3: 0.7056 - prec_4: 0.7119 - recall_4: 0.7463 - prec_5: 0.9508 - recall_5: 0.9557 - f1_m: 0.8681 - recall_m: 0.8582 - precision_m: 0.8783 - precision: 0.8783 - recall_6: 0.8582 - f1score: 0.8681 - val_loss: 0.3228 - val_acc: 0.8604 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1577 - val_prec_2: 0.1897 - val_recall_2: 0.1765 - val_prec_3: 0.1762 - val_recall_3: 0.1362 - val_prec_4: 0.1890 - val_recall_4: 0.1354 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8593 - val_recall_m: 0.8488 - val_precision_m: 0.8704 - val_precision: 0.8704 - val_recall_6: 0.8488 - val_f1score: 0.8593\n",
      "Epoch 273/300\n",
      "epoch:  272\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3101 - acc: 0.8667 - prec: 0.9718 - recall: 0.9728 - prec_1: 0.8906 - recall_1: 0.9243 - prec_2: 0.9568 - recall_2: 0.9475 - prec_3: 0.7305 - recall_3: 0.6910 - prec_4: 0.6950 - recall_4: 0.7377 - prec_5: 0.9471 - recall_5: 0.9494 - f1_m: 0.8657 - recall_m: 0.8558 - precision_m: 0.8759 - precision: 0.8759 - recall_6: 0.8558 - f1score: 0.8657 - val_loss: 0.3230 - val_acc: 0.8514 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1592 - val_prec_2: 0.1897 - val_recall_2: 0.1759 - val_prec_3: 0.1762 - val_recall_3: 0.1168 - val_prec_4: 0.1863 - val_recall_4: 0.1436 - val_prec_5: 0.1672 - val_recall_5: 0.1579 - val_f1_m: 0.8496 - val_recall_m: 0.8401 - val_precision_m: 0.8596 - val_precision: 0.8596 - val_recall_6: 0.8401 - val_f1score: 0.8496\n",
      "Epoch 274/300\n",
      "epoch:  273\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 663us/step - loss: 0.3103 - acc: 0.8667 - prec: 0.9655 - recall: 0.9735 - prec_1: 0.9090 - recall_1: 0.9248 - prec_2: 0.9511 - recall_2: 0.9422 - prec_3: 0.7154 - recall_3: 0.6884 - prec_4: 0.6999 - recall_4: 0.7395 - prec_5: 0.9525 - recall_5: 0.9583 - f1_m: 0.8651 - recall_m: 0.8557 - precision_m: 0.8748 - precision: 0.8748 - recall_6: 0.8557 - f1score: 0.8651 - val_loss: 0.3237 - val_acc: 0.8591 - val_prec: 0.1762 - val_recall: 0.1712 - val_prec_1: 0.1758 - val_recall_1: 0.1577 - val_prec_2: 0.1897 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1275 - val_prec_4: 0.1878 - val_recall_4: 0.1419 - val_prec_5: 0.1672 - val_recall_5: 0.1589 - val_f1_m: 0.8581 - val_recall_m: 0.8483 - val_precision_m: 0.8683 - val_precision: 0.8683 - val_recall_6: 0.8483 - val_f1score: 0.8581\n",
      "Epoch 275/300\n",
      "epoch:  274\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3123 - acc: 0.8642 - prec: 0.9654 - recall: 0.9732 - prec_1: 0.8991 - recall_1: 0.9170 - prec_2: 0.9552 - recall_2: 0.9416 - prec_3: 0.7172 - recall_3: 0.6876 - prec_4: 0.6938 - recall_4: 0.7393 - prec_5: 0.9384 - recall_5: 0.9542 - f1_m: 0.8630 - recall_m: 0.8533 - precision_m: 0.8731 - precision: 0.8731 - recall_6: 0.8533 - f1score: 0.8630 - val_loss: 0.3219 - val_acc: 0.8581 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1580 - val_prec_2: 0.1897 - val_recall_2: 0.1759 - val_prec_3: 0.1762 - val_recall_3: 0.1231 - val_prec_4: 0.1875 - val_recall_4: 0.1461 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8583 - val_recall_m: 0.8488 - val_precision_m: 0.8681 - val_precision: 0.8681 - val_recall_6: 0.8488 - val_f1score: 0.8583\n",
      "Epoch 276/300\n",
      "epoch:  275\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 684us/step - loss: 0.3128 - acc: 0.8637 - prec: 0.9681 - recall: 0.9783 - prec_1: 0.9048 - recall_1: 0.9165 - prec_2: 0.9556 - recall_2: 0.9410 - prec_3: 0.7132 - recall_3: 0.6882 - prec_4: 0.6951 - recall_4: 0.7294 - prec_5: 0.9495 - recall_5: 0.9549 - f1_m: 0.8631 - recall_m: 0.8534 - precision_m: 0.8732 - precision: 0.8732 - recall_6: 0.8534 - f1score: 0.8631 - val_loss: 0.3255 - val_acc: 0.8566 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1144 - val_prec_4: 0.1840 - val_recall_4: 0.1502 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8549 - val_recall_m: 0.8453 - val_precision_m: 0.8649 - val_precision: 0.8649 - val_recall_6: 0.8453 - val_f1score: 0.8549\n",
      "Epoch 277/300\n",
      "epoch:  276\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 675us/step - loss: 0.3087 - acc: 0.8694 - prec: 0.9671 - recall: 0.9783 - prec_1: 0.9052 - recall_1: 0.9222 - prec_2: 0.9599 - recall_2: 0.9454 - prec_3: 0.7246 - recall_3: 0.7026 - prec_4: 0.7082 - recall_4: 0.7406 - prec_5: 0.9432 - recall_5: 0.9536 - f1_m: 0.8678 - recall_m: 0.8581 - precision_m: 0.8779 - precision: 0.8779 - recall_6: 0.8581 - f1score: 0.8678 - val_loss: 0.3265 - val_acc: 0.8611 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1570 - val_prec_2: 0.1897 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1286 - val_prec_4: 0.1886 - val_recall_4: 0.1436 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8604 - val_recall_m: 0.8506 - val_precision_m: 0.8707 - val_precision: 0.8707 - val_recall_6: 0.8506 - val_f1score: 0.8604\n",
      "Epoch 278/300\n",
      "epoch:  277\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3083 - acc: 0.8637 - prec: 0.9669 - recall: 0.9730 - prec_1: 0.9046 - recall_1: 0.9205 - prec_2: 0.9516 - recall_2: 0.9383 - prec_3: 0.7158 - recall_3: 0.6924 - prec_4: 0.6920 - recall_4: 0.7282 - prec_5: 0.9516 - recall_5: 0.9586 - f1_m: 0.8639 - recall_m: 0.8529 - precision_m: 0.8752 - precision: 0.8752 - recall_6: 0.8529 - f1score: 0.8639 - val_loss: 0.3278 - val_acc: 0.8546 - val_prec: 0.1762 - val_recall: 0.1712 - val_prec_1: 0.1758 - val_recall_1: 0.1580 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1121 - val_prec_4: 0.1837 - val_recall_4: 0.1509 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8547 - val_recall_m: 0.8441 - val_precision_m: 0.8657 - val_precision: 0.8657 - val_recall_6: 0.8441 - val_f1score: 0.8547\n",
      "Epoch 279/300\n",
      "epoch:  278\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3055 - acc: 0.8664 - prec: 0.9669 - recall: 0.9737 - prec_1: 0.9022 - recall_1: 0.9169 - prec_2: 0.9581 - recall_2: 0.9462 - prec_3: 0.7271 - recall_3: 0.6958 - prec_4: 0.7021 - recall_4: 0.7437 - prec_5: 0.9513 - recall_5: 0.9567 - f1_m: 0.8663 - recall_m: 0.8571 - precision_m: 0.8758 - precision: 0.8758 - recall_6: 0.8571 - f1score: 0.8663 - val_loss: 0.3216 - val_acc: 0.8596 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1602 - val_prec_2: 0.1897 - val_recall_2: 0.1757 - val_prec_3: 0.1762 - val_recall_3: 0.1250 - val_prec_4: 0.1872 - val_recall_4: 0.1446 - val_prec_5: 0.1672 - val_recall_5: 0.1571 - val_f1_m: 0.8585 - val_recall_m: 0.8486 - val_precision_m: 0.8688 - val_precision: 0.8688 - val_recall_6: 0.8486 - val_f1score: 0.8585\n",
      "Epoch 280/300\n",
      "epoch:  279\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3086 - acc: 0.8654 - prec: 0.9636 - recall: 0.9737 - prec_1: 0.9060 - recall_1: 0.9194 - prec_2: 0.9565 - recall_2: 0.9510 - prec_3: 0.7185 - recall_3: 0.6815 - prec_4: 0.6911 - recall_4: 0.7369 - prec_5: 0.9483 - recall_5: 0.9528 - f1_m: 0.8636 - recall_m: 0.8531 - precision_m: 0.8746 - precision: 0.8746 - recall_6: 0.8531 - f1score: 0.8636 - val_loss: 0.3272 - val_acc: 0.8641 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1585 - val_prec_2: 0.1897 - val_recall_2: 0.1754 - val_prec_3: 0.1762 - val_recall_3: 0.1417 - val_prec_4: 0.1916 - val_recall_4: 0.1346 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8629 - val_recall_m: 0.8521 - val_precision_m: 0.8742 - val_precision: 0.8742 - val_recall_6: 0.8521 - val_f1score: 0.8629\n",
      "Epoch 281/300\n",
      "epoch:  280\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3108 - acc: 0.8659 - prec: 0.9726 - recall: 0.9786 - prec_1: 0.8977 - recall_1: 0.9125 - prec_2: 0.9484 - recall_2: 0.9423 - prec_3: 0.7200 - recall_3: 0.7040 - prec_4: 0.7028 - recall_4: 0.7352 - prec_5: 0.9437 - recall_5: 0.9533 - f1_m: 0.8641 - recall_m: 0.8538 - precision_m: 0.8747 - precision: 0.8747 - recall_6: 0.8538 - f1score: 0.8641 - val_loss: 0.3233 - val_acc: 0.8629 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1575 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1296 - val_prec_4: 0.1886 - val_recall_4: 0.1441 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8626 - val_recall_m: 0.8524 - val_precision_m: 0.8733 - val_precision: 0.8733 - val_recall_6: 0.8524 - val_f1score: 0.8626\n",
      "Epoch 282/300\n",
      "epoch:  281\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 6s 693us/step - loss: 0.3100 - acc: 0.8647 - prec: 0.9645 - recall: 0.9710 - prec_1: 0.8943 - recall_1: 0.9235 - prec_2: 0.9568 - recall_2: 0.9495 - prec_3: 0.7110 - recall_3: 0.6998 - prec_4: 0.6976 - recall_4: 0.7236 - prec_5: 0.9463 - recall_5: 0.9482 - f1_m: 0.8653 - recall_m: 0.8553 - precision_m: 0.8757 - precision: 0.8757 - recall_6: 0.8553 - f1score: 0.8653 - val_loss: 0.3218 - val_acc: 0.8634 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1584 - val_prec_2: 0.1897 - val_recall_2: 0.1759 - val_prec_3: 0.1762 - val_recall_3: 0.1306 - val_prec_4: 0.1886 - val_recall_4: 0.1439 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8628 - val_recall_m: 0.8534 - val_precision_m: 0.8726 - val_precision: 0.8726 - val_recall_6: 0.8534 - val_f1score: 0.8628\n",
      "Epoch 283/300\n",
      "epoch:  282\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3097 - acc: 0.8637 - prec: 0.9689 - recall: 0.9779 - prec_1: 0.9056 - recall_1: 0.9259 - prec_2: 0.9548 - recall_2: 0.9432 - prec_3: 0.6972 - recall_3: 0.6885 - prec_4: 0.6897 - recall_4: 0.7150 - prec_5: 0.9475 - recall_5: 0.9536 - f1_m: 0.8626 - recall_m: 0.8529 - precision_m: 0.8728 - precision: 0.8728 - recall_6: 0.8529 - f1score: 0.8626 - val_loss: 0.3217 - val_acc: 0.8631 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1584 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1248 - val_prec_4: 0.1886 - val_recall_4: 0.1491 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8610 - val_recall_m: 0.8514 - val_precision_m: 0.8711 - val_precision: 0.8711 - val_recall_6: 0.8514 - val_f1score: 0.8610\n",
      "Epoch 284/300\n",
      "epoch:  283\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 677us/step - loss: 0.3085 - acc: 0.8677 - prec: 0.9668 - recall: 0.9784 - prec_1: 0.9044 - recall_1: 0.9251 - prec_2: 0.9513 - recall_2: 0.9356 - prec_3: 0.7253 - recall_3: 0.6922 - prec_4: 0.6992 - recall_4: 0.7474 - prec_5: 0.9575 - recall_5: 0.9597 - f1_m: 0.8660 - recall_m: 0.8563 - precision_m: 0.8761 - precision: 0.8761 - recall_6: 0.8563 - f1score: 0.8660 - val_loss: 0.3219 - val_acc: 0.8576 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1590 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1255 - val_prec_4: 0.1872 - val_recall_4: 0.1414 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8563 - val_recall_m: 0.8466 - val_precision_m: 0.8665 - val_precision: 0.8665 - val_recall_6: 0.8466 - val_f1score: 0.8563\n",
      "Epoch 285/300\n",
      "epoch:  284\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3093 - acc: 0.8669 - prec: 0.9669 - recall: 0.9772 - prec_1: 0.9045 - recall_1: 0.9142 - prec_2: 0.9521 - recall_2: 0.9506 - prec_3: 0.7279 - recall_3: 0.6934 - prec_4: 0.7040 - recall_4: 0.7506 - prec_5: 0.9465 - recall_5: 0.9529 - f1_m: 0.8651 - recall_m: 0.8539 - precision_m: 0.8767 - precision: 0.8767 - recall_6: 0.8539 - f1score: 0.8651 - val_loss: 0.3251 - val_acc: 0.8466 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1759 - val_recall_1: 0.1611 - val_prec_2: 0.1909 - val_recall_2: 0.1744 - val_prec_3: 0.1762 - val_recall_3: 0.1157 - val_prec_4: 0.1853 - val_recall_4: 0.1394 - val_prec_5: 0.1672 - val_recall_5: 0.1579 - val_f1_m: 0.8453 - val_recall_m: 0.8361 - val_precision_m: 0.8550 - val_precision: 0.8550 - val_recall_6: 0.8361 - val_f1score: 0.8453\n",
      "Epoch 286/300\n",
      "epoch:  285\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 676us/step - loss: 0.3083 - acc: 0.8686 - prec: 0.9652 - recall: 0.9761 - prec_1: 0.9073 - recall_1: 0.9219 - prec_2: 0.9458 - recall_2: 0.9485 - prec_3: 0.7188 - recall_3: 0.7098 - prec_4: 0.7086 - recall_4: 0.7314 - prec_5: 0.9554 - recall_5: 0.9571 - f1_m: 0.8668 - recall_m: 0.8571 - precision_m: 0.8769 - precision: 0.8769 - recall_6: 0.8571 - f1score: 0.8668 - val_loss: 0.3225 - val_acc: 0.8591 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1582 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1290 - val_prec_4: 0.1872 - val_recall_4: 0.1399 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8569 - val_recall_m: 0.8466 - val_precision_m: 0.8676 - val_precision: 0.8676 - val_recall_6: 0.8466 - val_f1score: 0.8569\n",
      "Epoch 287/300\n",
      "epoch:  286\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 682us/step - loss: 0.3081 - acc: 0.8694 - prec: 0.9659 - recall: 0.9750 - prec_1: 0.9104 - recall_1: 0.9219 - prec_2: 0.9578 - recall_2: 0.9411 - prec_3: 0.7287 - recall_3: 0.6954 - prec_4: 0.6981 - recall_4: 0.7441 - prec_5: 0.9499 - recall_5: 0.9625 - f1_m: 0.8663 - recall_m: 0.8566 - precision_m: 0.8764 - precision: 0.8764 - recall_6: 0.8566 - f1score: 0.8663 - val_loss: 0.3249 - val_acc: 0.8644 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1575 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1412 - val_prec_4: 0.1904 - val_recall_4: 0.1346 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8635 - val_recall_m: 0.8539 - val_precision_m: 0.8735 - val_precision: 0.8735 - val_recall_6: 0.8539 - val_f1score: 0.8635\n",
      "Epoch 288/300\n",
      "epoch:  287\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3114 - acc: 0.8661 - prec: 0.9590 - recall: 0.9748 - prec_1: 0.9009 - recall_1: 0.9187 - prec_2: 0.9562 - recall_2: 0.9443 - prec_3: 0.7185 - recall_3: 0.7034 - prec_4: 0.7003 - recall_4: 0.7306 - prec_5: 0.9459 - recall_5: 0.9546 - f1_m: 0.8652 - recall_m: 0.8549 - precision_m: 0.8759 - precision: 0.8759 - recall_6: 0.8549 - f1score: 0.8652 - val_loss: 0.3249 - val_acc: 0.8604 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1577 - val_prec_2: 0.1897 - val_recall_2: 0.1759 - val_prec_3: 0.1762 - val_recall_3: 0.1409 - val_prec_4: 0.1904 - val_recall_4: 0.1309 - val_prec_5: 0.1672 - val_recall_5: 0.1589 - val_f1_m: 0.8583 - val_recall_m: 0.8486 - val_precision_m: 0.8685 - val_precision: 0.8685 - val_recall_6: 0.8486 - val_f1score: 0.8583\n",
      "Epoch 289/300\n",
      "epoch:  288\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 668us/step - loss: 0.3108 - acc: 0.8666 - prec: 0.9660 - recall: 0.9766 - prec_1: 0.9053 - recall_1: 0.9276 - prec_2: 0.9568 - recall_2: 0.9447 - prec_3: 0.7146 - recall_3: 0.7088 - prec_4: 0.7050 - recall_4: 0.7146 - prec_5: 0.9500 - recall_5: 0.9520 - f1_m: 0.8660 - recall_m: 0.8568 - precision_m: 0.8755 - precision: 0.8755 - recall_6: 0.8568 - f1score: 0.8660 - val_loss: 0.3234 - val_acc: 0.8569 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1585 - val_prec_2: 0.1897 - val_recall_2: 0.1754 - val_prec_3: 0.1762 - val_recall_3: 0.1252 - val_prec_4: 0.1872 - val_recall_4: 0.1416 - val_prec_5: 0.1672 - val_recall_5: 0.1589 - val_f1_m: 0.8553 - val_recall_m: 0.8453 - val_precision_m: 0.8658 - val_precision: 0.8658 - val_recall_6: 0.8453 - val_f1score: 0.8553\n",
      "Epoch 290/300\n",
      "epoch:  289\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3092 - acc: 0.8657 - prec: 0.9641 - recall: 0.9723 - prec_1: 0.9178 - recall_1: 0.9281 - prec_2: 0.9566 - recall_2: 0.9464 - prec_3: 0.7156 - recall_3: 0.6871 - prec_4: 0.6884 - recall_4: 0.7313 - prec_5: 0.9521 - recall_5: 0.9576 - f1_m: 0.8643 - recall_m: 0.8548 - precision_m: 0.8741 - precision: 0.8741 - recall_6: 0.8548 - f1score: 0.8643 - val_loss: 0.3249 - val_acc: 0.8569 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1592 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1143 - val_prec_4: 0.1845 - val_recall_4: 0.1507 - val_prec_5: 0.1672 - val_recall_5: 0.1579 - val_f1_m: 0.8555 - val_recall_m: 0.8448 - val_precision_m: 0.8667 - val_precision: 0.8667 - val_recall_6: 0.8448 - val_f1score: 0.8555\n",
      "Epoch 291/300\n",
      "epoch:  290\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3059 - acc: 0.8666 - prec: 0.9670 - recall: 0.9727 - prec_1: 0.9037 - recall_1: 0.9223 - prec_2: 0.9543 - recall_2: 0.9376 - prec_3: 0.7196 - recall_3: 0.6970 - prec_4: 0.6967 - recall_4: 0.7277 - prec_5: 0.9491 - recall_5: 0.9589 - f1_m: 0.8661 - recall_m: 0.8561 - precision_m: 0.8765 - precision: 0.8765 - recall_6: 0.8561 - f1score: 0.8661 - val_loss: 0.3228 - val_acc: 0.8626 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1594 - val_prec_2: 0.1897 - val_recall_2: 0.1757 - val_prec_3: 0.1762 - val_recall_3: 0.1323 - val_prec_4: 0.1886 - val_recall_4: 0.1404 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8602 - val_recall_m: 0.8504 - val_precision_m: 0.8706 - val_precision: 0.8706 - val_recall_6: 0.8504 - val_f1score: 0.8602\n",
      "Epoch 292/300\n",
      "epoch:  291\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 669us/step - loss: 0.3094 - acc: 0.8666 - prec: 0.9723 - recall: 0.9788 - prec_1: 0.8997 - recall_1: 0.9204 - prec_2: 0.9501 - recall_2: 0.9381 - prec_3: 0.7278 - recall_3: 0.7097 - prec_4: 0.7069 - recall_4: 0.7446 - prec_5: 0.9502 - recall_5: 0.9533 - f1_m: 0.8660 - recall_m: 0.8554 - precision_m: 0.8771 - precision: 0.8771 - recall_6: 0.8554 - f1score: 0.8660 - val_loss: 0.3221 - val_acc: 0.8579 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1221 - val_prec_4: 0.1875 - val_recall_4: 0.1451 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8560 - val_recall_m: 0.8458 - val_precision_m: 0.8666 - val_precision: 0.8666 - val_recall_6: 0.8458 - val_f1score: 0.8560\n",
      "Epoch 293/300\n",
      "epoch:  292\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 687us/step - loss: 0.3133 - acc: 0.8657 - prec: 0.9685 - recall: 0.9743 - prec_1: 0.8861 - recall_1: 0.9200 - prec_2: 0.9589 - recall_2: 0.9388 - prec_3: 0.7302 - recall_3: 0.6996 - prec_4: 0.6993 - recall_4: 0.7395 - prec_5: 0.9465 - recall_5: 0.9573 - f1_m: 0.8658 - recall_m: 0.8552 - precision_m: 0.8769 - precision: 0.8769 - recall_6: 0.8552 - f1score: 0.8658 - val_loss: 0.3215 - val_acc: 0.8634 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1589 - val_prec_2: 0.1897 - val_recall_2: 0.1762 - val_prec_3: 0.1762 - val_recall_3: 0.1236 - val_prec_4: 0.1886 - val_recall_4: 0.1502 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8626 - val_recall_m: 0.8536 - val_precision_m: 0.8719 - val_precision: 0.8719 - val_recall_6: 0.8536 - val_f1score: 0.8626\n",
      "Epoch 294/300\n",
      "epoch:  293\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 672us/step - loss: 0.3069 - acc: 0.8676 - prec: 0.9661 - recall: 0.9742 - prec_1: 0.9055 - recall_1: 0.9190 - prec_2: 0.9575 - recall_2: 0.9443 - prec_3: 0.7259 - recall_3: 0.6933 - prec_4: 0.6947 - recall_4: 0.7400 - prec_5: 0.9498 - recall_5: 0.9562 - f1_m: 0.8660 - recall_m: 0.8559 - precision_m: 0.8765 - precision: 0.8765 - recall_6: 0.8559 - f1score: 0.8660 - val_loss: 0.3294 - val_acc: 0.8594 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1568 - val_prec_2: 0.1897 - val_recall_2: 0.1770 - val_prec_3: 0.1762 - val_recall_3: 0.1317 - val_prec_4: 0.1916 - val_recall_4: 0.1396 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8592 - val_recall_m: 0.8501 - val_precision_m: 0.8688 - val_precision: 0.8688 - val_recall_6: 0.8501 - val_f1score: 0.8592\n",
      "Epoch 295/300\n",
      "epoch:  294\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 681us/step - loss: 0.3086 - acc: 0.8671 - prec: 0.9666 - recall: 0.9711 - prec_1: 0.9099 - recall_1: 0.9214 - prec_2: 0.9535 - recall_2: 0.9497 - prec_3: 0.7304 - recall_3: 0.6935 - prec_4: 0.6940 - recall_4: 0.7418 - prec_5: 0.9443 - recall_5: 0.9529 - f1_m: 0.8663 - recall_m: 0.8562 - precision_m: 0.8767 - precision: 0.8767 - recall_6: 0.8562 - f1score: 0.8663 - val_loss: 0.3247 - val_acc: 0.8541 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1592 - val_prec_2: 0.1897 - val_recall_2: 0.1749 - val_prec_3: 0.1762 - val_recall_3: 0.1232 - val_prec_4: 0.1858 - val_recall_4: 0.1404 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8526 - val_recall_m: 0.8423 - val_precision_m: 0.8633 - val_precision: 0.8633 - val_recall_6: 0.8423 - val_f1score: 0.8526\n",
      "Epoch 296/300\n",
      "epoch:  295\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 671us/step - loss: 0.3085 - acc: 0.8682 - prec: 0.9656 - recall: 0.9761 - prec_1: 0.9027 - recall_1: 0.9274 - prec_2: 0.9508 - recall_2: 0.9467 - prec_3: 0.7214 - recall_3: 0.7021 - prec_4: 0.7078 - recall_4: 0.7289 - prec_5: 0.9599 - recall_5: 0.9510 - f1_m: 0.8671 - recall_m: 0.8562 - precision_m: 0.8786 - precision: 0.8786 - recall_6: 0.8562 - f1score: 0.8671 - val_loss: 0.3239 - val_acc: 0.8559 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1577 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1294 - val_prec_4: 0.1866 - val_recall_4: 0.1359 - val_prec_5: 0.1672 - val_recall_5: 0.1586 - val_f1_m: 0.8547 - val_recall_m: 0.8438 - val_precision_m: 0.8660 - val_precision: 0.8660 - val_recall_6: 0.8438 - val_f1score: 0.8547\n",
      "Epoch 297/300\n",
      "epoch:  296\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 673us/step - loss: 0.3105 - acc: 0.8658 - prec: 0.9663 - recall: 0.9756 - prec_1: 0.9045 - recall_1: 0.9265 - prec_2: 0.9621 - recall_2: 0.9449 - prec_3: 0.7150 - recall_3: 0.6915 - prec_4: 0.6939 - recall_4: 0.7323 - prec_5: 0.9489 - recall_5: 0.9549 - f1_m: 0.8646 - recall_m: 0.8538 - precision_m: 0.8758 - precision: 0.8758 - recall_6: 0.8538 - f1score: 0.8646 - val_loss: 0.3243 - val_acc: 0.8581 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1580 - val_prec_2: 0.1897 - val_recall_2: 0.1757 - val_prec_3: 0.1762 - val_recall_3: 0.1380 - val_prec_4: 0.1899 - val_recall_4: 0.1319 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8563 - val_recall_m: 0.8463 - val_precision_m: 0.8667 - val_precision: 0.8667 - val_recall_6: 0.8463 - val_f1score: 0.8563\n",
      "Epoch 298/300\n",
      "epoch:  297\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 674us/step - loss: 0.3090 - acc: 0.8621 - prec: 0.9618 - recall: 0.9759 - prec_1: 0.9044 - recall_1: 0.9026 - prec_2: 0.9560 - recall_2: 0.9478 - prec_3: 0.7200 - recall_3: 0.6968 - prec_4: 0.7017 - recall_4: 0.7316 - prec_5: 0.9373 - recall_5: 0.9529 - f1_m: 0.8612 - recall_m: 0.8511 - precision_m: 0.8717 - precision: 0.8717 - recall_6: 0.8511 - f1score: 0.8612 - val_loss: 0.3227 - val_acc: 0.8491 - val_prec: 0.1762 - val_recall: 0.1722 - val_prec_1: 0.1758 - val_recall_1: 0.1587 - val_prec_2: 0.1897 - val_recall_2: 0.1752 - val_prec_3: 0.1762 - val_recall_3: 0.1232 - val_prec_4: 0.1882 - val_recall_4: 0.1366 - val_prec_5: 0.1672 - val_recall_5: 0.1584 - val_f1_m: 0.8485 - val_recall_m: 0.8396 - val_precision_m: 0.8578 - val_precision: 0.8578 - val_recall_6: 0.8396 - val_f1score: 0.8485\n",
      "Epoch 299/300\n",
      "epoch:  298\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3104 - acc: 0.8658 - prec: 0.9667 - recall: 0.9733 - prec_1: 0.9027 - recall_1: 0.9212 - prec_2: 0.9538 - recall_2: 0.9468 - prec_3: 0.7275 - recall_3: 0.6881 - prec_4: 0.6951 - recall_4: 0.7486 - prec_5: 0.9431 - recall_5: 0.9499 - f1_m: 0.8652 - recall_m: 0.8552 - precision_m: 0.8757 - precision: 0.8757 - recall_6: 0.8552 - f1score: 0.8652 - val_loss: 0.3288 - val_acc: 0.8519 - val_prec: 0.1762 - val_recall: 0.1717 - val_prec_1: 0.1758 - val_recall_1: 0.1594 - val_prec_2: 0.1897 - val_recall_2: 0.1765 - val_prec_3: 0.1762 - val_recall_3: 0.1065 - val_prec_4: 0.1832 - val_recall_4: 0.1522 - val_prec_5: 0.1672 - val_recall_5: 0.1576 - val_f1_m: 0.8521 - val_recall_m: 0.8418 - val_precision_m: 0.8628 - val_precision: 0.8628 - val_recall_6: 0.8418 - val_f1score: 0.8521\n",
      "Epoch 300/300\n",
      "epoch:  299\n",
      "Learning rate:  0.0001\n",
      "8004/8004 [==============================] - 5s 678us/step - loss: 0.3104 - acc: 0.8686 - prec: 0.9633 - recall: 0.9757 - prec_1: 0.9000 - recall_1: 0.9248 - prec_2: 0.9590 - recall_2: 0.9402 - prec_3: 0.7397 - recall_3: 0.7082 - prec_4: 0.7068 - recall_4: 0.7495 - prec_5: 0.9513 - recall_5: 0.9567 - f1_m: 0.8672 - recall_m: 0.8588 - precision_m: 0.8758 - precision: 0.8758 - recall_6: 0.8588 - f1score: 0.8672 - val_loss: 0.3248 - val_acc: 0.8641 - val_prec: 0.1762 - val_recall: 0.1719 - val_prec_1: 0.1758 - val_recall_1: 0.1580 - val_prec_2: 0.1897 - val_recall_2: 0.1767 - val_prec_3: 0.1762 - val_recall_3: 0.1325 - val_prec_4: 0.1890 - val_recall_4: 0.1424 - val_prec_5: 0.1672 - val_recall_5: 0.1581 - val_f1_m: 0.8631 - val_recall_m: 0.8529 - val_precision_m: 0.8739 - val_precision: 0.8739 - val_recall_6: 0.8529 - val_f1score: 0.8631\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "batch_size = 64\n",
    "split_val = 0.3\n",
    "fit_shuffle = True\n",
    "optimizer = 'sgd'\n",
    "start_lr = 1e-2\n",
    "\n",
    "split_shuffle = False\n",
    "history = []\n",
    "my_models = []\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "        lr = start_lr\n",
    "        if epoch > 225:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 150:\n",
    "            lr *= 1e-1\n",
    "        print('epoch: ', epoch)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "# 3.   \n",
    "sgd = SGD(lr=lr_schedule(0), decay=1e-4, momentum=0.9, nesterov=True)\n",
    "# adam = Adam(lr=lr_schedule(0))\n",
    "# nAdam = Nadam(lr=lr_schedule(0))\n",
    "\n",
    "\n",
    "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# modelv3.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "#                                                                          f1_m, recall_m, precision_m])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# Instantiate the cross validator\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "# Loop through the indices the split() method returns\n",
    "\n",
    "for index, (train_indices, val_indices) in enumerate(skf.split(x_data, y_data)):\n",
    "\n",
    "#     print(x[train_indices], x[val_indices])\n",
    "#     print(y[train_indices], y[val_indices])\n",
    "    xtrain, xval = x[train_indices], x[val_indices]\n",
    "    ytrain, yval = y[train_indices], y[val_indices]\n",
    "    \n",
    "    model = None\n",
    "#     model = createModel(101)\n",
    "    model = createCNNModel(depth=10, growthRate=12, input_shape=(500, 1), classes=6)\n",
    "    model.summary()\n",
    "#     model = createVGGModel(input_shape=(500,1), n_classes=6)\n",
    "#     model.summary()\n",
    "\n",
    "    csv_logger_path = save_path+f'/{model.name}(batch_size={batch_size}, split_val={split_val}, fit_shuffle={fit_shuffle}, split_shuffle={split_shuffle}, optimizer={optimizer}, lr=s:{start_lr}, per:1e-1, e:(150, 225))_log.csv'\n",
    "    if not os.path.isdir(os.path.dirname(csv_logger_path)):\n",
    "        os.makedirs(os.path.dirname(csv_logger_path))\n",
    "    csv_logger = CSVLogger(csv_logger_path, append=True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "    callbacks = [lr_scheduler, csv_logger]\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "                                                              \n",
    "                                                                single_class_precision(0), single_class_recall(0),\n",
    "                                                                single_class_precision(1), single_class_recall(1),\n",
    "                                                                single_class_precision(2), single_class_recall(2),\n",
    "                                                                single_class_precision(3), single_class_recall(3),\n",
    "                                                                single_class_precision(4), single_class_recall(4),\n",
    "                                                                single_class_precision(5), single_class_recall(5),\n",
    "#                                                                 single_class_precision(6), single_class_recall(6),\n",
    "#                                                                 single_class_precision(7), single_class_recall(7),\n",
    "#                                                                 single_class_precision(8), single_class_recall(8)\n",
    "                                                                            f1_m, recall_m, precision_m, precision, recall, f1score\n",
    "                                                                          ]) \n",
    "    \n",
    "    \n",
    "    history.append(model.fit(xtrain, ytrain, \n",
    "                        epochs=300, batch_size=batch_size, \n",
    "                        shuffle=fit_shuffle,\n",
    "#                         workers=4, \n",
    "                        validation_data=(xval, yval),\n",
    "                        callbacks=callbacks))\n",
    "\n",
    "    my_models.append(model)\n",
    "\n",
    "# # 4.  \n",
    "# history = modelv3.fit(x_train, y_train, \n",
    "#                     epochs=300, batch_size=batch_size, \n",
    "#                     shuffle=fit_shuffle,\n",
    "# #                     workers=1, \n",
    "#                       validation_data=(x_test, y_test),\n",
    "#                     callbacks=callbacks)\n",
    "\n",
    "# histories[model] = models[model].fit(x_train, y_train, epochs=300, batch_size=32, workers=4, validation_data=(x_train, y_train))\n",
    "\n",
    "# 6.  \n",
    "# evaluate = modelv3.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate = modelv3.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(\"loss     acc      f1      recall      precision\")\n",
    "print(evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hist in history:\n",
    "    for key, value in hist.history.items():\n",
    "        if key == \"val_acc\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_f1_m\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_recall_m\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_precision_m\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_prec\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_recall\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_prec_1\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_recall_1\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_prec_2\":\n",
    "            value = max(value)\n",
    "            print(key, value)\n",
    "            print(\"\\n\")\n",
    "        elif key == \"val_recall_2\":\n",
    "            value = max(value)\n",
    "            print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f5b116b0898>,\n",
       " <keras.engine.training.Model at 0x7f5b0f9e99b0>,\n",
       " <keras.engine.training.Model at 0x7f5b1f128390>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 1s 364us/step\n",
      "2160/2160 [==============================] - 1s 342us/step\n",
      "2160/2160 [==============================] - 1s 324us/step\n"
     ]
    }
   ],
   "source": [
    "for index, (train_indices, val_indices) in enumerate(skf.split(x_data, y_data)):\n",
    "\n",
    "#     print(x[train_indices], x[val_indices])\n",
    "#     print(y[train_indices], y[val_indices])\n",
    "    xtrain, xval = x[train_indices], x[val_indices]\n",
    "    ytrain, yval = y[train_indices], y[val_indices]\n",
    "    \n",
    "\n",
    "    loss_and_metrics = my_models[0].evaluate(xtrain, ytrain, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6091297308603922,\n",
       " 0.3402777777777778,\n",
       " 0.08888888888888889,\n",
       " 0.006481481481481481,\n",
       " 0.32804232813693857,\n",
       " 0.05416666666666667,\n",
       " 0.3333333333333333,\n",
       " 0.2800925925925926,\n",
       " 0.1728775711523162,\n",
       " 0.1162037037037037,\n",
       " 0.3454673724042045,\n",
       " 0.3454673724042045,\n",
       " 0.1162037037037037,\n",
       " 0.1728775711523162]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4002/4002 [==============================] - 1s 162us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-e1c4e81082ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_pred_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/pyqt/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1850\u001b[0m     \"\"\"\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyqt/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for index, (train_indices, val_indices) in enumerate(skf.split(x_data, y_data)):\n",
    "\n",
    "#     print(x[train_indices], x[val_indices])\n",
    "#     print(y[train_indices], y[val_indices])\n",
    "    xtrain, xval = x[train_indices], x[val_indices]\n",
    "    ytrain, yval = y[train_indices], y[val_indices]\n",
    "    \n",
    "    y_pred = my_models[0].predict(xval, batch_size=64, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    print(classification_report(yval, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ps8QIuN_mWhs"
   },
   "source": [
    "# version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiBxlLW7H-WZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# inputs = Input(shape=(100, 1))\n",
    "\n",
    "# modelv1 = DcnnNetv1(depth=100, growthRate=32, include_top=True, input_tensor=inputs, pooling='avg', classes=9)\n",
    "# modelv1.summary()\n",
    "# modelv1.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfsjvrWu9jjn"
   },
   "source": [
    "# v1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "9AK0uGhL9nwy",
    "outputId": "1914d5b7-aae1-499f-e744-f790287e3573"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "batch_size = 64\n",
    "split_val = 0.3\n",
    "fit_shuffle = True\n",
    "optimizer = 'sgd'\n",
    "start_lr = 1e-2\n",
    "split_shuffle = False\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "        lr = start_lr\n",
    "        if epoch > 225:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 150:\n",
    "            lr *= 1e-1\n",
    "        print('epoch: ', epoch)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "# 3.   \n",
    "sgd = SGD(lr=lr_schedule(0), decay=1e-4, momentum=0.9, nesterov=True)\n",
    "# adam = Adam(lr=lr_schedule(0))\n",
    "# nAdam = Nadam(lr=lr_schedule(0))\n",
    "\n",
    "\n",
    "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# models[model].compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "#                                                                                     single_class_precision(0), single_class_recall(0),\n",
    "#                                                                                     single_class_precision(1), single_class_recall(1),\n",
    "#                                                                                     single_class_precision(2), single_class_recall(2),\n",
    "#                                                                                     single_class_precision(3), single_class_recall(3),\n",
    "#                                                                                     single_class_precision(4), single_class_recall(4),\n",
    "#                                                                                     single_class_precision(5), single_class_recall(5),\n",
    "#                                                                                     single_class_precision(3), single_class_recall(6),\n",
    "#                                                                                     single_class_precision(4), single_class_recall(7),\n",
    "#                                                                                     single_class_precision(5), single_class_recall(8)])\n",
    "\n",
    "modelv1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "csv_logger_path = save_path+f'/{modelv1.name}(batch_size={batch_size}, split_val={split_val}, fit_shuffle={fit_shuffle}, split_shuffle={split_shuffle}, optimizer={optimizer}, lr=s:{start_lr}, per:1e-1, e:(150, 225))_log.csv'\n",
    "if not os.path.isdir(os.path.dirname(csv_logger_path)):\n",
    "    os.makedirs(os.path.dirname(csv_logger_path))\n",
    "csv_logger = CSVLogger(csv_logger_path, append=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "callbacks = [lr_scheduler, csv_logger]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4.  \n",
    "history = modelv1.fit(x_train, y_train, \n",
    "                                        epochs=300, batch_size=batch_size, \n",
    "                                        shuffle=fit_shuffle,\n",
    "                                        workers=4, validation_data=(x_test, y_test),\n",
    "                                        callbacks=callbacks)\n",
    "\n",
    "# histories[model] = models[model].fit(x_train, y_train, epochs=300, batch_size=32, workers=4, validation_data=(x_train, y_train))\n",
    "\n",
    "# 6.  \n",
    "evaluate = modelv1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "FquYdJ59SDb_",
    "outputId": "29ae5bd6-557a-4a85-a8db-67e0e53b0582"
   },
   "outputs": [],
   "source": [
    "# 6.  \n",
    "evaluate = modelv1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jyXxNNjSSnb"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtIw2AGf9fu6"
   },
   "source": [
    "# version2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Y_wU9z4T69A5",
    "outputId": "6ee3d579-6f69-49dc-f9d0-6b939517a8ee"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(100, 1))\n",
    "\n",
    "modelv2 = DcnnNetv2(depth=201, growthRate=32, include_top=True, input_tensor=inputs, pooling='avg', classes=9)\n",
    "modelv2.summary()\n",
    "modelv2.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awvyPBrqLXzj"
   },
   "source": [
    "# version2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-jkUc-9jLTlF",
    "outputId": "d334086f-1d42-4722-9bdf-fb8489ea970e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "batch_size = 64\n",
    "split_val = 0.3\n",
    "fit_shuffle = True\n",
    "optimizer = 'sgd'\n",
    "start_lr = 1e-2\n",
    "\n",
    "split_shuffle = False\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "        lr = start_lr\n",
    "        if epoch > 225:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 150:\n",
    "            lr *= 1e-1\n",
    "        print('epoch: ', epoch)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "# 3.   \n",
    "sgd = SGD(lr=lr_schedule(0), decay=1e-4, momentum=0.9, nesterov=True)\n",
    "# adam = Adam(lr=lr_schedule(0))\n",
    "# nAdam = Nadam(lr=lr_schedule(0))\n",
    "\n",
    "\n",
    "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# models[model].compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "#                                                                                     single_class_precision(0), single_class_recall(0),\n",
    "#                                                                                     single_class_precision(1), single_class_recall(1),\n",
    "#                                                                                     single_class_precision(2), single_class_recall(2),\n",
    "#                                                                                     single_class_precision(3), single_class_recall(3),\n",
    "#                                                                                     single_class_precision(4), single_class_recall(4),\n",
    "#                                                                                     single_class_precision(5), single_class_recall(5),\n",
    "#                                                                                     single_class_precision(3), single_class_recall(6),\n",
    "#                                                                                     single_class_precision(4), single_class_recall(7),\n",
    "#                                                                                     single_class_precision(5), single_class_recall(8)])\n",
    "\n",
    "modelv2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "                                                                         f1_m, recall_m, precision_m])\n",
    "\n",
    "csv_logger_path = save_path+f'/{modelv2.name}(batch_size={batch_size}, split_val={split_val}, fit_shuffle={fit_shuffle}, split_shuffle={split_shuffle}, optimizer={optimizer}, lr=s:{start_lr}, per:1e-1, e:(150, 225))_log.csv'\n",
    "if not os.path.isdir(os.path.dirname(csv_logger_path)):\n",
    "    os.makedirs(os.path.dirname(csv_logger_path))\n",
    "csv_logger = CSVLogger(csv_logger_path, append=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "callbacks = [lr_scheduler, csv_logger]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4.  \n",
    "history = modelv2.fit(x_train, y_train, \n",
    "                    epochs=300, batch_size=batch_size, \n",
    "                    shuffle=fit_shuffle,\n",
    "#                     workers=1, \n",
    "                      validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# histories[model] = models[model].fit(x_train, y_train, epochs=300, batch_size=32, workers=4, validation_data=(x_train, y_train))\n",
    "\n",
    "# 6.  \n",
    "evaluate = modelv2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "dz5gIfVHbSJW",
    "outputId": "c2a98073-fc17-4f62-fe73-3af81086b854"
   },
   "outputs": [],
   "source": [
    "evaluate = modelv2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(\"loss     acc      f1      recall      precision\")\n",
    "print(evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "qn-aGNVuO35L",
    "outputId": "2a0d756d-5860-4e55-d839-b9ca09ca341d"
   },
   "outputs": [],
   "source": [
    "epoch:  0\n",
    "Learning rate:  0.01\n",
    "Train on 2268 samples, validate on 972 samples\n",
    "Epoch 1/300\n",
    "epoch:  0\n",
    "Learning rate:  0.01\n",
    "2268/2268 [==============================] - 110s 48ms/step - loss: 0.5774 - acc: 0.7743 - f1_m: 0.7240 - recall_m: 0.6958 - precision_m: 0.7823 - val_loss: 8.3245 - val_acc: 0.3313 - val_f1_m: 0.3313 - val_recall_m: 0.3313 - val_precision_m: 0.3313\n",
    "Epoch 2/300\n",
    "epoch:  1\n",
    "Learning rate:  0.01\n",
    "2268/2268 [==============================] - 14s 6ms/step - loss: 0.1662 - acc: 0.9374 - f1_m: 0.9380 - recall_m: 0.9361 - precision_m: 0.9399 - val_loss: 0.8003 - val_acc: 0.7840 - val_f1_m: 0.7828 - val_recall_m: 0.7809 - val_precision_m: 0.7848\n",
    "Epoch 3/300\n",
    "epoch:  2\n",
    "Learning rate:  0.01\n",
    "2268/2268 [==============================] - 15s 7ms/step - loss: 0.1239 - acc: 0.9612 - f1_m: 0.9616 - recall_m: 0.9603 - precision_m: 0.9628 - val_loss: 5.2734 - val_acc: 0.3539 - val_f1_m: 0.3538 - val_recall_m: 0.3313 - val_precision_m: 0.3803\n",
    "Epoch 4/300\n",
    "epoch:  3\n",
    "Learning rate:  0.01\n",
    " 832/2268 [==========>...................] - ETA: 8s - loss: 0.1580 - acc: 0.9363 - f1_m: 0.9380 - recall_m: 0.9303 - precision_m: 0.9462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JtOOvKnfqt-"
   },
   "source": [
    "# version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlMAN-jlfphr"
   },
   "outputs": [],
   "source": [
    "img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "nChannels = 2 * growthRate\n",
    "bn_axis = 2\n",
    "\n",
    "x = layers.ZeroPadding1D(padding=1)(img_input)\n",
    "x = layers.Conv1D(nChannels, 3, strides=1, use_bias=False, name='conv1/conv')(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bg0XYdPOSDA4"
   },
   "source": [
    "#  \n",
    "\n",
    "## version1\n",
    "\n",
    "depth = 100\n",
    "\n",
    "### concatenate\n",
    "```\n",
    "start lr = 0.01\n",
    "    972/972 [==============================] - 1s 975us/step\n",
    "    [0.26581049285380653, 0.9259259259259259]\n",
    "\n",
    "start lr = 0.1\n",
    "    972/972 [==============================] - 1s 976us/step\n",
    "    [0.43105335702636727, 0.9176954732510288]\n",
    "```\n",
    "\n",
    "\n",
    "### add\n",
    "\n",
    "```\n",
    "start lr = 0.01\n",
    "    972/972 [==============================] - 1s 520us/step\n",
    "    [0.3993826474181909, 0.8713991769547325]\n",
    "\n",
    "start lr = 0.1\n",
    "\n",
    " \n",
    "```\n",
    "\n",
    "## version 2\n",
    "\n",
    "201 layer\n",
    "\n",
    "### concatenate\n",
    "\n",
    "```\n",
    "start lr = 0.1\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    [0.5396262257064811, 0.8446502060066035]\n",
    "Data normalization\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [2.230289366755466, 0.6306584359687052, 0.6331204848034392, 0.6255144030468944, 0.6410546889030394]\n",
    "min-max normalization\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [2.013901627603382, 0.6707818932494019, 0.6677523871017582, 0.6615226339901426, 0.674254747329916]\n",
    "\n",
    "start lr = 0.01\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [0.5466709941502952, 0.8631687245251219, 0.8630309512095197, 0.8621399179407598, 0.8639362478943028]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python \n",
    "start lr = 0.01\n",
    "x = layers.ZeroPadding1D(padding=3)(img_input)\n",
    "x = layers.Conv1D(nChannels, 7, strides=1, use_bias=False, name='conv1/conv')(x)\n",
    "\n",
    "# x = layers.BatchNormalization(\n",
    "#     axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "# x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "\n",
    "# x = layers.ZeroPadding1D(padding=1)(x)\n",
    "# x = layers.MaxPooling1D(3, strides=2, name='pool1')(x)\n",
    "\n",
    "    972/972 [==============================] - 2s 2ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [0.22257122489547493, 0.9454732510288066, 0.9459430514049137, 0.9454732510288066, 0.9464204066084245]\n",
    "\n",
    "x = layers.ZeroPadding1D(padding=3)(img_input)\n",
    "x = layers.Conv1D(nChannels, 7, strides=2, use_bias=False, name='conv1/conv')(x)\n",
    "\n",
    "# x = layers.BatchNormalization(\n",
    "#     axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "# x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "\n",
    "# x = layers.ZeroPadding1D(padding=1)(x)\n",
    "# x = layers.MaxPooling1D(3, strides=2, name='pool1')(x)\n",
    "\n",
    "    972/972 [==============================] - 1s 1ms/step\n",
    "    loss     acc      f1      recall      precision\n",
    "    [0.5289181569809623, 0.8837448559670782, 0.8851272049264162, 0.8816872427983539, 0.8886497324876824]\n",
    "\n",
    "```\n",
    "\n",
    "### add\n",
    "\n",
    "```\n",
    "start lr = 0.01\n",
    "    972/972 [==============================] - 1s 923us/step\n",
    "    [0.8464200724790125, 0.7129629632082496]\n",
    "\n",
    "\n",
    "start lr = 0.1\n",
    "    972/972 [==============================] - 1s 915us/step\n",
    "    [1.0798768680772663, 0.49074074074074076]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLgdTKS9XtSV"
   },
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "params = {\n",
    "    'batch_size' : [2, 4, 8, 16, 32, 64, 128, 256],\n",
    "    'fit_shuffle' : [False, True],\n",
    "    'split_shuffle' : [False, True],\n",
    "    'start_lr' : [1e-1, 1e-2, 1e-3],\n",
    "    'optimizer' : ['sgd','adam','nAdam'],\n",
    "    'split_val' : [0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "batch_size = params['batch_size'][0]\n",
    "fit_shuffle = params['fit_shuffle'][0]\n",
    "split_shuffle = params['split_shuffle'][0]\n",
    "start_lr = params['start_lr'][0]\n",
    "optimizer = params['optimizer'][0]\n",
    "split_val = params['split_val'][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_s-3f2pReqM"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential # \n",
    "from keras.models import Model # \n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Gw7r_DpX-rs"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "#    \n",
    "def single_class_precision(interesting_class_id):\n",
    "    def prec(y_true, y_pred):\n",
    "        class_id_true = K.argmax(y_true, axis=-1)\n",
    "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
    "        precision_mask = K.cast(K.equal(class_id_pred, interesting_class_id), 'int32')\n",
    "        class_prec_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * precision_mask\n",
    "        class_prec = K.cast(K.sum(class_prec_tensor), 'float32') / K.cast(K.maximum(K.sum(precision_mask), 1), 'float32')\n",
    "        return class_prec\n",
    "    return prec\n",
    "\n",
    "\n",
    "#    \n",
    "def single_class_recall(interesting_class_id):\n",
    "    def recall(y_true, y_pred):\n",
    "        class_id_true = K.argmax(y_true, axis=-1)\n",
    "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
    "        recall_mask = K.cast(K.equal(class_id_true, interesting_class_id), 'int32')\n",
    "        class_recall_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * recall_mask\n",
    "        class_recall = K.cast(K.sum(class_recall_tensor), 'float32') / K.cast(K.maximum(K.sum(recall_mask), 1), 'float32')\n",
    "        return class_recall\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZa_m3V5_a-Z"
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "model_type = {\n",
    "    \"MLP\" : {},\n",
    "    \"LSTM\" : {},\n",
    "    \"CONV1D\" : {},\n",
    "    \"CONV1D+LSTM\" : {},\n",
    "    \"LSTM+CONV1D\" : {}\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "evaluates = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsbzcSNPubub"
   },
   "source": [
    "#MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyC9i6cgFK1-"
   },
   "outputs": [],
   "source": [
    "\n",
    "for ii in range(6):\n",
    "    iii = 0\n",
    "    inputs = Input(shape=(1, 100))\n",
    "    x = Dense(10, activation='relu')(inputs)\n",
    "    for iii in range(ii):\n",
    "        x = Dense(10, activation='relu')(x)\n",
    "    y = Dense(9, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    models[f\"[MLP]{ii+1}_layer\"] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5k2Zv6BeuwUe"
   },
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZ0AkepIuzmR"
   },
   "outputs": [],
   "source": [
    "\n",
    "for ii in range(6):\n",
    "    iii = 0\n",
    "    inputs = Input(shape=(1,100))\n",
    "    x = LSTM(10, return_sequences=True, activation=\"relu\")(inputs)\n",
    "    for iii in range(ii):\n",
    "        if iii != ii - 1:\n",
    "            x = LSTM(10, return_sequences=True, activation=\"relu\")(x)\n",
    "        else:\n",
    "            x = LSTM(10, return_sequences=True, activation=\"relu\")(x)\n",
    "    y = Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "    models[f\"[LSTM]{ii+1}_layer\"] = Model(inputs=inputs, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pt0wBtJ0uf9D"
   },
   "source": [
    "#Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQ11U9_cufdc"
   },
   "outputs": [],
   "source": [
    "\n",
    "for ii in range(6):\n",
    "    inputs = Input(shape=(1,100))\n",
    "    x = Conv1D(10, 1, activation=\"relu\", padding=\"same\", strides=1)(inputs)\n",
    "    for iii in range(ii):\n",
    "        x = Conv1D(100, 3, activation=\"relu\", padding=\"same\", strides=1)(x)\n",
    "    y = Dense(9, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    models[f\"[Conv1D]{ii+1}_layer\"] = model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "datVAgzhul62"
   },
   "source": [
    "#Conv1D + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-7dCgy-ulXn"
   },
   "outputs": [],
   "source": [
    "for ii in range(6):\n",
    "    inputs = Input(shape=(1,100))\n",
    "    x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(inputs)\n",
    "    x = LSTM(10, activation=\"relu\", return_sequences=True)(x)\n",
    "    for iii in range(ii):\n",
    "        x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "        x = LSTM(10, activation=\"relu\", return_sequences=True)(x)  \n",
    "    y = Dense(9, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    models[f\"[Conv1D + LSTM]{ii+1}_layer\"] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b86CS4nLulQ3"
   },
   "source": [
    "#LSTM + Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "TP2dAIciuuBq",
    "outputId": "384fde54-57ab-460c-9912-684576ade9e4"
   },
   "outputs": [],
   "source": [
    " for ii in range(6):\n",
    "    inputs = Input(shape=(1,100))\n",
    "    x = LSTM(10, activation=\"relu\", return_sequences=True)(inputs)\n",
    "    x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "    for iii in range(ii):\n",
    "        x = LSTM(10, activation=\"relu\", return_sequences=True)(x)  \n",
    "        x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "    y = Dense(9, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    models[f\"[LSTM + Conv1D]{ii+1}_layer\"] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VWYJUHQw88a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cze9MLtlw845"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "8iTEQH-kJmKa",
    "outputId": "3ea2f8b9-68d2-4766-b340-48de2186cca6"
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WPzvqKLCWCb"
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    models[model].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lkBi8hG_Renb",
    "outputId": "ae34035d-db04-43a8-cfc7-ba99da73be7f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    def lr_schedule(epoch):\n",
    "        lr = start_lr\n",
    "        if epoch > 225:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 150:\n",
    "            lr *= 1e-1\n",
    "        print('epoch: ', epoch)\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "    # 3.   \n",
    "    if optimizer == 'sgd':\n",
    "        sgd = SGD(lr=lr_schedule(0), decay=1e-4, momentum=0.9, nesterov=True)\n",
    "    elif optimizer == 'adam':\n",
    "        adam = Adam(lr=lr_schedule(0))\n",
    "    elif optimizer == 'nAdam':\n",
    "        nAdam = Nadam(lr=lr_schedule(0))\n",
    "\n",
    "\n",
    "    # model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # models[model].compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',\n",
    "    #                                                                                     single_class_precision(0), single_class_recall(0),\n",
    "    #                                                                                     single_class_precision(1), single_class_recall(1),\n",
    "    #                                                                                     single_class_precision(2), single_class_recall(2),\n",
    "    #                                                                                     single_class_precision(3), single_class_recall(3),\n",
    "    #                                                                                     single_class_precision(4), single_class_recall(4),\n",
    "    #                                                                                     single_class_precision(5), single_class_recall(5),\n",
    "    #                                                                                     single_class_precision(3), single_class_recall(6),\n",
    "    #                                                                                     single_class_precision(4), single_class_recall(7),\n",
    "    #                                                                                     single_class_precision(5), single_class_recall(8)])\n",
    "\n",
    "    models[model].compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    csv_logger_path = save_path+f'/{model}(batch_size={batch_size}, split_val={split_val}, fit_shuffle={fit_shuffle}, split_shuffle={split_shuffle}, optimizer={optimizer}, lr=s:{start_lr}, per:1e-1, e:(150, 225))_log.csv'\n",
    "    if not os.path.isdir(os.path.dirname(csv_logger_path)):\n",
    "        os.makedirs(os.path.dirname(csv_logger_path))\n",
    "    csv_logger = CSVLogger(csv_logger_path, append=True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "    callbacks = [lr_scheduler, csv_logger]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 4.  \n",
    "    histories[model] = models[model].fit(x_train, y_train, \n",
    "                                         epochs=300, batch_size=batch_size, \n",
    "                                         shuffle=fit_shuffle,\n",
    "                                         workers=4, validation_data=(x_test, y_test),\n",
    "                                         callbacks=callbacks)\n",
    "    \n",
    "    # histories[model] = models[model].fit(x_train, y_train, epochs=300, batch_size=32, workers=4, validation_data=(x_train, y_train))\n",
    "\n",
    "    # 6.  \n",
    "    evaluates[model] = models[model].evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "TLIdaxrQaJxX",
    "outputId": "1e9b6beb-5ec5-40cc-b9c2-54b0020bcf1c"
   },
   "outputs": [],
   "source": [
    "evaluates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2KkJdfuD8UZ"
   },
   "outputs": [],
   "source": [
    "# 5.  \n",
    "\n",
    "for history in histories:\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.plot(histories[history].history['loss'], 'y', label=\"train loss\")\n",
    "    ax.plot(histories[history].history['val_loss'], 'r', label=f\"val loss({format(evaluates[history][0], '0.4f')})\")\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"{history}\")\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.plot(histories[history].history['acc'], 'y', label=\"train acc\")\n",
    "    ax.plot(histories[history].history['val_acc'], 'r', label=f\"val acc({format(evaluates[history][1],'0.4f')})\")\n",
    "    ax.set_ylabel('accuray')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"{history}\")\n",
    "\n",
    "    save_type = \"history\"\n",
    "    history_path = f\"{save_path}/{save_type}\"\n",
    "\n",
    "    if not os.path.isdir(history_path):\n",
    "        os.mkdir(history_path)\n",
    "    plt.savefig(history_path + \"/\" + history)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "LNP-CW0ETOVP",
    "outputId": "a7760fac-7115-42f6-9855-3860bf986d2f"
   },
   "outputs": [],
   "source": [
    "*histories['[MLP]1_layer'].history[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apf6Nyv-GpD2"
   },
   "outputs": [],
   "source": [
    "for history in histories:\n",
    "\n",
    "    precs = [x for x in histories[history].history.keys() if \"prec\" in x if \"val\" not in x]\n",
    "    recalls = [x for x in histories[history].history.keys() if \"recall\" in x if \"val\" not in x]\n",
    "\n",
    "    val_precs = [x for x in histories[history].history.keys() if \"prec\" in x if \"val\" in x]\n",
    "    val_recalls = [x for x in histories[history].history.keys() if \"recall\" in x if \"val\" in x]\n",
    "\n",
    "    fig = plt.figure(figsize=(30,5))\n",
    "\n",
    "    for prec in precs:\n",
    "        ax = fig.add_subplot(1,4,1)\n",
    "        ax.plot(histories[history].history[prec], label=prec)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('precision')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{history}\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        \n",
    "    for recall in recalls:\n",
    "        ax = fig.add_subplot(1,4,2)\n",
    "        ax.plot(histories[history].history[recall], label=recall)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('recall')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{history}\")  \n",
    "        ax.set_ylim(0, 1.0)\n",
    "\n",
    "    for prec in val_precs:\n",
    "        ax = fig.add_subplot(1,4,3)\n",
    "        ax.plot(histories[history].history[prec], label=prec)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('val_precision')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{history}\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        \n",
    "    for recall in val_recalls:\n",
    "        ax = fig.add_subplot(1,4,4)\n",
    "        ax.plot(histories[history].history[recall], label=recall)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('val_recall')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{history}\")  \n",
    "        ax.set_ylim(0, 1.0)\n",
    "\n",
    "    save_type = \"precision,recall\"\n",
    "    history_path = f\"{save_path}/{save_type}\"\n",
    "\n",
    "    if not os.path.isdir(history_path):\n",
    "        os.mkdir(history_path)\n",
    "    plt.savefig(history_path + \"/\" + history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTdZoxM_YJL-"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "\n",
    "for id, evaluate in enumerate(evaluates):\n",
    "    # print(evaluates[evaluate])\n",
    "    # print(evaluates[evaluate][2:])\n",
    "    metrics = np.array(evaluates[evaluate][2:])\n",
    "\n",
    "    idx = np.linspace(0, 11, 12) \n",
    "    # print(idx)\n",
    "    precision = metrics[(idx % 2) == 0]\n",
    "    recall = metrics[((idx+1) % 2) == 0]\n",
    "\n",
    "    # print(precision)\n",
    "    # print(recall)\n",
    "    # print(\"\\n\")\n",
    "    # print(\"\\n\")\n",
    "    # print(\"\\n\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "\n",
    "    N = 6\n",
    "    ind = np.arange(N)\n",
    "    width = 0.35\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,id+1)\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    prec_bar = ax.bar(ind, precision, width, color='r')\n",
    "    recall_bar = ax.bar(ind + width, recall, width, color='y')\n",
    "\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Precision and Recall')\n",
    "    ax.set_xticks(ind + width / 2)\n",
    "    ax.set_xticklabels(('C1', 'C2', 'C3', 'C4', 'C5', 'C6'))\n",
    "\n",
    "    ax.legend((prec_bar[0], recall_bar[0]), ('Precision', 'Recall'))\n",
    "    # ax.grid(True)\n",
    "    ax.set_title(evaluate)\n",
    "    ax.set_ylim(0,1.0)\n",
    "\n",
    "        \n",
    "save_type = \"precision,recall_graph\"\n",
    "history_path = f\"{save_path}/{save_type}\"\n",
    "\n",
    "if not os.path.isdir(history_path):\n",
    "    os.mkdir(history_path)\n",
    "plt.savefig(history_path + \"/\" + evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQZwJqsBYF6j"
   },
   "outputs": [],
   "source": [
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8L9-hm1_43vO",
    "outputId": "218eedc8-2af7-49ab-a65a-65a74831d8ac"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 1, 100)\n",
    "x_test  = x_test.reshape(-1, 1, 100)\n",
    "y_train = y_train.reshape(-1,  6)\n",
    "y_test = y_test.reshape(-1, 6)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOrPqCLTDxl4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "m80D0FrrISs3",
    "outputId": "56f6f592-b172-49c0-aec2-d369ef68b2b3"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,100))\n",
    "x = LSTM(10, return_sequences=True, activation=\"relu\")(inputs)\n",
    "y = Dense(6, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "52oawxk5RekY",
    "outputId": "da0de6b0-c54c-47cd-8722-922d0ab5b378"
   },
   "outputs": [],
   "source": [
    "# 2.  \n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128))\n",
    "# # model.add(Flatten())\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "evaluates = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    models[model].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "UMU5OAb_N6Kh",
    "outputId": "7195b0a7-d3c4-43bb-f9b7-5d91d913fef5"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,100))\n",
    "x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(inputs)\n",
    "x = LSTM(10, activation=\"relu\",return_sequences=True)(x)\n",
    "x = Conv1D(10, 1, activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "x = LSTM(10, activation=\"relu\")(x)  \n",
    "y = Dense(6, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mXnija4fK0tR",
    "outputId": "d48b7d05-6a3b-4b51-a97d-b92f26bb021c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "evaluates = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(\"\\n\"+model)\n",
    "    models[model].summary()\n",
    "# model.add(Conv1D(50,\n",
    "#                  1,\n",
    "#                  input_shape=(1,100),\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# # model.add(GlobalMaxPooling1D())\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nD8PJcbGWfC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3.   \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4.  \n",
    "hist = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# 5.  \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "ax.set_ylabel('loss')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "ax.set_ylabel('accuray')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "# fig, loss_ax = plt.subplots()\n",
    "\n",
    "# acc_ax = loss_ax.twinx()\n",
    "\n",
    "# loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "# loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "# acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "# acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "# acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "# loss_ax.set_xlabel('epoch')\n",
    "# loss_ax.set_ylabel('loss')\n",
    "# acc_ax.set_ylabel('accuray')\n",
    "\n",
    "# loss_ax.legend(loc='upper left')\n",
    "# acc_ax.legend(loc='lower left')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# 6.  \n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "GPc317BeUxfj",
    "outputId": "adcdb115-a4a3-4860-a48d-1e6e0a2f08c9"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958
    },
    "colab_type": "code",
    "id": "ld0YvmINUxzr",
    "outputId": "04a838bc-8b34-4128-a9fd-6a36931c4e0f"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(50,\n",
    "                 1,\n",
    "                 input_shape=(1,100),\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 3.   \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4.  \n",
    "hist = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# 5.  \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6.  \n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rf0mstPzcqmF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "fgJET27RjWTv",
    "outputId": "56298d1c-8730-42e1-e35f-e1909b4e2a0e"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "                    # Input(shape=(1,100)),\n",
    "                    Conv1D(50,1, input_shape=(1,100) ,padding='valid',activation='relu',strides=1),\n",
    "                    Conv1D(50,1, padding='valid',activation='relu',strides=1),\n",
    "                    Conv1D(50,1, padding='valid',activation='relu',strides=1),\n",
    "                    Conv1D(50,1, padding='valid',activation='relu',strides=1),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(50, activation='relu'),\n",
    "                    # model.add(Dropout(0.2))\n",
    "                    Dense(6, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CE3nu0OtjWRc",
    "outputId": "e2a4298a-d377-4bef-b534-36963170460c"
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# # model.add(Embedding(10, 128, input_length=100))\n",
    "# # model.add(Dropout(0.2))\n",
    "# model.add(Input(shape=(1,100)))\n",
    "# model.add(Conv1D(100,\n",
    "#                  1,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# # model.add(Dropout(0.2))\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# 3.   \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4.  \n",
    "hist = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# 5.  \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "ax.set_ylabel('loss')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "ax.set_ylabel('accuray')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig, loss_ax = plt.subplots()\n",
    "\n",
    "# acc_ax = loss_ax.twinx()\n",
    "\n",
    "# loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "# loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "# acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "# acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "# acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "# loss_ax.set_xlabel('epoch')\n",
    "# loss_ax.set_ylabel('loss')\n",
    "# acc_ax.set_ylabel('accuray')\n",
    "\n",
    "# loss_ax.legend(loc='upper left')\n",
    "# acc_ax.legend(loc='lower left')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# 6.  \n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4bNYV8ijYRS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KjCs0sUyFeeV",
    "RfsjvrWu9jjn"
   ],
   "name": "1D DenseNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
